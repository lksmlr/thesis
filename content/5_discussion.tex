\chapter{Diskussion}\label{ch:discussion}

\section{Interpretation der Fehler}\label{sec:error_patterns}

Sowohl die YOLO/OCR-Pipeline als auch die \glspl{VLM} weisen spezifische Fehlermuster auf, die sich in Klassifikations-, Format-, Detektions- und Wertfehler unterteilen lassen.

Klassifikationsfehler wiegen in der aus mehreren Einzelkomponenten bestehenden YOLO/OCR-Pipeline besonders schwer, da die Klassifikation hier einen Single Point of Failure darstellt.
Wie die Confusion Matrix in Abbildung~\ref{fig:confusion_matrices_overview} verdeutlicht, wurden fünf Dokumente fälschlicherweise als \texttt{Sonstiges} eingestuft, was wegen der Systemarchitektur (Abbildung~\ref{fig:aube}) zu einem direkten Abbruch führt.
Bei diesen fünf Dokumenten handelte es sich auffälligerweise ausschließlich um fotografierte Unterlagen.
Fotos weisen häufig Verzerrungen, Schattenwurf oder eine ungleichmäßige Beleuchtung auf, wodurch die \gls{OCR}-Ergebnisse schlechter ausfallen und die Klassifikation behindern.

Diese Klassifikationsfehler mindern die Leistung der Information Extraction, da mit einem einzigen Fehler schon zehn (Vertrag) oder zwölf (KG5b) Felder fehlen.
Auch die fehlerhafte Klassifikation von Verträgen als KG5b-Formular oder umgekehrt, wirkt sich stark auf die Information Extraction aus, da unterschiedliche \gls{YOLO}-Modelle für Verträge und KG5b-Formulare trainiert wurden.
Ein entscheidender Vorteil des generalistischen \glspl{VLM} ist die Robustheit durch das einheitliche Modell.
Da die Felder eines Vertrags eine Teilmenge des KG5b-Schemas darstellen, bleibt die Information Extraction teilweise erfolgreich, selbst das Dokument falsch klassifiziert wurde.
Ein Fehler in der Klassifikation kann durch die Überlappung der Schemata demnach reduziert werden und propagiert nicht so stark wie bei der YOLO/OCR-Pipeline.
In Idealfall fehlen bei der Erkennung eines Vertrages statt eines KG5b-Formulars kein Feld und umgekehrten Fall maximal zwei.
Die Fehlklassifizierung als sonstiges Dokument ist jedoch auch bei den \glspl{VLM} kritisch, da hier das Modell keine weiteren Felder zurückliefert.
Gemäß den Confusion Matrices in Abbildung~\ref{fig:confusion_matrices_32b_peft} zu erkennen erweisen sich insbesondere die Modelle Qwen-2.5-VL-7B-finetuned und Qwen-2.5-VL-32B als äußert zuverlässig in der Klassifizierung.

Die Detektionsfehler beziehen sich auf Checkboxen, Stempel und Unterschriften.
Checkboxen sind im KG5b-Formular für das Feld \texttt{apprenticeship\_finished} relevant (siehe Abbildung~\ref{fig:kg5bpage1}).
Hier müssen die Modelle zwischen leeren, angekreuzten und durchgestrichenen Boxen unterscheiden.
Detektionsfehler sind häufig auf den unvermeidbaren Medienbruch zurückzuführen, der entsteht, wenn Dokumente zur Unterschrift oder Stempelung ausgedruckt und anschließend wieder digitalisiert werden.
Durch den Medienbruch verliert der Stempel und die Unterschriften ihren Kontrast oder werden in Schwarz-Weiß-Scans unkenntlich.

Zusätzlich erschwert die räumliche Nähe der Felder \texttt{date\_document} und \texttt{signature\_company} die Extraktion.
Da Unterschriften regelmäßig den vorgesehenen Platz überschreiten, wird das Datumsfeld häufig überdeckt.
Während die YOLO/OCR-Pipeline hier oft keine valide Bounding-Box mehr liefern kann, zeigen das Qwen-2.5-VL-32B und das Qwen-2.5-VL-7B-finetuned eine Robustheit gegenüber diesen Überlagerungen.

Formatfehler verdeutlichen Unterschiede zwischen den Instruction-Following Fähigkeiten der Basismodelle.
Während die Qwen-Modelle nahezu fehlerfrei valide \gls{JSON}-Strukturen lieferten, scheiterte das Pixtral-12B trotz der Retry-Logik an zwei Verträgen.
Das Modell hat in keinem der erneuten Versuche die umschließenden Klammern ({}) gesetzt, wodurch das Parsen fehlschlug.
Infolgedessen fehlten automatisch insgesamt 20 \gls{JSON}-Felder.
Dies deutet darauf hin, dass das Pixtral-12B aufgrund seines Trainings, eine geringere Stabilität als die Qwen-Modelle bei der Generierung von \gls{JSON}-Strukturen aufweist.

Bei den Wertfehlern muss zwischen \gls{OCR}-Fehlern und Mapping-Fehlern (falsche semantische Zuordnung) unterschieden werden.
Die YOLO/OCR-Pipeline zeigte deutliche Schwächen in der Erkennung von Namen und Datumsangaben.
Selbst die Levenshtein-Similarity mit einem Threshold von 0,8 reichte bei einigen Fällen nicht aus, um die Fehler zu kompensieren.
Bei Pixtral-12B und Qwen-2.5-VL-7B traten neben \gls{OCR}-Fehlern vermehrt Mapping-Fehler auf, bei denen beispielsweise das Geburtsdatum des Kindes mit dem Startdatum der Ausbildung vertauscht wurde.
Durch das Fine-Tuning des Qwen-2.5-VL-7B konnte eine deutliche Reduzierung der \gls{OCR}- und Mapping-Fehler erzielt werden.
Das Qwen-2.5-VL-32B wies seltener \gls{OCR}-Fehler auf als das Qwen-2.5-VL-7B-finetuned, produzierte jedoch geringfügig mehr Mapping-Fehler.

Halluzinationen von Feldern traten primär bei den Basismodellen auf, insbesondere bei der Klasse \texttt{Sonstiges}, wo die Modelle versuchten, Informationen zu extrahieren, die im Schema gar nicht vorgesehen waren.
Wegen der Konfiguration, können diese zusätzlichen Felder ohne Einfluss gefiltert werden, weshalb sie in der Evaluation nicht bestraft wurden.
Für reale System bedeuten Halluzinationen von Feldern keine Einschränkung in der Leistung.


\section{Abwägung Effektivität und Ressourceneffizienz}\label{sec:effectiveness_resource_efficiency}

Die Ergebnisse der Untersuchung offenbaren einen Zielkonflikt zwischen der Leistungsfähigkeit der Modelle und ihrem Ressourcenbedarf.
Während die YOLO/OCR-Pipeline durch ihre extrem geringe \gls{Latenz} und Ressourceneffizienz besticht, zeigen die \glspl{VLM} eine deutliche Überlegenheit in der Qualität der Information Extraction.

Die YOLO/OCR-Pipeline bildet mit einem durchschnittlichen Energieverbrauch von 117 J und einer \gls{Latenz} von unter einer Sekunde die Basislinie für die Effizienz.
Demgegenüber steht das leistungsstärkste Modell, das Qwen-2.5-VL-32B, welches im Durchschnitt die 160-fache Energie pro Dokument benötigt und eine \gls{Latenz} von durchschnittlich 64,16 Sekunden aufweist.
Kritisch betrachtet relativiert sich der Effizienzvorteil der Pipeline jedoch im operativen Kontext.
Ein niedriger F1-Score (0,51 bei Verträgen) zieht zwangsläufig eine Nachbearbeitung durch Sachbearbeiter nch sich.
Die durch die schnelle Verarbeitung gewonnene Zeit wird somit im Gesamtprozess durch menschliche Korrektureingriffe wieder reduziert.

Eine menschliche Korrektur von einer Minute ist wirtschaftlich gesehen teurer als eine Minute Latenz.
Die schlechtere Effektivität der YOLO/OCR-Pipeline wird damit finanziell kompensiert.

Neben dem Energiebedarf ist die Hardware-Infrastruktur, spezifisch die \gls{VRAM}-Auslastung, ein limitierender Faktor.
Während die YOLO/OCR-Pipeline auf Standard-Hardware lauffähig ist, setzen die großen \glspl{VLM} leistungsstarke \glspl{GPU} voraus.
Das Qwen-2.5-VL-32B übertrifft das Qwen-2.5-VL-7B-finetuned zwar in der Information Extraction bei Verträgen (F1-Score 0,94 gegenüber 0,84), erkauft sich diesen Vorteil jedoch mit einem fast vierfach höheren statischen \gls{VRAM}-Bedarf.

Als technischer \enquote{Sweet Spot} zwischen Verbrauch, \gls{Latenz} und Extraktionsgüte kristallisiert sich das Qwen-2.5-VL-7B-finetuned heraus.
Es übertrifft die Pipeline sowohl in der Klassifikation als auch in der Information Extraction signifikant und bleibt dabei ressourcentechnisch niedriger als das Qwen-2.5-VL-32B.
Das Betreiben von \glspl{VLM} bleibt aber ein Skalierungsproblem, da gleichzeitige Anfragen die dynamische VRAM-Auslastung massiv beeinflussen und man somit für ein ausreichend hohen Tokendurchsatz mehrere GPUs simultan betreiben muss.
Einfache Modelle wie die der YOLO/OCR-Pipeline haben dieses Problem nicht.



\section{Komplexität und Wartbarkeit der Systemarchitekturen}\label{sec:maintainability}

Neben der reinen Modellleistung stellt die architektonische Komplexität einen zentralen Faktor für den produktiven Einsatz dar.
Zur Einordnung wird das Qualitätsmodell der ISO/IEC~25010 herangezogen, wobei der Fokus auf dem Qualitätsmerkmal der Wartbarkeit mit den Teilaspekten Modularität, Analysierbarkeit, Änderbarkeit und Testbarkeit liegt.

Die Architektur der YOLO/OCR-Pipeline zeichnet sich durch eine Verkettung spezialisierter Komponenten aus (siehe Abbildung~\ref{fig:aube}).
Obwohl dieser Aufbau grundsätzlich modular ist, entsteht durch die sequenzielle Verarbeitung eine kritische Abhängigkeit.
Der Klassifikator nimmt eine zentrale Rolle ein, da seine Entscheidung den weiteren Verlauf bestimmt.
Fehlklassifikationen propagieren durch die Komponenten der Pipeline, was die Fehlerursachenanalyse erschwert.
Wie bereits in der Fehleranalyse~\ref{sec:error_patterns} identifiziert, stellt der Klassifikator hier architektonisch einen Single Point of Failure dar.

Hinsichtlich der Änderbarkeit offenbart die YOLO/OCR-Pipeline einen erhöhten Aufwand.
Die Einführung einer neuen Dokumentenart erfordert ein erneutes Training des Klassifikators und das Training eines neuen YOLO-Modells.
Durch das manuelle Annotieren der Bounding-Boxen ist das Training eines neuen Modell mit hohem Aufwand verbunden.
Zudem erhöhen die Schnittstellen und die Konvertierungslogiken zwischen den Modulen die Fehleranfälligkeit.

Im Gegensatz dazu verfolgt der \gls{VLM}-Ansatz eine Konsolidierung der Aufgaben: Klassifikation und Information Extraction erfolgen innerhalb eines einzigen Modells.
Dies reduziert die Anzahl der Systemkomponenten und Schnittstellen drastisch, was die \textbf{architektonische Komplexität} senkt.
Änderungen an Dokumentenarten beschränken sich – wie am Beispiel des Qwen-2.5-VL-7B-finetuned gezeigt – auf das Nachtraining einer einzigen Komponente oder, bei Basismodellen, auf die Anpassung des Prompts.

Dieser monolithische Ansatz bringt jedoch spezifische Nachteile für die Testbarkeit und Stabilität mit sich.
Ein \gls{VLM} agiert als Black Box, wodurch die interne Logik schwer analysierbar ist.
Während in der YOLO/OCR-Pipeline die Komponenten logisch entkoppelt sind – eine Änderung am YOLO-Modell lässt die Funktionalität des OCR-Moduls unberührt –, sind die Fähigkeiten eines \gls{VLM} in den Gewichten untrennbar verwoben.
Eine Änderung des Prompts oder ein Fine-Tuning zur Verbesserung einer Dokumentenart kann unvorhersehbare Seiteneffekte (Regressionen) auf andere Dokumentenarten haben.
Dies erhöht die Komplexität von Regressionstests signifikant, da nach jeder Änderung das gesamte Ausgabespektrum des Modells validiert werden muss.



\section{Zusammenfassende Beantwortung der Forschungsfragen}\label{sec:answer_ffs}

- Die Abwägung zwischen ökologischen Fußabdruck, Leistung und Einsparung von Mitarbeiterkosten ist

\section{Limitation und Probleme der Untersuchung}\label{sec:limitations}

- name usw auch aus sonstigen extrahieren lassen wegen klassifikationsfehlern
- bounding boxen für Kontrolle ob wirklich unterschriften zugeordnet werden können (labeln hätte um einiges länger gedauert)
- base64 ging nicht



\section{Handlungsempfehlung für die Bundesagentur für Arbeit}\label{sec:recommendation_for_action}

- mitarbeiterkosten
- andere Projekte
- weiter labeln mit 32B modell -> 7B-finetuned noch robuster

- yolo ocr schneidet am besten ab wegen pipline design bei sonstigen dokumenten
- fehlerhafte klassifikation zieht sich durch
- confusion matrix yolo ocr viele als sonstige -> große Fehler
- kg5d
- hohen Verbrauch erklären
