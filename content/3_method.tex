\chapter{Methodik und Implementierung}\label{ch:methodology}

Um die Eignung der \glspl{VLM} als potenziellen Ersatz für die \gls{YOLO}/\gls{OCR}-Pipeline zu beurteilen, werden die folgenden Forschungsfragen beantwortet:

\begin{enumerate}
    \item Wie verhält sich ein nicht domänenspezifisch angepasstes \gls{VLM} hinsichtlich \gls{Latenz}, Klassifikations- und Extraktionsleistung im Vergleich zur \gls{YOLO}/\gls{OCR}-Pipeline, und welche Vorteile bietet ein einzelnes generalistisches Modell gegenüber spezialisierten Einzelsystemen?
    \item Welche Auswirkungen hat ein domänenspezifisches Training eines kleineren Modells auf dessen \gls{Latenz}, Klassifikations- und Extraktionsleistung im Vergleich zu einem leistungsstärkeren Basismodell?
    \item Wie unterscheidet sich der Ressourcenverbrauch (Energieverbrauch und \gls{VRAM}"=Auslastung) der \glspl{VLM} von dem der \gls{YOLO}/\gls{OCR}-Pipeline?
\end{enumerate}

Abbildung~\ref{fig:overview} zeigt einen Überblick der gesamten Methodik.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/overview}
    \caption{Überblick der Methodik}
    \label{fig:overview}
\end{figure}


\section{Modellierung des Dokumentenbestands als Grundlage der Evaluation}\label{sec:data_preprocessing}

\subsection{Preprocessing des Dokumentenbestands}\label{sec:preprocessing}

Durch Datenlieferungen der Familienkasse stehen für die Evaluation mehrere Tausend Dokumente als PDF oder Bilddatei zur Verfügung.
Um einen einheitlichen Dokumentenkorpus zu schaffen und die Klassifikations- und Extraktionsleistung zu verbessern, erfolgt eine Vorverarbeitung der Dokumente.
Dieser Prozess ist in Abbildung~\ref{fig:overview} in Rot dargestellt.

Im Fokus des Preprocessings stehen die Korrektur der Bildausrichtung sowie die Konvertierung der PDF-Dokumente in Bilddateien.
Zur Korrektur der Rotation werden die Exchangeable Image File Format (\gls{EXIF})-Metadaten ausgelesen, um die ursprüngliche Ausrichtung wiederherzustellen.
Während ältere Modelle Bilder auf eine feste Dimension skalieren, unterstützen die in dieser Arbeit evaluierten Modelle eine dynamische Skalierung.
Zur Begrenzung der maximalen Token-Anzahl werden die Bilder dennoch auf eine maximale Pixelanzahl skaliert, wobei das ursprüngliche Seitenverhältnis beibehalten wird.
Listing~\ref{lst:preprocessing} zeigt die Korrektur der Bildausrichtung und Skalierung innerhalb des Preprocessings.

\begin{listing}[H]
    \caption{Preprocessing der Dokumente}
    \label{lst:preprocessing}
    \inputminted[]{python}{listings/preprocessing.py}
\end{listing}


\subsection{Modellierung der Dokumentenarten als JSON-Objekte}\label{subsec:documents_as_json}

Für die standardisierte Weiterverarbeitung der Modellausgaben ist eine konsistente Struktur essenziell.
Da moderne \glspl{VLM} darauf trainiert sind, Antworten im JavaScript Object Notation (\gls{JSON})-Format zu liefern, werden die extrahierten Informationen in ein vordefiniertes \gls{JSON}-Schema überführt.

Ein wesentlicher Vorteil gegenüber der herkömmlichen Pipeline besteht in der simultanen Klassifikation sowie der Information Extraction (\gls{IE}) in einem einzigen Inferenzschritt.
Die \gls{JSON}-Schemata für die relevanten Dokumentenarten sind in den Listings~\ref{lst:json_kg5b} und~\ref{lst:json_vertrag} dargestellt.
Bei dem \gls{JSON}-Schema der sonstigen Dokumente in Listing~\ref{lst:json_sonstiges} findet nur die Klassifikation statt, da hier keine Informationen relevant sind.
Die Felder der \gls{JSON}-Schemata geben die relevanten Informationen aus Abschnitt~\ref{sec:documenttypes} wieder.

\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps KG5b}
    \label{lst:json_kg5b}
    \inputminted[]{JSON}{listings/kg5b.json}
\end{listing}

\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps Ausbildungsvertrag}
    \label{lst:json_vertrag}
    \inputminted[]{JSON}{listings/vertrag.json}
\end{listing}

\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps Sonstiges}
    \label{lst:json_sonstiges}
    \inputminted[]{JSON}{listings/sonstiges.json}
\end{listing}

Die Klassifikation wird im \gls{JSON}-Objekt über das Feld \texttt{type} abgebildet und ist in allen \gls{JSON}-Schemata vorhanden.
Felder der Information Extraction sind über die Dokumentenarten nicht konsistent, jedoch bildet das Schema eines Vertrags eine Teilmenge des Schemas eines KG5b-Formulars.
Das Schema des KG5b-Formulars beinhaltet zusätzlich die Felder \texttt{apprenticeship\_finished} und \texttt{exam\_month}.
\texttt{apprenticeship\_finished} ist ein binäres Feld und bildet die Checkboxen auf der ersten Seite des Formulars ab (siehe Abbildung~\ref{fig:kg5bpage1}).

Bei den binären, booleschen Feldern markiert ein \texttt{true} das Vorhandensein eines Merkmals, während ein \texttt{false} dessen Fehlen oder das Nicht-Erkennen repräsentiert.
Um eine strukturelle Konsistenz zu erzwingen, werden Felder, bei denen die dazugehörige Information im Dokument fehlt, bei booleschen Typen mit \texttt{false} und bei Textfeldern mit einer leeren Zeichenkette belegt.

Da die Dokumente in den meisten Fällen aus mehreren Seiten bestehen, repräsentiert ein \gls{JSON}-Objekt das mehrseitige Dokument.


\subsection{Test- und Validierungsdatensatz}\label{subsec:test_val_dataset}

Die Erstellung des Test- sowie des Validierungsdatensatzes ist im methodischen Überblick (Abbildung~\ref{fig:overview}) orange hervorgehoben und schließt unmittelbar an die Vorverarbeitung der Dokumente an.
Beide Datensätze umfassen jeweils 60 Dokumente, aufgeteilt in 20 Beispiele pro Dokumentenart.
Um die reale Datenverteilung bestmöglich abzubilden, werden die Dokumente manuell ausgewählt.

Die Datensätze decken dabei verschiedene Herausforderungen ab.
Neben Dokumenten mit geringer Qualität, vorrangig mangelhaften Scans und Fotos, befinden sich fehlerhaft ausgefüllte Dokumente sowie Dokumente mit fehlenden Unterschriften und Stempeln in den Datensätzen.
Um die hohe Varianz der Verträge und sonstigen Dokumente abzubilden, enthalten die Datensätze viele unterschiedliche Layouts.
Die Exemplare für die Verträge beinhalten sowohl starre Layouts als auch Fließtexte unterschiedlicher Firmen und Kammern.
Die sonstigen Dokumente beinhalten keine kontextfremden Dokumente, sondern Schulbescheinigungen, Studienbescheinigungen und Eintragungen bei der Handwerkskammer, um die Komplexität der Unterscheidung zu erhöhen.

Zur Erstellung der Wahrheitswerte (Ground Truth) werden alle Dokumente manuell annotiert und in das entsprechende \gls{JSON}-Schema überführt.

Die bestehende YOLO/OCR-Pipeline wird mit dem Testdatensatz getestet und die Metriken bestimmt.


\section{Evaluation der generierten JSON-Objekte}\label{sec:evaluation}

\subsection{Validierung des VLM-Outputs}\label{subsec:validate}

Um eine korrekte Formatierung des \gls{JSON}-Outputs der \glspl{VLM} sicherzustellen, existieren verschiedene Ansätze.
Während komplexere Lösungen nur Tokens zulassen, die syntaktisch in \gls{JSON}-Objekten enthalten sein könnten (Constrained Decoding), wird in dieser Evaluation ein direkterer Ansatz gewählt.
Hierbei werden aus der generierten Antwort potenzielle \gls{JSON}-Objekte extrahiert und geparst.
Schlägt das Parsen der \gls{JSON}-Objekte fehl, werden diese zurück an das \gls{VLM} geliefert, mit der Anweisung, diese zu korrigieren.
Das Modell bekommt insgesamt drei Versuche, ein valides \gls{JSON}-Objekt zu generieren, bevor das Dokument als ungültig eingestuft wird.


\subsection{Vergleichslogik der unterschiedlichen JSON-Felder}\label{subsec:comparator}

Im Gegensatz zur Verarbeitung von Antworten herkömmlicher Modelle weisen \glspl{VLM} eine hohe Varianz an Ausgabeformaten auf.
Verschiedene Feldtypen der generierten \gls{JSON}-Objekte müssen deshalb spezifisch normalisiert werden, um eine Grundlage für den Vergleich zu bilden.
Des Weiteren müssen Feldtypen wie Namen, Booleans, Zeichenketten, Datumsangaben und Monate nach unterschiedlichen Logiken verglichen werden.
Für den Vergleich werden auf Basis der Klasse in Listing~\ref{lst:comparator} verschiedene Komparatoren (Comparator) definiert.

\begin{listing}[H]
    \caption{Basisklasse der verschiedenen Comparator}
    \label{lst:comparator}
    \inputminted[]{python}{listings/comparator.py}
\end{listing}

Der \texttt{NameComparator} in Listing~\ref{lst:name_comparator} vergleicht die Namensfelder mithilfe der Levenshtein-Similarity, um kleinere Fehler und verschiedene Schreibweisen zu tolerieren.
Beispielsweise wird ein Name, der \texttt{ue} statt \texttt{ü} enthält, nicht als Fehler gewertet.
Der Threshold, der zwischen korrekt und nicht korrekt entscheidet, liegt bei 0,8.
Da die \gls{YOLO}/\gls{OCR}-Pipeline denselben Vergleich mit identischem Threshold verwendet, wird eine Vergleichbarkeit der Ergebnisse gewährleistet.

\begin{listing}[H]
    \caption{Comparator für Namen}
    \label{lst:name_comparator}
    \inputminted[]{python}{listings/name_comparator.py}
\end{listing}

Der \texttt{BooleanComparator} normalisiert boolesche Felder entsprechend der Python-Syntax zu \texttt{True} und \texttt{False}.
Zu den akzeptierten Varianten gehören sowohl \texttt{true} und \texttt{false}, \texttt{0} und \texttt{1}, \texttt{wahr} und \texttt{falsch} als auch \texttt{vorhanden} und \texttt{nicht vorhanden}.

Zeichenketten, die keine Namen darstellen, werden durch den \texttt{ExactComparator} verglichen.
Hierbei findet keine Normalisierung statt, sodass die Zeichenketten exakt übereinstimmen müssen.

Datumsangaben werden durch den \texttt{DateComparator} einheitlich in das Format \texttt{DD.MM.YYYY} überführt.
Obwohl dies nicht dem internationalen Standard entspricht, erweist sich dieses Vorgehen als am robustesten, da die vorliegenden deutschen Dokumente primär in diesem Format ausgefüllt sind.

Der \texttt{MonthComparator} normalisiert alphanumerische Monate in eine numerische Darstellung.
Falls vollständige Datumsangaben geliefert werden, wird der \texttt{DateComparator} genutzt und anschließend der Monat extrahiert.

Um die verschiedenen Comparator den Feldern zuzuordnen, existiert für jede Dokumentenart eine Konfiguration, die dem jeweiligen \gls{JSON}-Schema aus Abschnitt~\ref{subsec:documents_as_json} entspricht.
Das Schema~\ref{lst:scheme_kg5b} definiert beispielhaft die Konfiguration für das KG5b-Formular.

\begin{listing}[H]
    \caption{Konfiguration des KG5b-Formulars}
    \label{lst:scheme_kg5b}
    \inputminted[]{python}{listings/kg5b_scheme.py}
\end{listing}


\subsection{Bewertung der Klassifikation und der Information Extraction}\label{subsec:evaluation_classification_ie}

Um die generierten \gls{JSON}-Objekte der \glspl{VLM} schlussendlich bewerten zu können, werden die Klassifikation und Information Extraction unabhängig voneinander bewertet.
Für die Evaluation werden die einzelnen \gls{JSON}-Felder jedes Dokuments mithilfe der in der Konfiguration definierten Comparator mit der Ground Truth verglichen und verschiedene Metadaten erfasst.
Die Metadaten beinhalten den Status (vorhanden, nicht vorhanden oder halluziniert), die Korrektheit sowie die Fehlerart des Feldes.
Zu den Fehlerarten gehören Wert- und Formatfehler.
Halluzinierte Felder sind definiert als Felder, die nicht in der jeweiligen Konfiguration beziehungsweise Schema vorkommen.
Das Ergebnis ist ein Python-Dictionary (Listing~\ref{lst:dict}), das für jedes \gls{JSON}-Objekt eines Dokuments die Metadaten für jedes Feld beinhaltet.

\begin{listing}[H]
    \caption{Ergebnis-Dictionary mit den Metadaten des gesamten Dokumentenkorpus}
    \label{lst:dict}
    \inputminted[]{python}{listings/result_dict.py}
\end{listing}

Auf Basis dieses Dictionaries wird für die Bewertung der Klassifikation aus jedem Dokument das Feld \texttt{type} extrahiert.
Da das Feld \texttt{type} in jeder \gls{JSON} der Dokumentenarten (siehe Listings~\ref{lst:json_kg5b},~\ref{lst:json_vertrag} und~\ref{lst:json_sonstiges}) vorkommt, kann es nicht halluziniert werden.
Jedoch kann das Feld fehlen, weshalb neben den Klassen KG5b, Vertrag und Sonstiges eine weitere Klasse \texttt{missing} eingeführt wird.
\texttt{Missing} wird in der Auswertung nur betrachtet, wenn mindestens ein Dokument in der Klasse vorkommt.
Für die Bestimmung der Güte der Modelle wird eine Confusion-Matrix erstellt, wobei der Fokus auf dem F1-Score liegt.

Die Bewertung der Information Extraction ist komplexer als die der Klassifikation.
Um den Entity-Level F1-Score, der die Hauptmetrik für den Vergleich darstellt, zu berechnen, werden die Metadaten aller Felder außer dem Feld \texttt{type} benötigt.

Auf Basis dieser Metadaten werden für den gesamten Dokumentenkorpus die True Positives (\gls{TP}), False Positives (\gls{FP}) und False Negatives (\gls{FN}) gezählt.
Ein Feld zählt als \gls{TP}, wenn es vorhanden sowie korrekt ist.
Zu den \gls{FN} gehören Felder, die entweder fehlen oder vorhanden, aber falsch sind.
Ein Feld, das vorhanden, aber falsch ist, zählt zusätzlich zu den \gls{FP}, zusammen mit den Feldern, die halluziniert werden.
Diese doppelte Bestrafung stellt eine sehr strenge Bewertung für inhaltliche Fehler dar.
True Negatives (\gls{TN}) sind nicht zählbar, da die Menge an nicht gefundenen, leeren Feldern theoretisch unendlich groß ist.
Durch die Schemata und die Ground Truths ist eindeutig feststellbar, welche Felder halluziniert sind.
Da mithilfe der Schemata auch ohne Ground Truth die benötigten Felder gefiltert werden können, kann wie in Listing~\ref{lst:penalize_halluzinations} entschieden werden, ob die Halluzinationen generell bestraft werden.

\begin{listing}[H]
    \caption{Parameter, der bestimmt, ob Halluzinationen bestraft werden}
    \label{lst:penalize_halluzinations}
    \inputminted[]{python}{listings/penalize_halluzinations.py}
\end{listing}

\section{Messung von Latenz, Energieverbrauch und VRAM-Auslastung}\label{sec:measure}

Im Hinblick auf die Bewertung der Modelle werden neben der Güte der Klassifikation und Information Extraction die \gls{Latenz}, die \gls{VRAM}-Auslastung sowie der Energieverbrauch gemessen.

Die \gls{Latenz} der Modelle stellt lediglich das Delta zwischen Start- und Endzeit der Inferenz dar.
Hierbei werden die Zeitpunkte gemessen, an denen die eigentliche Inferenz startet und die Antwort bereitsteht.
Das Laden des Modells wird nicht betrachtet, da dieses keinen Einfluss auf die Antwortzeit in der produktiven Umgebung hat und folglich nicht relevant für die Evaluation ist.

Des Weiteren wird der Energieverbrauch während der Inferenz in Joule erfasst.
Die Messung der Leistung in Watt ist nicht zielführend, da sie die Zeit nicht berücksichtigt und Modelle mit geringerer \gls{Latenz} bei vergleichbarem Energieverbrauch benachteiligt werden.

Ebenso wie der Energieverbrauch wird der dynamische \gls{VRAM}-Verbrauch während der Inferenz gemessen.
Mit der Initialisierung des Modells wird zudem einmalig der statische Verbrauch erfasst.

\newpage
\section{Auswahl des Basismodells}\label{sec:model_selection}

Die Auswahl eines geeigneten Basismodells ist ein wichtiger Schritt für das spätere Fine-Tuning.
Wie in Abbildung~\ref{fig:overview} (grün markierte Felder) dargestellt, erfolgt diese Auswahl in einem iterativen Prozess.
Ziel ist es, das Modell zu identifizieren, das bereits ohne Training die beste Leistung sowohl bei der Klassifikation als auch bei der Information Extraction erreichen kann.

In jeder Iteration werden die Modelle Pixtral-12B und Qwen-2.5-VL-7B mit dem Validierungsdatensatz~\ref{subsec:test_val_dataset} evaluiert.
Die Temperatur der Modelle wird auf null gesetzt, um Halluzinationen zu vermeiden.
Jede Iteration gliedert sich in folgende Schritte:

\begin{enumerate}
    \item Evaluation: Durchführung der Inferenz beider Modelle mit dem Validierungsdatensatz.
    \item Fehleranalyse: Untersuchung der Fehler in der Klassifikation und in der Information Extraction.
    \item Prompt-Refinement: Anpassung des Prompts auf Basis der Fehleranalyse.
\end{enumerate}

Der Prozess wird so lange durchlaufen, bis durch Änderungen des Prompts keine Steigerungen der Metriken mehr erzielt werden.
Im Anschluss werden mit dem finalen Prompt und dem Testdatensatz~\ref{subsec:test_val_dataset} die finalen Metriken bestimmt und auf Basis dieser das Modell gewählt.
Die Trennung von Validierungs- und Trainingsdatensatz schließt ein Data Leakage aus und verhindert ein Overfitting auf den Testdaten.

Bezogen auf das Prompt-Engineering wird bewusst ein Zero-Shot-Ansatz gewählt.
Im Gegensatz zu One-Shot- und Few-Shot-Prompting werden hier weniger Tokens verbraucht, was die Kosten pro Inferenz senkt.
Des Weiteren kann das Modell so schneller antworten, da es weniger Input-Tokens verarbeiten muss.

Die Gestaltung des finalen Prompts (siehe Listing~\ref{lst:prompt}) folgt der R-K-F-Formel.
Während der Kontext die vorliegenden Dokumentenarten beschreibt und die Schemata definiert, gibt das Format die Ausgabeformatierung vor.
Zum Einsparen von Tokens wird das Modell strikt angewiesen, lediglich das \gls{JSON}-Objekt zu generieren.

Der hier evaluierte Prompt wird im Laufe der Arbeit auch für die Bewertung des Qwen-2.5-VL-32B genutzt.

\begin{listing}[H]
    \caption{Finaler Prompt der Modellauswahl}
    \label{lst:prompt}
    \inputminted[]{python}{listings/prompt.py}
\end{listing}


\section{Fine-Tuning des Basismodells}\label{sec:fine_tuning}

\subsection{Trainingsdatensatz}\label{subsec:train_dataset}

Um ein qualitativ hochwertiges Fine-Tuning zu ermöglichen, ist ein deutlich umfangreicherer Datensatz als für die Modellauswahl erforderlich.
Der finale Trainingsdatensatz umfasst 610 Dokumente, die sich aus 227 Verträgen, 165 KG5b-Formularen und 218 sonstigen Dokumenten zusammensetzen.

Die Klassen sind bewusst ungleich verteilt.
KG5b-Formulare haben ein starres Layout, wodurch schon mit einer geringeren Anzahl an Trainingsdaten ein gutes Ergebnis erwartet wird.
Infolge der hohen Varianz der Verträge und der sonstigen Dokumente erhalten diese im Training eine höhere Gewichtung.
Anders als beim Testdatensatz werden hier die Exemplare nicht manuell, sondern mithilfe einer Stichprobe ausgewählt.
Dabei werden die Daten aus einem einwöchigen Zeitraum betrachtet, wodurch ohne manuelle Auswahl eine genaue Repräsentation der realen Daten entsteht.
Des Weiteren wird so ein Selection Bias verhindert.

Zur effizienten Erstellung der Ground Truth wird ein Model-Assisted Labeling eingesetzt.
Hierbei generiert das im Prozess der Modellauswahl~\ref{sec:model_selection} gewonnene Modell, zusammen mit dem angepassten Prompt, für jedes Dokument das jeweilige \gls{JSON}-Objekt.
Im Anschluss werden diese \gls{JSON}-Objekte manuell kontrolliert und korrigiert.
Durch den Einsatz des Modells zur Ermittlung der Ground Truth kann der Aufwand erheblich gesenkt werden.

Für das Fine-Tuning muss der Datensatz in das \gls{OpenAI Message Format} (Listing~\ref{lst:messages}) gebracht werden.
Des Weiteren müssen die \gls{JSON}-Objekte in Markdown-Blöcke eingebettet werden, um dem bereits gelernten Format des \gls{VLM} zu entsprechen.

\begin{listing}[H]
    \caption{OpenAI Message Format}
    \label{lst:messages}
    \inputminted[]{python}{listings/messages.py}
\end{listing}


\subsection{Optimierung des Fine-Tunings}\label{subsec:optimise_fine_tuning}

Das in Abschnitt~\ref{sec:model_selection} gewählte Basismodell wird mittels \gls{PEFT} an die Dokumentenarten angepasst.
Dabei kommen \gls{LoRA} und die stabilisierte Variante \gls{rsLoRA} zum Einsatz.
Für das Training wird auf das Framework \gls{Unsloth} gesetzt, um den Ressourcenverbrauch gering zu halten.
Des Weiteren wird das Modell in 4-Bit-Quantisierung geladen, um die \gls{VRAM}-Auslastung weiter zu senken.

Der Systemprompt in Listing~\ref{lst:prompt_finetuning} ist pragmatisch gehalten, damit das Modell anhand von Beispielen statt durch Instruktion lernt.

\begin{listing}[H]
    \caption{Prompt für das Fine-Tuning des Qwen-2.5-VL-7B}
    \label{lst:prompt_finetuning}
    \inputminted[]{python}{listings/prompt_finetuning.py}
\end{listing}

Der Trainingsdatensatz~\ref{subsec:train_dataset} wird mithilfe von Datenaugmentation an das Training angepasst.
Vor jedem Trainingslauf wird der Datensatz vollständig randomisiert, um eine zufällige Reihenfolge der Trainingsdaten zu erhalten.
Darüber hinaus werden die Seiten der Dokumente zufällig getauscht, um dem Modell anzutrainieren, Informationen unabhängig von der Position zu extrahieren.

Die Parameteroptimierung erfolgt iterativ, wie im blau markierten Bereich in Abbildung~\ref{fig:overview} zu erkennen ist, und es werden alle Komponenten des \gls{VLM} trainiert.
Die wichtigsten Parameter der finalen Konfiguration sind in Listing~\ref{lst:parameter} abgebildet.

Um ein Overfitting zu vermeiden, wird neben dem Trainingsdatensatz auch hier der Validierungsdatensatz~\ref{subsec:test_val_dataset} verwendet.
Nach 80 Dokumenten wird mit dem Validierungsdatensatz der Trainingsfortschritt gemessen.
Am Ende des Trainingslaufs wird das Modell geladen, das den geringsten Loss gegenüber dem Validierungsdatensatz hat.

\begin{listing}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \inputminted[]{python}{listings/model.py}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \inputminted[]{python}{listings/config_trainer.py}
    \end{minipage}
    \caption{Konfiguration des Fine-Tunings}
    \label{lst:parameter}
\end{listing}

Abschließend wird wie bei der Auswahl des Basismodells mit dem Testdatensatz~\ref{subsec:test_val_dataset} die finalen Metriken bestimmt.
Die trainierten \gls{LoRA}-Adapter werden zur Laufzeit geladen, um Speicherplatz zu sparen, da sonst das Qwen-2.5-VL-7B zusammen mit jedem Adapter gespeichert werden müsste.
