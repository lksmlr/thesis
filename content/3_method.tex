\chapter{Methodik und Implementierung}\label{ch:data}

Im Rahmen dieses Proof of Concepts werden verschiedene \glspl{VLM} vergleichend gegenüber der bestehenden \gls{YOLO}/\gls{OCR}-Pipeline evaluiert.
Für die Entscheidung, ob eines der \glspl{VLM} die \gls{YOLO}/\gls{OCR}-Pipeline ersetzt, sollen die folgenden Forschungsfragen beantwortet werden:

\begin{enumerate}
    \item Inwiefern ist ein nicht domänenspezifisch angepasstes \gls{VLM} hinsichtlich der Extraktionsgüte und Klassifikationsgenauigkeit der trainierten \gls{YOLO}/\gls{OCR}-Pipeline überlegen, und welche Vorteile bietet ein einzelnes generalistisches Modell gegenüber der Verwendung spezialisierter Einzelsysteme?
    \item Wie verhält sich ein kleineres, domänenspezifisch trainiertes Modell gegenüber einem leistungsstärkeren Basismodell in Bezug auf Performanz und Effizienz?
    \item In welchem Verhältnis steht der Ressourcenverbrauch der \glspl{VLM} zu dem der \gls{YOLO}/\gls{OCR}-Pipeline?
\end{enumerate}

In Abbildung~\ref{fig:overview} ist ein Überblick der gesamten Methodik abgebildet.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/overview}
    \caption{Überblick der Methodik.}
    \label{fig:overview}
\end{figure}

\section{Modellierung des Dokumentenbestands als Grundlage der Evaluation}\label{sec:data_preprocessing}


\subsection{Preprocessing und Darstellung der Dokumentenarten als JSON-Objekte}\label{sec:documents_as_json}

Durch Datenlieferungen der Familienkasse stehen mehrere Tausend Dokumente als PDF oder Bilddatei zur Verfügung.
Um eine einheitliche Basis zu schaffen und die Klassifikations- und Extraktionsgüte zu verbessern, müssen die Daten vorverarbeitet werden.
Dieser Prozess ist in Abbildung~\ref{fig:overview} in Rot dargestellt.
Im Fokus stehen dabei die Korrektur der Bildausrichtung sowie die Konvertierung der PDF-Dokumente in Bilder.
Zur Korrektur der Rotation werden die Exchangeable Image File Format (\gls{EXIF})-Metadaten ausgelesen, um die ursprüngliche Ausrichtung wiederherzustellen.
Während ältere \glspl{VLM} Bilder oft auf fixe Größe skalieren, unterstützen die in dieser Arbeit evaluierten Modelle eine dynamische Skalierung.
Um jedoch die maximale Token-Anzahl, werden die Bilder auf eine maximale Pixelanzahl skaliert, wobei das ursprüngliche Seitenverhältnis beibehalten wird.
Das Listing~\ref{lst:preprocessing} zeigt einen Ausschnitt des Preprocessings.

\begin{listing}[H]
    \caption{Preprocessing der Dokumente}
    \label{lst:preprocessing}
    \inputminted[]{python}{listings/preprocessing.py}
\end{listing}


Für die standardisierte Weiterverarbeitung der Modellausgaben ist eine feste Struktur essenziell.
Da moderne \glspl{VLM} darauf trainiert sind, Antworten im JavaScript Object Notation (\gls{JSON})-Format zu liefern, werden die extrahierten Informationen in ein vordefiniertes \gls{JSON}-Schema überführt.
Ein wesentlicher Vorteil gegenüber der herkömmlichen Pipeline ist die gleichzeitige Durchführung der Klassifikation sowie der \gls{IE} in einem einzigen Inferenzschritt.
Das Modell klassifiziert die Dokumentenart, die im Feld \texttt{type} abgebildet wird.
Während das Feld \texttt{type} in allen \gls{JSON}-Schemata konsistent vorhanden ist, variieren die Felder der \gls{IE} je nach Dokumentenart.

Bei den binären, booleschen Feldern markiert ein \texttt{true} das Vorhandensein eines Merkmals, während ein \texttt{false} dessen Fehlen oder das Nicht-Erkennen repräsentiert.
Um eine strukturelle Konsistenz zu erzwingen, werden Felder, bei denen die dazugehörige Information im Dokument fehlt, bei booleschen Typen mit \texttt{false} und bei Textfeldern mit einer leeren Zeichenkette belegt.

Da die Dokumente in den meisten Fällen aus mehreren Seiten bestehen, repräsentiert ein \gls{JSON}-Objekt das mehrseitige Dokument.
Sollten Dokumente verschiedener Arten vermischt vorliegen, generiert das Modell für jede erkannte Dokumentenart ein separates \gls{JSON}-Objekt.

Die \gls{JSON}-Schemata für die relevanten Dokumentenarten sind in den Schemata~\ref{lst:json_kg5b} und~\ref{lst:json_vertrag} dargestellt.
Bei dem \gls{JSON}-Schema der sonstigen Dokumente~\ref{lst:json_sonstiges}, findet nur die Klassifikation statt, da hier keine Informationen relevant sind.

\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps KG5b}
    \label{lst:json_kg5b}
    \inputminted[]{JSON}{listings/kg5b.json}
\end{listing}


\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps Ausbildungsvertrag}
    \label{lst:json_vertrag}
    \inputminted[]{JSON}{listings/vertrag.json}
\end{listing}


\begin{listing}[H]
    \caption{JSON-Schema des Dokumententyps Sonstiges}
    \label{lst:json_sonstiges}
    \inputminted[]{JSON}{listings/sonstiges.json}
\end{listing}


\subsection{Test- und Validierungsdatensatz}\label{subsec:test_val_dataset}

Die Erstellung des Test- und Validierungsdatensatz, befindet sich im Überblick~\ref{fig:overview} in Orange dargestellt nach der Vorbereitung der Dokumente.
Der Testdatensatz als auch der Validierungsdatensatz umfasst insgesamt 60 Dokumente, aufgeteilt in jeweils 30 Beispiele Dokumentenarten.
Um die reale Datenverteilung bestmöglich abzubilden, werden die Dokumente manuell ausgewählt.

Die Datensätze deckt dabei verschiedene Herausforderungen ab.
Neben Dokumenten mit geringer Qualität, vorrangig schlechte Scans und Fotos, befinden sich fehlerhaft ausgefüllte Dokumente sowie Dokumente mit fehlenden Unterschriften und Stempeln in den Datensätzen.
Um die hohe Varianz der Verträge und sonstigen Dokumente abzubilden, werden in den Datensätzen viele unterschiedliche Layouts gewählt.
Die Exemplare für die Verträge beinhalten sowohl starre Layouts als auch Fließtexte unterschiedlicher Firmen und Kammern.
Die sonstigen Dokumente beinhalten keine kontextfremden Dokumente, sondern Schulbescheinigungen, Studienbescheinigungen und Eintragungen bei der Handwerkskammer, um eine zusätzliche Herausforderung zu stellen.

Zur Erstellung der Wahrheitswerte (Ground Truth) werden alle Dokumente manuell annotiert und in das entsprechende \gls{JSON}-Format überführt.


\section{Evaluation der generierten JSON-Objekte}\label{sec:evaluation}

\subsection{Validierung des VLM-Outputs}\label{subsec:validate}

Um den \gls{JSON}-Output der \glspl{VLM} zu limitieren, existieren verschiedene Ansätze.
Während komplexere Lösungen nur Tokens zulassen die in \gls{JSON}-Objekten enthalten sein könnten, wird in dieser Evaluation ein einfacherer Ansatz gewählt.
Hierbei werden aus der generierten Antwort \glspl{JSON} gefiltert und geparst.
Sobald das Parsen der \gls{JSON} fehlschlägt, wird diese zurück an das \gls{VLM} geliefert, mit der Anweisung diese zu korrigieren.
Das Modell bekommt insgesamt drei Versuche eine valide \gls{JSON} zu generieren, bevor das Dokument als ungültig eingestuft wird.


\subsection{Vergleichslogik der unterschiedlichen JSON-Felder}\label{subsec:comparator}

Im Gegensatz zur Verarbeitung von Antworten herkömmlicher Modelle haben \glspl{VLM} eine hohe Varianz an Ausgabeformaten.
Verschiedene Feldtypen der generierten \gls{JSON} müssen deshalb spezifisch normalisiert werden, um eine Grundlage für den Vergleich zu bilden.
Des Weiteren müssen die Feldtypen wie Namen, Booleans, Zeichenketten, Daten und Monate unterschiedlich verglichen werden.
Für den Vergleich werden auf Basis der Klasse in Listing~\ref{lst:comparator} verschiedene Komparatoren (Comparator) definiert.

\begin{listing}[H]
    \caption{Basisklasse der verschiedenen Comparator}
    \label{lst:comparator}
    \inputminted[]{python}{listings/comparator.py}
\end{listing}

Der NameComparator in Listing~\ref{lst:name_comparator} vergleicht die Namensfelder mithilfe der Levenshtein-Similarity, um kleinere Fehler und verschiedene Varianten zu tolerieren.
Beispielsweise wird ein Name, der \texttt{ue} statt \texttt{ü} enthält, nicht als Fehler erkannt.
Der Threshold, der zwischen korrekt und nicht korrekt entscheidet, liegt bei 0,8.
Die \gls{YOLO}/\gls{OCR}-Pipeline verwendet ebenso diesen Vergleich mit dem gleichen Threshold, wodurch das Ergebnis entsprechend wird.

\begin{listing}[H]
    \caption{Comparator für Namen}
    \label{lst:name_comparator}
    \inputminted[]{python}{listings/name_comparator.py}
\end{listing}

Der BooleanComparator normalisiert boolesche Felder nach Python-Syntax zu \texttt{True} und \texttt{False}.
Zu den akzeptierten Varianten gehören sowohl \texttt{true} und \texttt{false}, \texttt{0} und \texttt{1}, \texttt{wahr} und \texttt{falsch} als auch \texttt{vorhanden} und \texttt{nicht vorhanden}.

Zeichenketten, die keine Namen sind, werden durch den ExactComparator verglichen.
Hierbei findet keine Normalisierung statt und die Zeichenketten müssen exakt übereinstimmen.

Daten werden einheitlich im DateComparator in das Format DD.MM.YYYY gebracht, obwohl dies nicht dem internationalen Standard entspricht.
Das ist darauf zurückzuführen, dass so die Erkennung des \glspl{VLM} am robustesten ist, da die deutschen Dokumente auch mit diesem Format ausgefüllt werden.

Der MonthComparator normalisiert alphanumerische Monate in die numerische Darstellung.
In dem Fall, dass komplette Datumsangaben geliefert werden, wird der DateComparator benutzt und der Monat extrahiert.

Um die verschiedenen Comparator den Feldern zuzuordnen, gibt es für jede Dokumentenart eine Konfiguration das der jeweiligen \gls{JSON} ähnelt.
Das Schema~\ref{lst:scheme_kg5b} definiert beispielhaft die Konfiguration für das KG5b-Formular.

\begin{listing}[H]
    \caption{Konfiguration des KG5b-Formulars}
    \label{lst:scheme_kg5b}
    \inputminted[]{python}{listings/kg5b_scheme.py}
\end{listing}

\subsection{Bewertung der Klassifikation und der Information Extraction}\label{subsec:evaluation_classification_ie}

Um die generierten \gls{JSON}-Objekte des \glspl{VLM} schlussendlich bewerten zu können, werden die Klassifikation und \gls{IE} unabhängig voneinander bewertet.
Für die Bewertung werden die einzelnen \gls{JSON}-Felder jedes Dokuments mithilfe der in der Konfiguration definierten Comparator mit der Ground Truth verglichen und verschiedene Metadaten gesammelt.
Die Metadaten beinhalten den Status (vorhanden, nicht vorhanden oder halluziniert), die Korrektheit und die Fehlerart des Feldes.
Halluzinierte Felder sind Felder, die nicht in der jeweiligen Konfiguration vorkommen.
Das Ergebnis ist ein Python-Dictionary (Listing~\ref{lst:dict}) das für jedes Dokument die Metadaten für jedes Feld beinhaltet.

\begin{listing}[H]
    \caption{Ergebnis-Dictionary mit den Metadaten des gesamten Dokumentenkorpus}
    \label{lst:dict}
    \inputminted[]{python}{listings/result_dict.py}
\end{listing}

Auf Basis dieses Dictionaries wird für die Bewertung der Klassifikation aus jedem Dokument das Feld \texttt{type} extrahiert.
Da das Feld \texttt{type} in jeder \gls{JSON} der Dokumentenarten (siehe Abbildungen~\ref{lst:json_kg5b, lst:json_vertrag, lst:json_sonstiges}) vorkommt, kann es nicht halluziniert werden.
Jedoch kann das Feld fehlen, weshalb neben den Klassen KG5b, Vertrag und Sonstiges, eine weitere Klasse \texttt{missing} eingeführt wird.
Diese wird in der Auswertung aber nur betrachtet, wenn es ein Dokument in der Klasse gibt.
Für die Bestimmung der Güte der Modelle, wird eine Confusion-Matrix erstellt, wobei der Fokus auf dem F1-Score liegt.

Die Bewertung der Information Extraction ist komplexer als die der Klassifikation.
Um den Entity-Level F1-Score, der die Hauptmetrik für den Vergleich darstellt, zu berechnen werden die Metadaten aller Felder, außer des Feld \texttt{type}, benötigt.

Auf Basis dieser Metadaten werden für den gesamten Dokumentenkorpus die True Positive (\gls{TP}), False Positive (\gls{FP}) und False Negative (\gls{FN}) gezählt.
Ein Feld zählt als \gls{TP}, wenn es vorhanden sowie korrekt ist.
Zu den \gls{FN} gehören Felder, die entweder fehlen oder vorhanden, aber falsch sind.
Ein Feld, das vorhanden aber falsch ist, zählt zusätzlich zu den \gls{FP} zusammen mit den Feldern die halluziniert wurden.
Diese doppelte Bestrafung stellt eine sehr strenge Bewertung für inhaltliche Fehler dar.
True Negatives (\gls{TN}) sind nicht zählbar, da die Menge an nicht gefundenen, leeren Feldern, theoretisch unendlich groß ist.
Durch die Konfiguration und die Ground Truths ist eindeutig feststellbar welche Felder halluziniert sind.
Da mithilfe der Konfiguration auch ohne Ground Truth die benötigten Felder gefiltert werden können, kann wie in Listing~\ref{lst:penalize_halluzinations}entschieden werden, ob die Halluzinationen bestraft werden.

\begin{listing}[H]
    \caption{Parameter der bestimmt ob Halluzinationen bestraft werden}
    \label{lst:penalize_halluzinations}
    \inputminted[]{python}{listings/penalize_halluzinations.py}
\end{listing}

\section{Messung von Latenz, Energieverbrauch und VRAM-Nutzung}\label{sec:measure}

Im Hinblick auf die Bewertung der Modelle werden neben der Güte der Klassifikation und \gls{IE} die \gls{Latenz}, die \gls{VRAM}-Nutzung sowie der Energieverbrauch gemessen.

Die \gls{Latenz} der Modelle stellt lediglich das Delta zwischen Start- und Endzeit der Inferenz dar.
Hierbei wird der Zeitpunkt gemessen, an dem die eigentliche Inferenz startet und wenn die Antwort bereitsteht.
Das Laden des Modells wird nicht betrachtet, da diese keinen Einfluss auf die Antwortzeit in der produktiven Umgebung hat und folglich nicht relevant für die Evaluation ist.

Des Weiteren wird der Energieverbrauch während der Inferenz in Joule erfasst.
Die Messung der Leistung in Watt ist nicht zielführend, da sie die Zeit nicht berücksichtigt und Modelle mit geringerer \gls{Latenz} bei vergleichbarem Energieverbrauch benachteiligt werden.

Wie auch der Energieverbrauch wird die \gls{VRAM}-Nutzung während der Inferenz gemessen.
Der Messzeitraum für den Energieverbrauch als auch für die \gls{VRAM}-Nutzung ist gleich der \gls{Latenz} der Modelle.
Mit der Initialisierung des Modells wird einmalig der statische Verbrauch gemessen.


\section{Auswahl des Basismodells}\label{sec:model_selection}

Die Auswahl eines geeigneten Basismodells ist ein wichtiger Schritt für das spätere Fine-Tuning.
Wie in Abbildung~\ref{fig:overview} (grün markierte Felder) dargestellt, erfolgt diese Auswahl in einem iterativen Prozess.
Ziel ist es, das Modell zu identifizieren, das bereits ohne Training die beste Leistung bei der Klassifikation als auch \gls{IE} erreichen kann.

In jeder Iteration werden die Modelle Pixtral-12B und Qwen-2.5-VL-7B mit dem Validierungsdatensatz~\ref{subsec:test_val_dataset} evaluiert.
Die Iteration gliedert sich in folgende Schritte:

\begin{enumerate}
    \item Evaluation: Durchführung der Inferenz beider Modelle mit dem Validierungsdatensatz.
    \item Fehleranalyse: Untersuchung der Fehler in der Klassifikation und in der \gls{IE}.
    \item Prompt-Refinement: Anpassung des Prompt auf Basis der Fehleranalyse.
\end{enumerate}

Der Prozess wird so lange durchlaufen, bis durch Änderungen des Prompts keine Steigerungen der Metriken mehr erzielt wird.
Im Anschluss wird mit dem finalen Prompt und dem Testdatensatz~\ref{subsec:test_val_dataset} die finalen Metriken bestimmt und auf Basis dieser das Modell gewählt.
Die Trennung von Validierung- und Trainingsdatensatz schließt ein Data Leakage aus und verhindert ein Overfitting auf den Testdaten.

Bezogen auf das Prompt-Engineering wurde bewusst ein Zero-Shot-Ansatz gewählt.
Im Gegensatz zu One-Shot- und Few-Shot-Prompting werden hier weniger Tokens verbraucht, was die Kosten pro Inferenz senkt.
Des Weiteren kann das Modell so schneller antworten, da er weniger Input-Tokens verarbeiten muss.

Die Gestaltung des finalen Prompts (siehe Listing~\ref{lst:prompt}) folgt der R-K-F-Formel.
Während der Kontext die vorliegenden Dokumentenarten beschreibt und die Schemen definiert, gibt das Format die Ausgabeformatierung vor.
Zum Einsparen von Tokens wird das Modell strikt angewiesen lediglich das \gls{JSON}-Objekt zu generieren.

Der hier evaluierte Prompt wird im Laufe der Arbeit auch für das größere Qwen-2.5-VL-32B genutzt, jedoch war dieses Modell keine Option für das spätere Fine-Tuning, da das nicht Teil der Forschung ist.

\begin{listing}[H]
    \caption{Finaler Prompt der Modellauswahl}
    \label{lst:prompt}
    \inputminted[]{python}{listings/prompt.py}
\end{listing}


\section{Fine-Tuning des Basismodells}\label{sec:fine_tuning}

\subsection{Trainingsdatensatz}\label{subsec:train_dataset}

Um ein qualitativ hochwertiges Fine-Tuning zu ermöglichen, ist ein deutlich umfangreicherer Datensatz als für die Modellauswahl erforderlich.
Der finale Trainingsdatensatz umfasst 610 Dokumente, die sich aus 227 Verträgen, 165 KG5b-Formularen und 218 sonstigen Dokumenten zusammensetzen.

Die Klassen sind bewusst ungleich verteilt.
KG5b-Formulare haben ein starres Layout, wodurch schon mit einer geringeren Anzahl an Trainingsdaten ein gutes Ergebnis erwartet wird.
Infolge der hohen Varianz der Verträge und der sonstigen Dokumente erhalten diese im Training eine höhere Gewichtung.
Anders als beim Testdatensatz werden hier die Exemplare nicht manuell, sondern mithilfe einer Stichprobe ausgewählt.
Dabei werden die Daten aus einem einwöchigen Zeitraum betrachtet, wodurch ohne manuelle Auswahl eine genaue Repräsentation der realen Daten entsteht.
Des Weiteren wird so der Selektionsbias verhindert.

Zur effizienten Erstellung der Ground Truth wird ein Model-Assisted Labeling eingesetzt.
Hierbei generiert das im Prozess der Modellauswahl~\ref{sec:model_selection} gewonnene Modell, zusammen mit dem angepassten Prompt, für jedes Dokument das jeweilige \gls{JSON}-Objekt.
Im Anschluss werden diese \gls{JSON}-Objekte manuell kontrolliert und korrigiert.
Durch den Einsatz des Modells zur Ermittelung der Ground Truth konnte der Aufwand erheblich gesenkt werden.

Für das Fine-Tuning muss der Datensatz in das \gls{OpenAI Message Format} (Listing~\ref{lst:messages}) gebracht werden
Des Weiteren müssen die \gls{JSON}-Objekte in Markdown-Blöcke eingebettet werden, um dem bereits gelernten Format des \gls{VLM} zu entsprechen.

\begin{listing}[H]
    \caption{OpenAI Message Format}
    \label{lst:messages}
    \inputminted[]{python}{listings/messages.py}
\end{listing}

\subsection{Optimierung des Basismodells durch Fine-Tuning}\label{subsec:optimise_fine_tuning}

Das Basismodell aus~\ref{sec:model_selection} wurde mit mehreren Methoden des \gls{PEFT} (LoRA und rsLoRA) und dem Framework \gls{Unsloth} trainiert.
Die Optimierung der Parameter erfolgt iterativ wie im blau markierten Bereich in Abbildung~\ref{fig:overview} zu erkennen.

Der Systemprompt in Listing~\ref{lst:prompt_finetuning} ist einfach gehalten, damit das Modell durch Beispiele statt durch Instruktion lernt.

\begin{listing}[H]
    \caption{Prompt für das Fine-Tuning des Qwen-2.5-VL-7B}
    \label{lst:prompt_finetuning}
    \inputminted[]{python}{listings/prompt_finetuning.py}
\end{listing}

Ein weiterer wesentlicher Aspekt der Optimierung lag in der Augmentation des Datensatzes.
Vor jedem erneuten Trainingslauf wurde der Datensatz vollständig randomisiert, um eine zufällige Reihenfolge der Trainingsdaten zu erhalten.
Des Weiteren wurden in einigen der Trainingsläufe die Seiten der einzelnen Dokumente getauscht, um dem Modell zu lernen, die Informationen ohne Position zu extrahieren.

Um ein Overfitting zu vermeiden, wird neben dem Trainingsdatensatz auch hier der Validierungsdatensatz~\ref{subsec:test_val_dataset} verwendet.
Nach 80 Dokumenten wird mit dem Validierungsdatensatz der Trainingsfortschritt gemessen.
Am Ende des Trainings wird das Modell geladen, dass am besten auf dem Validierungsdatensatz funktioniert hat.

Die wichtigsten Parameter der finalen Konfiguration sind in Listing~\ref{lst:parameter} abgebildet.

\begin{listing}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \caption{LoRA-Konfiguration}
        \inputminted[]{python}{listings/model.py}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \caption{Trainingsparameter des Fine-Tunings}
        \inputminted[]{python}{listings/config_trainer.py}
    \end{minipage}
    \label{lst:parameter}
\end{listing}

Abschließend wurde wie bei der Auswahl des Basismodells mit dem Testdatensatz~\ref{subsec:test_val_dataset} die finalen Metriken bestimmt.
