\chapter{Ergebnisse}\label{ch:results}
% TODO: Penalize hallucinations false
\section{Ergebnisse der OCR/YOLO-Pipeline gegenüber den Basismodellen}\label{sec:results_ie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/results_ie}
    \caption{Ergebnisse der Information Extraction}
    \label{fig:results_ie}
\end{figure}

Abbildung~\ref{fig:results_ie} stellt die F1-Scores der Basismodelle im Vergleich zur OCR/YOLO-Pipeline für die KG5b-Formulare und Ausbildungsverträge dar.

Bei den KG5b-Formularen zeigt sich ein enges Feld der Qwen-Modelle und der OCR/YOLO-Pipeline.
Die Pipeline liegt mit einem F1-Score von 0,75 geringfügig hinter dem Qwen-2.5-VL-7B (0,80).
Das größere Qwen-2.5-VL-32B bildet die Spitze mit 0,87, während sich das Pixtral-12B am unteren Ende befindet.

Ein differenzierteres Bild zeigt sich bei den heterogenen Verträgen.
Die OCR/YOLO-Pipeline fällt signifikant auf einen F1-Score von 0,51 ab.
Demgegenüber stehen die Qwen-Modelle mit 0,94 (Qwen-2.5-VL-32B) und 0,84 (Qwen-2.5-VL-7B), die bei den Verträgen besser abschnitten als bei den KG5b-Formularen..
Das Pixtral-12B bildet mit 0,50 erneut das Schlusslicht, liegt jedoch nur minimal hinter der OCR/YOLO-Pipeline.

Die Ergebnisse der Information Extraction zeigen die Abhängigkeit der Leistungsfähigkeit von der Dokumentenart bei der OCR/YOLO-Pipeline.
Im Vergleich zu den Qwen-Modellen sinkt die Leistung bei den variablen Verträgen gegenüber den starren KG5b-Formularen signifikant.
Die Daten belegen zusätzlich, das bereits ein kleines Modell wie das Qwen-2.5-VL-7B die Pipeline übertrifft.
In keiner Dokumentenart schnitt das Pixtral-12B besser als die bisherige Lösung ab, weswegen es unbrauchbar für diese Anwendung ist.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification}
\end{figure}

Abbildung~\ref{fig:results_classification} visualisiert die Klassifikationsleistung der Basismodelle und der OCR/YOLO-Pipeline anhand von F1-Scores.

Im Gegensatz zu der Information Extraction zeigt sich hier ein homogenes Bild.
Das Qwen-2.5-VL-32B erzielt in allen drei Kategorien die besten Ergebnisse und klassifiziert die KG5b-Formulare sogar mit einem Score von 1,0.
Die Ergebnisse des Pixtral-12B, Qwen-2.5-VL-7B und die OCR/YOLO-Pipeline liegen über alle Dokumentenarten hinweg dicht beisamen.
In der Kategorie der sonstigen Dokumente sinken alle Modelle gegenüber den anderen Dokumentenarten ab.
Hier erzielte das Pixtral-12B zusammen mit dem Qwen-2.5-VL-7B den niedrigsten Score mit 0,87.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_7b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_pixtral}
        \caption{Confusion Matrix für das Pixtral-12B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_yolo_ocr}
        \caption{Confusion Matrix für die OCR/YOLO-Pipeline}
    \end{subfigure}

    \caption{Confusion Matrices aller Modelle}
    \label{fig:confusion_matrices}
\end{figure}

Die Confusion Matrices in Abbildung~\ref{fig:confusion_matrices} repräsentiert die Ergebnisse der Klassifikation im Detail.

Während die F1-Scores der Klassifikation ein ähnliches Leistungsniveau zeigen, offenbaren die Matrizen unterschiedliche Fehlerschwerpunkte.
Das Qwen-2.5-VL-32B klassifiziert sowohl die KG5b-Formulare als auch die Verträge fehlerfrei.
Im Gegensatz dazu erkennt die OCR/YOLO-Pipeline die sonstigen Dokumente perfekt, ordnete aber fünf der anderen Dokumentenarten dieser zusätzlich zu.
Das Qwen-2.5-VL-7B und das Pixtral-12B haben ein ähnliches Fehlerschema und erkannten die Verträge perfekt.
Allerdings klassifizierten beiden Modelle KG5b-Formulare sowie sonstige Dokumente häufig als Vertrag.

Die gemeinsame Betrachtung der F1-Scores und der Confusion Matrices verdeutlichen, dass trotz quantitativ vergleichbarer Ergebnisse die Modelle qualitative Unterschiede aufweisen.
Besonders die Stabilität des Qwen-2.5-VL-32B ist hervorzuheben, da die relevanten Dokumente fehlerfrei erkannt werden.
Gegenüber stehen die kleineren VLMs, die Probleme mit der Trennung der Dokumentenklassen haben.
Die OCR/YOLO-Pipeline neigt zur Zuordnung zu den sonstigen Dokumenten, wodurch relevante Dokumente verloren gehen.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency}
    \caption{Ergebnisse der Latenzmessung}
    \label{fig:results_latency}
\end{figure}

Das Diagramm~\ref{fig:results_latency} verdeutlicht den Zusammenhang zwischen der Größe des Modells und dessen \gls{Latenz}.
Aufgrund der Architektur antwortet die OCR/YOLO-Pipeline mit einer durchschnittlichen \gls{Latenz} von 0,58 Sekunden nahezu in Echtzeit.
Gegenüber der Pipeline braucht selbst das kleinste Modell mit 7,43 Sekunden fast 13-mal länger.
Besonders auffällig ist die hohe maximale \gls{Latenz} des Qwen-2.5-VL-32B mit 252 Sekunden, womit dieses Modell deutlich über den anderen liegt.

Die \gls{Latenz}messung belegt den Zusammenhang der Modellgröße von VLMs mit dessen Antwortgeschwindigkeit.
Mit zunehmender Parameteranzahl wächst die \gls{Latenz} überproportional.


\section{Ergebnisse des weitertrainierten Modells gegenüber dem größeren Basismodell}\label{sec:results_latency}

Der Prozess der Auswahl des Basismodells~\ref{subsec:model_selection} lieferte ein eindeutiges Ergebnis.
Wie in Abbildung~\ref{fig:results_ie} und Abbildung~\ref{fig:results_classification} schlägt das Qwen-2.5-VL-7B das Pixtral-12B.
Besonders in der \gls{IE} stellt sich das Qwen-2.5-VL-7B als der bessere Kandidat für das Training heraus.
Demnach wird im folgenden Abschnitt das trainierte Qwen-2.5-VL-7B (Qwen-2.5-VL-7B-finetuned) gegen das Qwen-2.5-VL-32B verglichen.

Trotz ähnlicher Ergebnisse zeigt das Qwen-2.5-VL-32B auch gegenüber dem angepassten Modell seine Stärken.
Wie in Abbildung~\ref{fig:results_ie_peft_32b} zu erkennen, erreichten die beiden Modelle bei den KG5b-Formularen einen F1-Score von 0,87.
Bei den Verträgen lag das Qwen-2.5-VL-32B mit einem F1-Score von 0,94 mit 0,10 über dem Qwen-2.5-VL-7B-finetuned.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_ie_peft_32b}
    \caption{Ergebnisse der Information Extraction}
    \label{fig:results_ie_peft_32b}
\end{figure}

Abbildung~\ref{fig:results_classification_peft_32b} zeigt eine geringe Differenz in der Klassifikationsleistung auf.
Während beide Modelle die KG5b-Formulare perfekt zuordnen, liegt das Qwen-2.5-VL-7B-finetuned in den anderen Dokumentenarten vorne.
Hier erzielt das Modell einen F1-Score von 0,94 bei den Verträgen und 0,95 bei den sonstigen Dokumenten.
Die Confusion Matrices in Abbildung~\ref{fig:confusion_matrices_32b_peft} verdeutlichen dieses Ergebnis.
Im direkten Vergleich klassifiziert das Qwen-2.5-VL-7B-finetuned einen Vertrag als sonstiges Dokument, während das Qwen-2.5-VL-32B drei sonstige Dokumente als Vertrag erkannte.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification_peft_32b}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification_peft_32b}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_peft}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B-finetuned}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}
    \caption{Confusion Matrices aller Modelle}
    \label{fig:confusion_matrices_32b_peft}
\end{figure}

Die \gls{Latenz} des großen Modells ist erkennbar größer als die des Qwen-2.5-VL-7B-finetuned.
Wie in der Abbildung~\ref{fig:results_latency_peft_32b} zu erkennen, ist die Dauer der Inferenz des Qwen-2.5-VL-32B merklich höher.
Mit einer maximalen \gls{Latenz} von 252,06 Sekunden liegt das Qwen-2.5-VL-32B über dem Qwen-2.5-VL-7B-finetuned mit 21,6 Sekunden.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency_peft_32b}
    \caption{Ergebnisse der Latenzmessung pro Dokument}
    \label{fig:results_latency_peft_32b}
\end{figure}


\section{Ressourcenverbrauch der VLMs gegenüber der OCR/YOLO-Pipeline}\label{sec:results_vram_usage}

Für den Vergleich des Ressourcenverbrauchs wird sowohl die \gls{VRAM}-Auslastung als auch der Energieverbrauch betrachtet.

In Abbildung~\ref{fig:results_vram_model} ist die statische \gls{VRAM}-Auslastung abgebildet.
Die niedrigste Auslastung hat die OCR/YOLO-Pipeline mit 4,21 GB\@.
Das Qwen-2.5-VL-7B und Qwen-2.5-VL-7B-finetuned befinden sich 16,4 GB und 16,88 GB auf einer Ebene.
Während das Pixtral-12B mit 23,89 GB knapp über den 7B Modellen liegt, ist das Qwen-2.5-VL-32B mit 64,18 GB mit Abstand das größte Modell.

Die dynamische \gls{VRAM}-Auslastung in Abbildung~\ref{fig:results_vram_inference} zeigt ein anderes Ergebnis, jedoch liegt auch hier die OCR/YOLO-Pipeline mit einer durchschnittlichen Auslastung von 0,49 GB und einer maximalen von 1,02 GB unter den Mitstreitern.
Interessant sind die Werte des Qwen-2.5-VL-7B-finetuned und des Qwen-2.5-VL-32B, denn hier liegt das kleinere Modell mit durchschnittlich 3,03 GB über dem großen Modell mit durchschnittlich 3,01 GB


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_model}
    \caption{Ergebnisse der VRAM-Nutzung des geladenen Modells}
    \label{fig:results_vram_model}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_inference}
    \caption{Ergebnisse der VRAM-Nutzung während der Inferenz pro Dokument}
    \label{fig:results_vram_inference}
\end{figure}

Der Energieverbrauch in Abbildung~\ref{fig:results_energy_usage} zeigt den markantesten Unterschied zwischen der OCR/YOLO-Pipeline und den VLMs.
Während die Pipeline nur 117 J im Durchschnitt verbraucht, benötigt selbst das effizienteste Modell, das Qwen-2.5-VL-7B, etwas das 22-fache an Energie.
Im Vergleich zum Qwen-2.5-VL-32B, ist die Pipeline bei dem maximalen Verbrauch rund 250-fach sparsamer.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_energy}
    \caption{Ergebnisse des Energieverbrauchs pro Dokument}
    \label{fig:results_energy_usage}
\end{figure}