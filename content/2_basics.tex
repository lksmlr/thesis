\chapter{Theoretische Grundlagen}\label{ch:basics}

\section{Dokumentenarten}\label{sec:documenttypes}

Im Rahmen der Kindergeldbeantragung für Auszubildende sind verschiedene Nachweise gültig.
Zu den anerkannten Dokumententypen zählen der offizielle Vordruck der Bundesagentur für Arbeit (KG5b)\cite{KG5b} sowie Ausbildungsverträge.
Die relevanten Informationen aus diesen Dokumenten sind von der fachlichen Seite vorgegeben.
Zusätzlich laden Kunden häufig weitere Unterlagen, wie beispielsweise Schulbescheinigungen, im Portal hoch.
Da diese für den Kindergeldantrag nicht im Fokus stehen, werden sie im Folgenden unter der Kategorie „Sonstiges“ zusammengefasst.


\subsection{KG5b}\label{subsec:kg5b}

Das Formular KG5b ist, wie bereits erwähnt, ein offizielles Dokument der Bundesagentur für Arbeit, welches als Bescheinigung der Ausbildungsstätte dient.
Volljährige Kinder weisen damit gegenüber der Familienkasse den Status ihrer Ausbildung nach, was die Voraussetzung für den weiteren Kindergeldbezug ist.

Abbildung~\ref{fig:kg5b} zeigt ein exemplarisch ausgefülltes KG5b-Formular mit Markierung der für den Kindergeldantrag relevanten Felder.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kg5b_page_1}
        \caption{Erste Seite}
        \label{fig:kg5bpage1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kg5b_page_2}
        \caption{Zweite Seite}
        \label{fig:kg5bpage2}
    \end{subfigure}
    \caption{KG5b-Formular}
    \label{fig:kg5b}
\end{figure}

\subsection{Ausbildungsvertrag}\label{subsec:vertraege}

Im Gegensatz zu den standardisierten KG5b-Formularen weisen Ausbildungsverträge eine deutlich höhere Varianz auf.
Dies ist auf die Vielzahl unterschiedlicher zuständiger Stellen (z.\,B. Industrie- und Handelskammern, Handwerkskammern, Ärztekammern) und Firmen zurückzuführen, die jeweils ein eigenes Layout definieren.
Die Vielfalt der Dokumentenstruktur reicht dabei von formularbasierten Layouts bis hin zu unstrukturierten Fließtexten.

In der folgenden Abbildung~\ref{fig:vertrag} ist ein synthetischer Ausbildungsvertrag der Industrie- und Handelskammer\cite{Vertrag} abgebildet, in dem die relevanten Felder markiert sind.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/vertrag_page_1}
        \caption{Erste Seite}
        \label{fig:vertragpage1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/vertrag_page_2}
        \caption{Zweite Seite}
        \label{fig:vertragpage2}
    \end{subfigure}
    \caption{Ausbildungsvertrag der Industrie- und Handelskammer}
    \label{fig:vertrag}
\end{figure}

\subsection{Sonstige Dokumente}\label{subsec:other_documents}

Die Kategorie \texttt{Sonstiges} steht als Auffangklasse für alle restlichen Dokumente bereit.
Darin befinden sich zum Beispiel Schulbescheinigungen, Anträge auf Eintragung bei der Handelskammer oder Studienbescheinigungen.
Da diese Dokumente keine Relevanz für die Weiterbeantragung des Kindergeldes bei volljährigen Auszubildenden haben, werden keine Informationen aus ihnen benötigt.


\section{Fachliche Grundlagen}\label{sec:domain}

\subsection{Der Ist-Zustand}\label{subsec:current}

Der schematische Ablauf der \gls{YOLO}/\gls{OCR}-Pipeline ist in Abbildung~\ref{fig:aube} dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/aube}
    \caption{Schematischer Workflow der aktuellen AuBe-Pipeline}
    \label{fig:aube}
\end{figure}

Der Prozess der Dokumentenverarbeitung lässt sich in zwei Stufen einteilen.

In der ersten Stufe werden die hochgeladenen Dokumente des Kunden entgegengenommen.
Wenn ein Dokument als PDF vorliegt, wird mithilfe des Textlayers und eines \gls{OCR}-Modells der Text extrahiert, während bei einer Bilddatei lediglich das \gls{OCR}-Modell zum Einsatz kommt.
Als Ergebnis der ersten Stufe werden dann ein PDF/A und der extrahierte Text zurückgeliefert.

Auf Basis des extrahierten Textes wird in der nächsten Stufe zu Beginn eine Klassifikation durchgeführt.
Hierbei unterscheidet der Klassifikator zwischen den bereits vorgestellten Dokumententypen: KG5b, Vertrag und Sonstiges.
Wurde das Dokument als \texttt{Sonstiges} klassifiziert, endet an dieser Stelle die Bearbeitung.
Handelt es sich hingegen um ein KG5b oder einen Vertrag, wird je nach Dokumententyp eine Erkennung mit einem \gls{YOLO}-Modell gestartet.
Innerhalb der Bounding-Boxen wird der Text extrahiert und dem jeweiligen Label zugeordnet.

Schlussendlich stehen die erkannten Informationen zur weiteren Verarbeitung bereit.

\subsection{Rahmenbedingungen und Infrastruktur}\label{subsec:infrastructure}

Die Entwicklung und Evaluation der Modelle erfolgt unter datenschutzrechtlichen Auflagen.
Da im Rahmen dieser Arbeit personenbezogene Echtdaten verarbeitet werden, wird eine isolierte On-Premises-Infrastruktur verwendet.

Die technische Architektur ist in Abbildung~\ref{fig:infrastructure} schematisch dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/infrastructure}
    \caption{Schematische Darstellung der Trainings- und Inferenzinfrastruktur}
    \label{fig:infrastructure}
\end{figure}

Das System basiert auf einem abgeschotteten Kubernetes-Cluster.
Für die rechenintensiven Aufgaben, insbesondere das Fine-Tuning und die Inferenz der \glspl{VLM}, stehen innerhalb des Clusters zwei NVIDIA A40 GPUs mit jeweils 48 GB \gls{VRAM}\cite{NVIDIAA40} zur Verfügung.
Um die Bereitstellung der \glspl{VLM} zu erleichtern, wird auf dem Cluster Kubeflow verwendet.
Kubeflow ist eine Open-Source-Plattform, die speziell für das Entwickeln, Trainieren und Deployen von Machine-Learning-Modellen konzipiert wurde.

Der Zugang zum Cluster sowie zur Integrated Development Environment (\gls{IDE}) erfolgt über eine noVNC-Schnittstelle (browserbasierter Remote-Desktop).

Um einen Test- oder Trainingslauf durchzuführen, wird der folgende Prozess durchlaufen:
\begin{enumerate}
    \item \textbf{Initialisierung:} Ein Python-Skript in der \gls{IDE} startet den Kubeflow-Job.
    \item \textbf{Datenbereitstellung:} Die Pipeline lädt sowohl das \gls{VLM} als auch die zu klassifizierenden Dokumentenbilder aus dem S3-Store in den GPU-Cluster.
    \item \textbf{Verarbeitung:} Die Inferenz oder das Training findet im Kubernetes-Cluster statt.
    \item \textbf{Persistierung:} Die Ergebnisse werden zurück in den S3-Store geladen.
\end{enumerate}


\section{Vision Language Models (VLMs)}\label{sec:vlms}

Ein \gls{VLM} besteht aus drei Komponenten: einem Image-Encoder, einem Adapter und einem Large Language Model (\gls{LLM}).
In Abbildung~\ref{fig:vlm} wird der Aufbau dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/vlm}
    \caption{Architektur eines Vision Language Models}
    \label{fig:vlm}
\end{figure}

Der Image-Encoder verarbeitet die Bildeingabe und extrahiert visuelle Merkmale (Features).
Zu diesem Zweck kommen häufig vortrainierte Modelle wie Vision Transformer (\gls{ViT}) oder CLIP zum Einsatz.
Diese Modelle zerlegen ein Bild in kleinere Patches, die ähnlich wie Token in Sprachmodellen behandelt werden.
Jeder Patch wird in einen Vektor überführt, der die Eigenschaften dieses Bildausschnitts repräsentiert.
Das Ergebnis ist eine Sequenz von Bildvektoren, die die visuellen Informationen des gesamten Bildes beinhaltet.

Der Adapter verbindet den Image-Encoder mit dem Sprachmodell.
Diese Schicht transformiert die Ausgabe des Image-Encoders in ein Format, das mit den Textvektoren des Sprachmodells kompatibel ist.
In vielen Modellen besteht der Adapter aus einer linearen Schicht oder einem kleinen neuronalen Netz.
Die Bildvektoren des Image-Encoders werden in denselben Vektorraum wie die Textvektoren des \gls{LLM} projiziert.

Die Hauptkomponente bildet das \gls{LLM}, welches für die eigentliche Verarbeitung und Generierung zuständig ist.
Das \gls{LLM} erhält die Textvektoren zusammen mit den Bildvektoren und fusioniert diese.
Mithilfe des Attention-Mechanismus werden Beziehungen zwischen visuellen und textuellen Elementen verstanden.
So ist es möglich, Fragen zu Bildinhalten zu beantworten oder Bildbeschreibungen zu generieren.

Die größte Herausforderung dieser modernen Technologie ist der hohe \gls{VRAM}-Verbrauch.
Neben der statischen \gls{VRAM}-Nutzung kommt mit jeder Anfrage ein dynamischer Verbrauch dazu.
Dabei setzt sich der statische Verbrauch aus den Gewichten der Modelle zusammen, während sich der dynamische Verbrauch aus den Key-Value-Caches und den Aktivierungen bildet.


\section{Vorstellung der Modelle}\label{sec:models}

Für die Klassifikation und die Extraktion der Informationen aus den Dokumenten wurden drei verschiedene \glspl{VLM} evaluiert.

\subsection{Pixtral-12B}\label{subsec:pixtral}

Das Pixtral-12B-2409\cite{Pixtral12B} wurde im Jahr 2024 von Mistral AI veröffentlicht.
Es basiert auf einem 12 Milliarden Parameter großen Text-Decoder mit einem zusätzlichen 400 Millionen Parameter umfassenden Vision-Encoder.
Es wurde speziell auf das Verständnis von Bildern und Dokumenten trainiert, weshalb es einen optimalen Kandidaten für die vorliegende Arbeit darstellt.
Mit einem theoretischen Kontextfenster von 128.000 Token ermöglicht das Modell die gleichzeitige Verarbeitung mehrerer Bilder\cite{PixtralPaper}.

\subsection{Qwen-2.5-VL}\label{subsec:qwen2.5vl}

Das Qwen2.5-VL-7B-Instruct\cite{Qwen7B} sowie das Qwen2.5-VL-32B-Instruct\cite{Qwen32B} wurden im Jahr 2025 von Alibaba Cloud veröffentlicht.
Die beiden Modelle basieren auf dem gleichen Image-Encoder mit 600 Millionen Parametern.
Lediglich die Größe des Text-Decoders ist mit 7 Milliarden beziehungsweise 32 Milliarden Parametern unterschiedlich.
Eine Besonderheit dieser Modelle ist, dass sie speziell auf das Verarbeiten von Dokumenten trainiert wurden\cite{QwenPaper}.

\subsection{Benchmarks}\label{subsec:benchmarks}

Im Folgenden sind die Benchmarks der einzelnen Modelle gelistet.
Besonders das Ergebnis des DocVQA-Benchmarks ist von Interesse, da hier ein Visual Question Answering (\gls{VQA}) auf Dokumentenbildern durchgeführt wird\cite{DocVQA}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Modell} & \textbf{Pixtral-12B} & \textbf{Qwen2.5-VL-7B} & \textbf{Qwen2.5-VL-32B} \\
        \hline
        \textbf{DocVQA} & 90,7 & 95,7 & 94,8 \\
        \textbf{MMMU} & 52,0 & 58,6 & 70,0 \\
        \hline
    \end{tabular}
    \caption{Benchmark-Ergebnisse der evaluierten Modelle\cite{QwenPaper, Qwen32B, PixtralPaper}}
    \label{tab:benchmark}
\end{table}


\section{Parameter-Efficient Fine-Tuning}\label{sec:peft}

Das Anpassen der Gewichte eines bereits trainierten Modells an eine spezifische Domäne setzt eine Infrastruktur mit hoher Rechenleistung voraus.
Mithilfe von Parameter-Efficient Fine-Tuning (\gls{PEFT}) kann die Anzahl der zu trainierenden oder neuen Parameter signifikant gesenkt werden.
Infolgedessen verringert sich auch der Rechenaufwand für das Fine-Tuning der Modelle erheblich\cite{peft}.

Eine der bekanntesten Methoden im Bereich \gls{PEFT} ist die Low-Rank Adaptation (\gls{LoRA}).
Anstatt das gesamte Modell neu zu trainieren, friert \gls{LoRA} die ursprünglichen Gewichte ein und fügt stattdessen trainierbare Matrizen mit einem niedrigen Rang $r$ in jede Schicht der Transformer-Architektur ein.
Um Rechenaufwand und Speicherplatz zu sparen, werden ausschließlich diese kleineren Matrizen trainiert\cite{LoRA}.

Ergänzend zu \gls{LoRA} wird die Variante Rank-Stabilized \gls{LoRA} (\gls{rsLoRA}) betrachtet.
Bei der Standard-\gls{LoRA} führt ein steigender Rang $r$ oft nicht zu einer besseren Performance, da der verwendete Faktor ($\alpha/r$) das Lernen bei höheren Rängen verlangsamen oder hemmen kann.
\gls{rsLoRA} stabilisiert diesen Prozess, indem die Adapter durch die Quadratwurzel des Rangs ($\alpha/\sqrt{r}$) geteilt werden\cite{rsLoRA}.


\section{Weitere Grundlagen}\label{sec:further_basics}

\subsection{Information Extraction}\label{subsec:ie}

Information Extraction (\gls{IE}) umfasst nach Jurafsky und Martin die Aufgabe, Ereignisse oder Situationen in Dokumenten zu identifizieren und die entsprechenden Felder eines vorgegebenen Templates zu füllen\cite{IE}.
Ziel hierbei ist es, vordefinierte Entitäten aus einem Dokumentenbild zu extrahieren und diese in ein maschinenlesbares Schema zu überführen.

\subsection{Levenshtein-Similarity}\label{subsec:levenshtein}

Um zwei Zeichenketten vergleichen zu können, wird die Levenshtein-Similarity benötigt.
Die Levenshtein-Similarity ist ein aus der Levenshtein-Distanz abgeleiteter, normalisierter Ähnlichkeitswert zwischen zwei Zeichenketten.
Formal basiert sie auf der Levenshtein-Distanz, also der minimalen Anzahl von Einfügungen, Löschungen oder Ersetzungen, die nötig sind, um eine Zeichenkette in eine andere zu überführen\cite{gld}.

\subsection{Metriken}\label{subsec:metriken}
% TODO: Metriken anpassen
Für die Bewertung der Güte und Effizienz der Modelle werden verschiedene Metriken benötigt:

\begin{itemize}
    \item \textbf{Accuracy:} Der Anteil der insgesamt richtigen Vorhersagen an allen Vorhersagen.
    \item \textbf{Precision:} Der Anteil der tatsächlich positiven Fälle an allen als positiv klassifizierten Fällen.
    \item \textbf{Recall:} Der Anteil der korrekt als positiv erkannten Fälle an allen tatsächlich vorhandenen positiven Fällen.
    \item \textbf{F1-Score:} Das harmonische Mittel aus Precision und Recall.
    \item \textbf{Latenz:} Die Zeitdauer, die das System für die Inferenz einer einzigen Antwort benötigt.
    \item \textbf{\gls{VRAM}-Auslastung:} Der maximale Bedarf an Grafikspeicher während der Verarbeitung.
    \item \textbf{Leistungsaufnahme:} Der durchschnittliche Energieverbrauch der GPU während der Inferenz.
\end{itemize}

\subsection{Prompt Engineering}\label{subsec:prompt_engineering}

Prompt Engineering ist ein Prozess, um die effektivste Anweisung für eine spezifische Aufgabe zu finden\cite{prompt_engineering}.
Dabei unterscheidet man zwischen Zero-Shot, One-Shot und Few-Shot Prompting.
Während bei Zero-Shot Prompting das \gls{LLM} keine Beispiele erhält, werden bei One-Shot ein einzelnes und bei Few-Shot mehrere Frage-Antwort-Beispiele zur Verfügung gestellt\cite{few_shot_learners}.


\section{Verwandte Arbeiten}\label{sec:relatedwork}

Ein Feld gilt hierbei als True Positive (\gls{TP}), wenn sowohl der Key des Feldes vorhanden ist als auch der Value des Feldes mit dem der Ground Truth übereinstimmt\cite{ICDAR}.