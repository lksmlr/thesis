\chapter{Methodik und Datenaufbereitung}\label{ch:data}

%TODO:Forschungsfragen
Mit dem Proof of Concept gilt es mehrere Vergleiche zwischen verschiedenen \glspl{VLM} sowie der \gls{YOLO}/\gls{OCR}-Pipeline anzustellen.
Für die Entscheidung, ob und welches \gls{VLM} die bisherige Pipeline ersetzen wird, sind die Modellleistungen als auch der Ressourcenverbrauch interessant.
Aus diesem Grund müssen die folgenden Fragen beantwortet werden:
\begin{enumerate}
    \item Inwiefern ist ein nicht angepasstes \gls{VLM} in der Leistung der trainierten \gls{YOLO}/\gls{OCR}-Pipeline überlegen und welche Vorteile hat ein einzelnes generalistisches Modell gegenüber der Benutzung verschiedener Spezialisten?
    \item Wie verhält sich ein kleineres Modell, dass domänenspezifisch trainiert wurde, gegenüber einem leistungsstärkeren Modell in Bezug auf Leistung und Effizienz?
    \item In welchem Verhältnis steht der Ressourcenverbrauch der \glspl{VLM} zu dem der \gls{YOLO}/\gls{OCR}-Pipeline?
\end{enumerate}

\section{Modellierung der Dokumentenarten als JSON}\label{sec:documents_as_json}

Um eine standardisierte Weiterverarbeitung der Modellausgaben zu gewährleisten, ist eine feste Struktur essenziell.
Da moderne \glspl{VLM} darauf trainiert sind, Antworten im \gls{JSON}-Format zu liefern, werden die extrahierten Informationen in ein vordefiniertes \gls{JSON}-Schema überführt.
Diese Schemata unterscheiden sich je nach Dokumentenart, da jeweils unterschiedliche Informationen relevant sind.

Um den \gls{JSON}-Output des \glspl{VLM} zu restriktieren, gibt es verschiedene Möglichkeiten.
Während komplexere Lösungen nur Tokens zulassen die \gls{JSON} konform sind, wird in dieser Evaluation ein einfacherer Ansatz gewählt.
Hierbei werden aus der generierten Antwort \glspl{JSON} gefiltert und geparst.
Sobald das Parsen der \gls{JSON} fehlschlägt, wird diese zurück an das \gls{VLM} geliefert, mit der Anweisung diese zu korrigieren.
Das Modell bekommt insgesamt drei Versuche eine valide \gls{JSON} zu generieren, bevor das Dokument als ungültig eingestuft wird.


\subsection{KG5b}\label{subsec:json_kg5b}

Das Schema für das Formular KG5b ist in Abbildung~\ref{fig:json_kg5b} definiert.
Die Klassifikation spiegelt sich im Feld \texttt{type} wider, während die restlichen Felder Platzhalter für die \gls{IE} darstellen.
Bei den binären, booleschen Felder markiert ein \texttt{true} das Vorhandensein des Feldes, während ein \texttt{False} das nicht Vorhandensein repräsentiert.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/kg5b_json}
    \caption{JSON-Schema des Dokumententyps KG5b}
    \label{fig:json_kg5b}
\end{figure}


\subsection{Ausbildungsvertrag}\label{subsec:json_vertrag}

Das Schema für die Ausbildungsverträge ist in Abbildung~\ref{fig:json_vertrag} definiert.
Wie im Schema des KG5b dient das Feld \texttt{type} der Klassifikation, während die restlichen Felder die \gls{IE} repräsentieren.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/vertrag_json}
    \caption{JSON-Schema des Dokumententyps Ausbildungsvertrag}
    \label{fig:json_vertrag}
\end{figure}


\subsection{Sonstige Dokumente}\label{subsec:json_sonstiges}

Das Schema für die nicht relevanten Dokumente ist in Abbildung~\ref{fig:json_sonstiges} definiert.
Da hier keine Informationen benötigt werden, sondern rein die Klassifikation von Nutzen ist, besteht das Schema ausschließlich aus dem Feld \texttt{type}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/sonstiges_json}
    \caption{JSON-Schema des Dokumententyps Sonstiges}
    \label{fig:json_sonstiges}
\end{figure}


\section{Evaluation}\label{sec:evaluation}

Um die generierte \gls{JSON} des \glspl{VLM} schlussendlich bewerten zu können, werden verschiedene Vorverarbeitungschritte


\subsection{Vergleichslogik der unterschiedlichen JSON-Felder}\label{subsec:comparator}

Mit dem Ziel ein realistisches Ergebnis zu erhalten, müssen unterschiedliche Feldtypen spezifisch verglichen werden.
Die Menge an distinkten Typen beinhaltet: Namen, Booleans, Zeichenketten, Daten und Monate.

Namen werden mit der Levenshtein-Similarity verglichen, um kleinere Fehler oder verschiedene Varianten zu tolerieren.
Beispielsweise wird ein Name, der \texttt{ue} statt \texttt{ü} enthält, nicht als Fehler erkannt.
Der Schwellenwert, der zwischen korrekt und nicht korrekt entscheidet, liegt bei 0,8.
Des Weiteren werden minimale \gls{OCR}-Fehler gefiltert.
Die \gls{YOLO}/\gls{OCR}-Pipeline verwendet ebenso diesen Vergleich mit den gleichen Schwellenwerten, somit wird das Ergebnis vergleichbar.

Zeichenketten und Booleans werden exakt verglichen.
Hierzu zählen die Felder \texttt{type}, \texttt{stamp\_company}, \texttt{signature\_company}, \texttt{signature\_child}, \texttt{signature\_legal\_guardian} und \texttt{apprenticeship\_finished}.
Lediglich die booleschen Werte werden zu \texttt{true} normalisiert, da hier eine potenzielle Fehlerquelle liegt.

Daten werden einheitlich in das Format DD.MM.YYYY gebracht, obwohl dies nicht dem internationalen Standard entspricht.
Das ist darauf zurückzuführen, dass so die Erkennung des \glspl{VLM} am robustesten ist, da die deutschen Dokumente auch mit diesem Format ausgefüllt werden.

Monate werden in das Format MM normalisiert.
Hierbei werden ausgeschriebene Monatsnamen sowie vollständige Datumsangaben angepasst.


\subsection{Bewertung der Klassifikation}\label{subsec:evaluation_classification}

Die Bewertung der Klassifikation ist ein Multiclass-Problem, da hier mehr als zwei Klassen vorliegen (KG5b, Ausbildungsvertrag, Sonstiges).
Um die Güte der Modelle zu betrachten, wird eine Confusion-Matrix herangezogen sowie die Accuracy gemessen.
Durch die gleichverteilung der Klassen in den Datensätzen werden hier keine weiteren Metriken gebraucht.


\subsection{Bewertung der Information Extraction}\label{subsec:evaluation_ie}

Um den Entity-Level F1-Score, der die Hauptmetrik für den Vergleich darstellt, zu berechnen werden einige Metadaten für jede generierte \gls{JSON} bestimmt.
Dazu gehören die Status der Felder, also ob diese vorhanden, nicht vorhanden oder halluziniert sind.
Sobald ein Feld als vorhanden markiert wird, wird mit den verschiedenen Vergleichslogiken die Richtigkeit der Felder bestimmt und den Metadaten hinzugefügt.

Auf Basis dieser Metadaten werden für den gesamten Dokumentenkorpus die True Positive (\gls{TP}), False Positive (\gls{FP}) und False Negative (\gls{FN}) gezählt.
Ein Feld zählt als \gls{TP}, wenn es vorhanden sowie korrekt ist.
Zu den \gls{FN} gehören Felder, die entweder fehlen oder vorhanden aber falsch sind.
Ein Feld, das vorhanden aber falsch ist, zählt zusätzlich zu den \gls{FP} zusammen mit den Feldern die halluziniert wurden.
Diese doppelte Bestrafung stellt eine sehr strenge Bewertung für inhaltliche Fehler dar.
\glspl{TN} sind in dem Falle nicht zählbar, da die Menge an nicht gefundenen Feldern, in denen nichts stande, unendlich groß ist.


\subsection{Messung von Latenz, Energieverbrauch und VRAM-Nutzung}\label{subsec:measure}

Im Hinblick auf die Bewertung der Modelle werden neben der Güte der Klassifikation und \gls{IE} die Latenz, die \gls{VRAM}-Nutzung sowie der Energieverbrauch gemessen

Die Latenz der Modelle stellt lediglich das Delta zwischen Start- und Endzeit der Inferenz dar.
Hierbei wird der Zeitpunkt gemessen, an dem die eigentliche Inferenz startet und wenn die Antwort bereitsteht.
Das Laden des Modells wird nicht betrachtet, da diese keinen Einfluss auf die Antwortzeit in der produktiven Umgebung hat und folglich nicht relevant für die Evaluation ist.

Des Weiteren wird der Energieverbrauch während der Inferenz in Joule erfasst.
Die Messung der Leistung in Watt hat ist in diesem Fall nicht zielführend, da hier die Zeit eine Rolle spielt.
Modelle mit kürzerer Latenz können bei einem vergleichbaren Energieverbrauch eine höhere Leistung aufweisen.

Wie auch der Energieverbrauch wird die \gls{VRAM}-Nutzung während der Inferenz gemessen.
Der Messzeitraum für den Energieverbrauch als auch für die \gls{VRAM}-Nutzung ist gleich der Latenz der Modelle.
Mit der Initialisierung des Modells wird einmalig der statische Verbrauch gemessen.


\section{Daten und Wahl des Basismodells}\label{sec:data}

\subsection{Datenvorbereitung}\label{subsec:preprocessing}

Durch Datenlieferungen der Familienkasse stehen mehrere Tausend Dokumente als PDF oder Bilddatei bereit.
Um eine einheitliche Basis zu schaffen und gute Ergebnisse zu erzielen, müssen die Daten vorbereitet werden.
Speziell die Drehung der Bilddateien sowie die Konvertierung der PDFs in Bilddateien stehen im Vordergrund.

Um die Drehung der Bilddateien zu korrigieren, werden die Exchangeable Image File Format (\gls{EXIF})-Daten ausgelesen.
Mithilfe dieser wird die originale Ausrichtung wieder hergestellt.

Bei älteren \glspl{VLM} wurden Bilder naiv auf eine bestimmte Größe skaliert.
Die hier verwendeten Modelle unterstützen eine dynamische Skalierung.
Um dennoch die maximale Tokenanzahl der Bilder zu begrenzen, werden diese vor der Weitergabe an das \gls{VLM} auf eine maximale Pixelanzahl skaliert.
Dabei wird jedoch das Seitenverhältnis beibehalten, um möglichst viele Details beizubehalten.


\subsection{Testdatensatz}\label{subsec:test_dataset}

Nach der Vorbereitung der Dokumente wird ein Testdatensatz erstellt
Dieser ist ausbalanciert und besteht aus insgesamt 60 Dokumenten.
Darunter 20 KG5bs, 20 Verträge und 20 sonstige Dokumente.
Um die Ground-Truth \glspl{JSON} zu erstellen, wurden die Dokumente manuell gelabelt.


\subsection{Modellauswahl}\label{subsec:modelselection}

Für das Anpassen eines Modells an die spezifischen Dokumentenarten und das Ausgabeformat muss eines Basismodell gewählt werden.
Wie in Abbildung~\ref{fig:model_selection} zu sehen, erfolgte diese Modellauswahl iterativ.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/model_selection}
    \caption{Iterativer Prozess der Modellauswahl für das Fine-Tuning}
    \label{fig:model_selection}
\end{figure}

Mit jeder Iteration wurden die beiden Modelle, das Pixtral-12B sowie das Qwen-2.5-VL-7B, mit dem Testdatensatz getestet.
Die generierten Antworten werden im Anschluss bewertet und der Prompt gegebenenfalls angepasst.

Dieser iterative Vorgang wird bis zu dem Punkt ausgeführt, an dem mit Promptänderungen keine Verbesserungen mehr erzielt werden.


\subsection{Trainingsdatensatz}\label{subsec:train_dataset}

Der Trainingsdatensatz muss für hochwertige Ergebnisse deutlich umfangreicher als der Testdatensatz ausfallen.
Er besteht aus 227 Verträge, 165 KG5bs und 218 sonstigen Dokumenten.

Die Klassen sind nicht gleich verteilt, da aufgrund des starren Layouts der KG5b-Formulare hier die besten Ergebnisse erwartet werden.
Somit wurden die Klassen mit der höchsten Varianz häufiger repräsentiert.

Durch die vorherige Auswahl des Basismodells steht für das label dieses größeren Datensatzes ein Modell, sowie der passende Prompt, bereit.
Mithilfe des Modells wird der Aufwand um ein Vielfaches reduziert, da nur noch kontrolliert und in einzelnen Fällen korrigiert werden muss.


\section{Fine-Tuning}\label{sec:fine_tuning}

