\chapter{Theoretische Grundlagen}\label{ch:basics}


\section{Dokumentenarten}\label{sec:documenttypes}

Im Rahmen der Kindergeldbeantragung sind verschiedene Nachweise gültig.
Zu den anerkannten Dokumententypen zählen der offizielle Vordruck der Bundesagentur für Arbeit (KG5b)\cite{KG5b} sowie Ausbildungsverträge.
Zusätzlich laden Kunden häufig weitere Unterlagen, wie beispielsweise Schulbescheinigungen, im Portal hoch.
Da diese für den Kindergeldantrag nicht im Fokus stehen, werden sie im Folgenden unter der Kategorie „Sonstiges“ zusammengefasst.


\subsection{KG5b}\label{subsec:kg5b}

Der Vordruck KG5b ist, wie bereits erwähnt, ein offizielles Dokument der Bundesagentur für Arbeit, welches als Bescheinigung der Ausbildungsstätte dient.
Volljährige Kinder weisen damit gegenüber der Familienkasse den Fortbestand ihrer Ausbildung nach, was die Voraussetzung für den weiteren Kindergeldbezug ist.

Abbildung\ref{fig:kg5b} zeigt ein exemplarisch ausgefülltes KG5b-Formular mit Markierung der für den Kindergeldantrag relevanten Felder.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kg5b_page_1}
        \caption{Erste Seite}
        \label{fig:kg5bpage1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/kg5b_page_2}
        \caption{Zweite Seite}
        \label{fig:kg5bpage2}
    \end{subfigure}
    \caption{KG5b-Formular}
    \label{fig:kg5b}
\end{figure}


\subsection{Ausbildungsvertrag}\label{subsec:vertraege}

Im Gegensatz zu den standardisierten KG5b-Formularen weisen die Ausbildungsverträge eine signifikant höhere Varianz auf.
Dies ist auf die Vielzahl unterschiedlicher zuständiger Stellen (z.,B. Industrie- und Handelskammern, Handwerkskammern, Ärztekammern) und Firmen zurückzuführen, die jeweils ein eigenes Layout definieren.
Die Vielfalt der Dokumentenstruktur reicht dabei von formularbasierten Layouts bis hin zu unstrukturierten Fließtexten.

In der folgenden Abbildung\ref{fig:vertrag} ist ein synthetischer Ausbildungsvertrag der Industrie- und Handelskammer\cite{Vertrag} abgebildet in dem die relevanten Felder markiert sind.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/vertrag_page_1}
        \caption{Erste Seite}
        \label{fig:vertragpage1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/vertrag_page_2}
        \caption{Zweite Seite}
        \label{fig:vertragpage2}
    \end{subfigure}
    \caption{Ausbildungsvertrag der Industrie- und Handwerkskammer}
    \label{fig:vertrag}
\end{figure}


\subsection{Sonstige Dokumente}\label{subsec:other_documents}

Die Kategorie \texttt{Sonstiges} steht als Auffangklasse für alle restlichen Dokumente bereit.
Hier befinden sich zum Beispiel Schulbescheinigungen, Anträge auf Eintragung bei der Handelskammer oder Studienbescheinigungen.
Da diese Dokumente keine relevanz für die Weiterbeantragung des Kindergeldes bei volljährigen Auszubildenden hat, werden keine Information aus diesen benötigt.


\section{Datenschutz}\label{sec:data_privacy}

Da es sich bei allen verwendeten Dokumenten um Echtdaten aus den Fachverfahren handelt, enthalten diese sensible personenbezogene Daten.
Der Schutz dieser Daten hat höchste Priorität.

Um die Sicherheit bei der Verarbeitung der Echtdaten zu gewährleisten, wurde ausschließlich auf die Infrastruktur der Bundesagentur für Arbeit zurückgegriffen.
Die Daten liegen in einem gesicherten Netzwerkbereich, auf den nur ein autorisierter Personenkreis Zugriff hat.
Durch die Verwendung von Echtdaten wird die Qualität der Ergebnisse deutlich gesteigert.

Alle in dieser Arbeit abgebildeten Dokumentenbeispiele wurden synthetisch erzeugt und beinhalten keine sensiblen Daten.


\section{Fachliche Grundlagen}\label{sec:domain}

\subsection{Der Ist-Zustand}\label{subsec:current}

Wie bereits in der Einleitung angedeutet, setzt sich das aktuelle System aus einer mehrstufigen Pipeline zusammen.
Der schematische Ablauf ist in Abbildung\ref{fig:aube} dargestellt und lässt sich in drei logische Phasen unterteilen: Eingangsverarbeitung, Klassifikation und Informationsextraktion.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/aube}
    \caption{Schematischer Workflow der aktuellen AuBe-Pipeline}
    \label{fig:aube}
\end{figure}

Der Prozess beginnt mit dem Upload eines Dokuments durch den Kunden.
Ein vorgeschaltetes System nimmt diese Dateien entgegen, die als PDF oder Bildformate vorliegen können.
In diesem Schritt findet eine erste \gls{OCR}-Verarbeitung statt.
Das Ergebnis, welches an die \gls{YOLO}/\gls{OCR}-Pipeline übergeben wird, ist ein standardisiertes PDF/A-Dokument sowie der extrahierte Text.

Im ersten Schritt der eigentlichen Pipeline entscheidet ein Text-Klassifikator auf Basis des \gls{OCR}-Textes, um welche Art von Dokument es sich handelt.
Wird ein Dokument als „Sonstiges“ klassifiziert, endet die Verarbeitung an dieser Stelle.
Nur die für den Fachprozess relevanten Typen werden weiterverarbeitet.

Für die relevanten Typen wird das PDF/A in ein Bild konvertiert und einem \gls{YOLO}-Modell übergeben.
Dieses \gls{YOLO}-Modell zeichnet Bounding-Boxen um die relevanten Felder, wobei daraufhin, ein weiteres \gls{OCR}-Modell die Erkennung in diesen Boxen durchführt.


\subsection{Rahmenbedingungen und Infrastruktur}\label{subsec:infrastructure}

Die Entwicklung und Evaluation der Modelle erfolgt unter datenschutzrechtlichen Auflagen.
Da im Rahmen dieser Arbeit personenbezogene Echtdaten verarbeitet werden, wird eine isolierte On-Premise-Infrastruktur verwendet.

Die technische Architektur ist in Abbildung\ref{fig:infrastructure} schematisch dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/infrastructure}
    \caption{Schematische Darstellung der Trainings- und Inferenzinfrastruktur}
    \label{fig:infrastructure}
\end{figure}

Das System basiert auf einem abgeschotteten Kubernetes-Cluster.
Für die rechenintensiven Aufgaben, insbesondere das Fine-Tuning und die Inferenz der \gls{VLM}s, stehen innerhalb des Clusters zwei NVIDIA A40 GPUs, mit jeweils 48 GB vRAM\cite{NVIDIAA40}, zur Verfügung.
Um die Bereitstellung der \glspl{VLM} zu erleichtern, wird auf dem Cluster Kubeflow verwendet.
Kubeflow ist eine Open-Source-Plattform die speziell für das Entwickeln, Trainieren und Deployen von Machine-Learning-Modellen entwickelt wurde.

Der Zugang zu dem Cluster, als auch zum Integrated Development Environment (\gls{IDE}), erfolgt über eine noVNC-Schnittstelle (browserbasierter Remote-Desktop).

Um einen Test- oder Trainingslauf durchzuführen, wird der folgende Prozess ausgeführt.
\begin{enumerate}
    \item \textbf{Initialisierung:} Ein Python-Skript in der \gls{IDE} triggert den Kubeflow-Job.
    \item \textbf{Datenbereitstellung:} Die Pipeline lädt sowohl das \gls{VLM} als auch die zu klassifizierenden Dokumentenbilder aus dem S3-Store in den GPU-Cluster.
    \item \textbf{Verarbeitung:} Die Inferenz oder das Training findet im Kubernetes-Cluster statt.
    \item \textbf{Persistierung:} Die Ergebnisse werden zurück in den S3-Store geladen.
\end{enumerate}

\section{Vision-Language Models (VLMs)}\label{sec:vlms}

\subsection{Vom Sprachmodell zum multimodalen Modell}\label{subsec:mllm}

Moderne Large Language Models bauen auf einer Transformer-Architektur auf.
Diese Transformer bestehen aus zwei Teilen, dem Encoder und dem Decoder.
Ein Encoder nimmt eine Eingabe entgegen und wandelt diese in einen Vektor um.
Ein Decoder nimmt einen solchen Vektor und generiert daraus Schritt für Schritt die Ausgabesequenz.
Modelle, die ausschließlich Text generieren, implementieren nur den Decoder des Transformers, weshalb man \gls{LLM}s auch Decoder-Only Modelle nennt.

Die Stärke dieser Decoder-Only Modelle liegt in ihrer Fähigkeit, natürliche Sprache zu verstehen und zu generieren.
Um auch visuelle Informationen verarbeiten zu können, wurden \gls{VLM} entwickelt.
Diese erweitern die Decoder-Only Architektur um eine Vision-Komponente, die es ermöglicht, Bilder als Eingabe zu verwenden.
Der grundlegende Ansatz besteht darin, visuelle Informationen in eine Form zu bringen, die das Sprachmodell verarbeiten kann.
Dafür wird ein separater Vision Encoder verwendet, der Bilder in Vektorrepräsentationen transformiert, die dann zusammen mit textuellen Tokens dem Sprachmodell übergeben werden.


\subsection{Architekturkomponenten}\label{subsec:components}

Ein \gls{VLM} besteht aus drei Hauptkomponenten, die in Abbildung\ref{fig:vlm} dargestellt sind.

Der Vision Encoder verarbeitet die Bildeingabe und extrahiert visuelle Features.
Hierfür kommen häufig vortrainierte Modelle wie Vision Transformer (ViT) oder CLIP zum Einsatz.
Diese Modelle zerlegen ein Bild in kleinere Patches, die ähnlich wie Tokens in Sprachmodellen behandelt werden.
Jeder Patch wird in einen hochdimensionalen Vektor verwandelt, der die visuellen Gegebenheiten dieses Bildausschnitts repräsentiert.
Das Ergebnis ist eine Sequenz von Bildvektoren, die die visuelle Information des gesamten Bildes kodieren.

Der Adapter verbindet den Vision Encoder mit dem Sprachmodell.
Diese Schicht transformiert die Ausgabe des Vision Encoders in ein Format, das mit den Textembeddings des Sprachmodells kompatibel ist.
Dabei werden die visuellen Vektoren in denselben Vektorraum projiziert, in dem auch die textuellen Tokens liegen.
Diese Komponente ist entscheidend, damit das Sprachmodell visuelle und textuelle Informationen gemeinsam verarbeiten kann.
In vielen Architekturen besteht diese Schicht aus einem einfachen linearen Layer oder einem kleinen neuronalen Netz.

Das Language Model bildet die dritte Komponente und ist für die eigentliche Verarbeitung und Generierung zuständig.
Hier kommen etablierte \gls{LLM}-Architekturen zum Einsatz, die auf großen Textkorpora vortrainiert wurden.
Das Sprachmodell erhält als Eingabe eine gemischte Sequenz aus projizierten Bildvektoren und Textembeddings.
Durch die Attention-Mechanismen des Transformers kann das Modell Beziehungen zwischen visuellen und textuellen Elementen lernen.
So ist es möglich, Fragen zu Bildinhalten zu beantworten oder Bildbeschreibungen zu generieren.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/vlm}
    \caption{Architektur eines Vision Language Models}
    \label{fig:vlm}
\end{figure}


\subsection{VLMs für Document Understanding}\label{subsec:vlm-documents}

Für die Extraktion von Informationen aus Dokumenten bieten \glspl{VLM} entscheidende Vorteile gegenüber traditionellen Ansätzen.
Klassische \gls{OCR}-Systeme extrahieren zwar Text aus Bildern, verlieren dabei aber wichtige strukturelle Informationen wie Layout, Tabellen oder die Anordnung von Textfeldern.
\glspl{VLM} hingegen verarbeiten das Dokument als Bild und können sowohl textuelle als auch visuelle Hinweise nutzen.

Bei Formularen und Bescheinigungen ist diese Fähigkeit besonders nützlich.
Solche Dokumente folgen oft standardisierten Layouts, bei denen die Position eines Textfeldes seine Bedeutung bestimmt.
Ein \gls{VLM} kann beispielsweise erkennen, dass ein Datum rechts oben in einem Formular wahrscheinlich das Ausstellungsdatum ist, während dasselbe Datum an anderer Stelle eine andere Bedeutung haben könnte.

Zudem können \glspl{VLM} mit verschiedenen Dokumentqualitäten umgehen.
Gescannte Formulare mit schlechter Bildqualität, handschriftlichen Einträgen oder teilweise unleserlichen Bereichen stellen für reine \gls{OCR}-Systeme oft Probleme dar.
\glspl{VLM} können durch ihr Verständnis der Gesamtstruktur und des Kontextes auch in solchen Fällen noch relevante Informationen extrahieren.
Die Modelle nutzen ihr trainiertes Wissen über typische Dokumentstrukturen, um fehlende oder unleserliche Informationen zu kompensieren.


\section{Vorstellung der Modelle}\label{sec:models}

Für die Klassifikation und der Extraktion der Informationen aus den Dokumenten wurden drei verschiedene \glspl{VLM} evaluiert.
Neben einem europäischen Mitstreiter von der französischen Firma Mistral AI werden Modelle des chinesischen Unternehmens Alibaba Cloud hinzugezogen.


\subsection{Pixtral-12B}\label{subsec:pixtral}

Das Pixtral-12B-2409\cite{Pixtral12B} wurde im Jahr 2024 von Mistral AI veröffentlicht.
Es basiert auf einem 12 Milliarden Parameter großen Text-Decoder mit einem zusätzlich 400 Millionen Parameter Vision-Encoder.
Es ist speziell auf das Verständnis von Bildern und Dokumenten\cite{PixtralPaper} trainiert wurden, weshalb es einen optimalen Kandidaten für die vorliegende Arbeit darstellt.

Mit einem theoretischen Kontextfenster von 128000 Tokens, ermöglicht das Modell eine Verarbeitung von mehreren Bildern gleichzeitig.
Das Kontextfenster wird in der Realität aber durch die gegebenen Ressourcen beschränkt.


\subsection{Qwen-2.5-VL}\label{subsec:qwen2.5vl}

Das Qwen2.5-VL-7B-instruct\cite{Qwen7B} sowie das Qwen2.5-VL-32B-instruct\cite{Qwen32B} wurden im Jahr 2025 von Alibaba Cloud veröffentlicht.
Das kleinere Modell mit 7 Milliarden Parametern ist speziell für den Einsatz in ressourcenbeschränkten Umgebungen konzipiert und bietet dennoch eine wettbewerbsfähige Leistung im Vergleich zu deutlich größeren Modellen.

Ein zentraler Bestandteil der Architektur ist die Nutzung der Native Dynamic Resolution.
Ähnlich wie das Pixtral verarbeitet Qwen Bilder in ihrer ursprünglichen Auflösung, optimiert diesen Ansatz jedoch durch die Integration von Multimodal Rotary Positional Embeddings (M-RoPE).
Diese Technik zerlegt die Positionsinformationen in Zeit-, Höhen- und Breitenkomponenten, wodurch das Modell in der Lage ist, 1D-Textinformationen, 2D-Visuelle-Informationen und 3D-Video-Informationen in einem einheitlichen Kontext zu verknüpfen.
Dies ist besonders vorteilhaft für das Verständnis komplexer Dokumentenlayouts, da feine Details und räumliche Beziehungen präziser erfasst werden.

Ein entscheidender Vorteil gegenüber generalistischen Modellen liegt in den Trainingsdaten: Qwen-2.5-VL wurde intensiv auf Document Omni-Parsing trainiert.
Dadurch eignet es sich besonders gut für die hier geforderte Extraktion strukturierter Daten aus Formularen und Tabellen, da es nicht nur den Text erkennt, sondern auch dessen semantische Struktur innerhalb des Layouts interpretiert\cite{QwenPaper}.


\subsection{Benchmarks}\label{subsec:benchmarks}

Im Folgenden sind die Benchmarks der einzelnen Modelle gelistet.
Besonders das Ergebnis des DocVQA-Benchmarks ist interessant, da hier ein Visual Question Answering (\gls{VQA}) auf Bildern von Dokumenten gemacht wird\cite{DocVQA}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Modell} & \textbf{Pixtral-12B} & \textbf{Qwen2.5-VL-7B} & \textbf{Qwen2.5-VL-32B} \\
        \hline
        \textbf{DocVQA} & 90.7 & 95.7 & 94.8 \\
        \textbf{MMMU} & 52.0 & 58.6 & 70 \\
        \hline
    \end{tabular}
    \caption{Benchmark-Ergebnisse der evaluierten Modelle\cite{QwenPaper,Qwen32B, PixtralPaper}}
    \label{tab:benchmark}
\end{table}


\section{Parameter-Efficient Fine-Tuning (PEFT)}\label{sec:peft}

Um eines der vorgestellten Modelle domänenspezifisch anzupassen, wurde ein \gls{PEFT} angestrebt.
Hierbei werden im Gegensatz zu einem Full-Finetuning nicht alle Modellparameter angepasst, was den Ressourcenverbrauch erheblich senkt.
Statt sämtliche Gewichte des Modells zu verändern, werden beim \gls{PEFT} kleine, zusätzliche Parameterstrukturen in das Modell eingefügt, während der Großteil des neuronalen Netzes eingefroren bleibt.

Eine der etabliertesten Methoden im Bereich des \gls{PEFT} ist die Low-Rank Adaptation (\gls{LoRA}).
Anstatt das gesamte Modell neu zu trainieren, friert \gls{LoRA} die Pre-trained Weights ein und integriert stattdessen Low-Rank Matrizen in die Linear-Layers der Transformer-Architektur.
Dieser Ansatz reduziert die Anzahl der trainierbaren Parameter drastisch, ohne die Performance signifikant zu beeinträchtigen.
Da für das eingefrorene Basismodell keine Gradienten berechnet werden müssen, sinkt der Speicherbedarf massiv, was das Training auch auf Hardware mit begrenztem vRAM ermöglicht.

Für die technische Umsetzung dieser Methode kommt die Bibliothek Unsloth zum Einsatz.
Unsloth ist eine optimierte Bibliothek für das Fine-Tuning von \glspl{LLM}, die speziell darauf ausgelegt ist, den Trainingsprozess zu beschleunigen und den Speicherbedarf zu minimieren.
Dies wird durch spezielle GPU-Kernel erreicht, welche die \gls{LoRA} und die Backpropagation effizienter berechnen.


\section{Information Extraction und Metriken}\label{sec:ieandmetrics}

\subsection{Information Extraction}\label{subsec:ie}

Information Extraction (\gls{IE}) umfasst nach Jurafsky und Martin die Aufgabe, wiederkehrende Ereignisse oder Situationen in Dokumenten zu identifizieren und die entsprechenden Felder eines vorgegebenen Templates zu füllen\cite{IE}.
Ziel hierbei ist es, vordefinierte Entitäten aus einem Dokumentenbild zu extrahieren und diese in ein maschinenlesbares Schema zu überführen.


\subsection{Metriken}\label{subsec:metriken}
Für die Bewertung von der Klassifikation und der Information Extraction werden verschiedene Metriken herangezogen.

Die Güte der Multi-Class Klassifikation wird mit den klassischen Metriken Accuracy, Precision, Recall und dem F1-Score bewertet.
\begin{equation}\label{eq:accuracy}
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\end{equation}

\begin{equation}\label{eq:precision}
    Precision (P) = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}\label{eq:recall}
    Recall (R) = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}\label{eq:f1score}
    F1-Score = 2 \cdot \frac{P \cdot R}{P + R} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}
\end{equation}


Die Bewertung der Information Extraction ist um einiges komplexer als die der Klassifikation.
Um dennoch die Güte der \gls{IE} bewerten zu können, wurde ein feld-basiertet Ansatz gewählt.
Dabei wird jedes einzelne Feld der generierten \gls{JSON} mit dem der Ground Truth verglichen.
Ein Feld gilt hierbei als True Positive (\gls{TP}), wenn das Key des Feldes vorhanden ist, als auch der Value des Feldes mit dem der Ground Truth übereinstimmt\cite{ICDAR}.

Des Weiteren werden zum Beispiel Namen nicht exakt, sondern mithilfe der Levenshtein-Similarity verglichen.
Die Levenshtein-Similarity ist ein aus der Levenshtein-Distanz normalisierter Ähnlichkeitswert zwischen zwei Zeichenketten.
Formal basiert sie auf der Levenshtein-Distanz also der minimalen Anzahl von Einfügungen, Löschungen oder Ersetzungen, die nötig sind, um eine Zeichenkette in eine andere Zeichenkette zu überführen\cite{gld}.


\subsection{Ressourceneffizienz}\label{subsec:ressources}
Neben der Qualität der Vorhersagen spielen beim Einsatz von \glspl{VLM} in lokalen Umgebungen Effizienzmetriken eine entscheidende Rolle:
\begin{itemize}
    \item \textbf{Latenz:} Die Zeitdauer, die das System für die Inferenz einer einzigen Antwort benötigt.
    \item \textbf{\gls{VRAM}-Auslastung:} Der maximale Bedarf an Grafikspeicher während der Verarbeitung.
    \item \textbf{Leistungsaufnahme:} Der durchschnittliche Energieverbrauch der GPU während der Inferenz.
\end{itemize}

\section{Verwandte Arbeiten}\label{sec:relatedwork}
