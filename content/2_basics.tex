\chapter{Theoretische Grundlagen}\label{ch:basics}


\section{Fachliche Grundlagen}\label{sec:domain}

\subsection{Der Ist-Zustand}\label{subsec:current}

Wie bereits in der Einleitung angedeutet, setzt sich das aktuelle System aus einer mehrstufigen Pipeline zusammen.
Der schematische Ablauf ist in Abbildung\ref{fig:aube} dargestellt und lässt sich in drei logische Phasen unterteilen: Eingangsverarbeitung, Klassifikation und Informationsextraktion.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/aube}
    \caption{Schematischer Workflow der aktuellen AuBe-Pipeline}
    \label{fig:aube}
\end{figure}

Der Prozess beginnt mit dem Upload eines Dokuments durch den Kunden.
Ein vorgeschaltetes System nimmt diese Dateien entgegen, die als PDF oder Bildformate vorliegen können.
In diesem Schritt findet eine initiale \gls{OCR}-Verarbeitung statt.
Das Ergebnis, welches an die \gls{AuBe}-Pipeline übergeben wird, ist ein standardisiertes PDF/A-Dokument sowie der extrahierte Text.

Im ersten Schritt der eigentlichen \gls{AuBe}-Pipeline entscheidet ein Text-Klassifikator auf Basis des \gls{OCR}-Textes, um welche Art von Dokument es sich handelt.
Hierbei wird zwischen drei Kategorien unterschieden:

\begin{itemize}
    \item \textbf{KG5b:} Eine spezifische Arbeitgeberbescheinigung, die für das Kindergeld relevant ist.
    \item \textbf{Vertrag:} Ein klassischer Berufsausbildungsvertrag.
    \item \textbf{Sonstiges:} Alle Dokumente, die nicht den beiden oben genannten Typen entsprechen.
\end{itemize}

Eine detaillierte Beschreibung der Dokumentenarten erfolgt in Kapitel\ref{ch:data}.

Wird ein Dokument als „Sonstiges“ klassifiziert, endet die Verarbeitung an dieser Stelle.
Nur die für den Fachprozess relevanten Typen werden weiterverarbeitet.

Für die relevanten Typen wird das PDF/A in ein Bild konvertiert und einem \gls{YOLO}-Modell übergeben.
Dieses \gls{YOLO}-Modell zeichnet Bounding-Boxen um die relevanten Felder, wobei daraufhin, ein weiteres \gls{OCR}-Modell die Erkennung in diesen Boxen durchführt


\subsection{Rahmenbedingungen und Infrastruktur}\label{subsec:infrastructure}


Die Entwicklung und Evaluation der Modelle erfolgt unter strengen datenschutzrechtlichen Auflagen.
Da im Rahmen dieser Arbeit personenbezogene Echtdaten verarbeitet werden, ist eine Nutzung öffentlicher Cloud-API-Dienste (wie OpenAI oder Anthropic) ausgeschlossen.
Stattdessen wird eine vollständig isolierte On-Premise-Infrastruktur verwendet.

Die technische Architektur ist in Abbildung\ref{fig:infrastructure} schematisch dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/infrastructure}
    \caption{Schematische Darstellung der Trainings- und Inferenzinfrastruktur}
    \label{fig:infrastructure}
\end{figure}

Das System basiert auf einem abgeschotteten Kubernetes-Cluster.
Der Zugang zur Entwicklungsumgebung erfolgt für den Entwickler ausschließlich über eine noVNC-Schnittstelle (browserbasierter Remote-Desktop).
Dies stellt sicher, dass keine sensiblen Daten die gesicherte Umgebung verlassen.

Um die Bereitstellung der \gls{VLM}s zu erleichtern, wird auf dem Cluster Kubeflow verwendet.
Kubeflow ist eine Open-Source-Plattform die speziell für das Entwickeln, Trainieren und Deployen von Machine-Learning-Modellen entwickelt wurde.

Für die rechenintensiven Aufgaben, insbesondere das Fine-Tuning und die Inferenz der \gls{VLM}s, stehen innerhalb des Clusters zwei NVIDIA A40 GPUs, mit jeweils 48 GB vRAM\cite{NVIDIAA40}, zur Verfügung.


\section{Vision-Language Models (VLMs)}\label{sec:vlms}

\subsection{Vom Sprachmodell zum multimodalen Modell}\label{subsec:mllm}

Moderne Large Language Models bauen auf einer Transformerarchitektur auf.
Diese Transformer bestehen aus zwei Teilen, dem Encoder und dem Decoder.
Ein Encoder nimmt eine Eingabe entgegen und wandelt diese in einen Vektor um.
Ein Decoder nimmt einen solchen Vektor und generiert daraus Schritt für Schritt die Ausgabesequenz.
Modelle, die ausschließlich Text generieren, implementieren nur den Decoder des Transformers, weshalb man \gls{LLM}s auch Decoder-Only Modelle nennt.

Die Stärke dieser Decoder-Only Modelle liegt in ihrer Fähigkeit, natürliche Sprache zu verstehen und zu generieren.
Um auch visuelle Informationen verarbeiten zu können, wurden \gls{VLM} entwickelt.
Diese erweitern die Decoder-Only Architektur um eine Vision-Komponente, die es ermöglicht, Bilder als Eingabe zu verwenden.
Der grundlegende Ansatz besteht darin, visuelle Informationen in eine Form zu bringen, die das Sprachmodell verarbeiten kann.
Dafür wird ein separater Vision Encoder verwendet, der Bilder in Vektorrepräsentationen transformiert, die dann zusammen mit textuellen Tokens dem Sprachmodell übergeben werden.

\subsection{Architekturkomponenten}\label{subsec:components}

Ein \gls{VLM} besteht aus drei Hauptkomponenten, die in Abbildung\ref{fig:vlm} dargestellt sind.

Der Vision Encoder verarbeitet die Bildeingabe und extrahiert visuelle Features.
Hierfür kommen häufig vortrainierte Modelle wie Vision Transformer (ViT) oder CLIP zum Einsatz.
Diese Modelle zerlegen ein Bild in kleinere Patches, die ähnlich wie Tokens in Sprachmodellen behandelt werden.
Jeder Patch wird in einen hochdimensionalen Vektor verwandelt, der die visuellen Gegebenheiten dieses Bildausschnitts repräsentiert.
Das Ergebnis ist eine Sequenz von Bildvektoren, die die visuelle Information des gesamten Bildes kodieren.

Der Adapter verbindet den Vision Encoder mit dem Sprachmodell.
Diese Schicht transformiert die Ausgabe des Vision Encoders in ein Format, das mit den Textembeddings des Sprachmodells kompatibel ist.
Dabei werden die visuellen Vektoren in denselben Vektorraum projiziert, in dem auch die textuellen Tokens liegen.
Diese Komponente ist entscheidend, damit das Sprachmodell visuelle und textuelle Informationen gemeinsam verarbeiten kann.
In vielen Architekturen besteht diese Schicht aus einem einfachen linearen Layer oder einem kleinen neuronalen Netz.

Das Language Model bildet die dritte Komponente und ist für die eigentliche Verarbeitung und Generierung zuständig.
Hier kommen etablierte \gls{LLM}-Architekturen zum Einsatz, die auf großen Textkorpora vortrainiert wurden.
Das Sprachmodell erhält als Eingabe eine gemischte Sequenz aus projizierten Bildvektoren und Textembeddings.
Durch die Attention-Mechanismen des Transformers kann das Modell Beziehungen zwischen visuellen und textuellen Elementen lernen.
So ist es möglich, Fragen zu Bildinhalten zu beantworten oder Bildbeschreibungen zu generieren.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/vlm}
    \caption{Architektur eines Vision Language Models}
    \label{fig:vlm}
\end{figure}

\subsection{VLMs für Document Understanding}\label{subsec:vlm-documents}

Für die Extraktion von Informationen aus Dokumenten bieten \glspl{VLM} entscheidende Vorteile gegenüber traditionellen Ansätzen.
Klassische \gls{OCR}-Systeme extrahieren zwar Text aus Bildern, verlieren dabei aber wichtige strukturelle Informationen wie Layout, Tabellen oder die Anordnung von Textfeldern.
\glspl{VLM} hingegen verarbeiten das Dokument als Bild und können sowohl textuelle als auch visuelle Hinweise nutzen.

Bei Formularen und Bescheinigungen ist diese Fähigkeit besonders nützlich.
Solche Dokumente folgen oft standardisierten Layouts, bei denen die Position eines Textfeldes seine Bedeutung bestimmt.
Ein \gls{VLM} kann beispielsweise erkennen, dass ein Datum rechts oben in einem Formular wahrscheinlich das Ausstellungsdatum ist, während dasselbe Datum an anderer Stelle eine andere Bedeutung haben könnte.

Zudem können \glspl{VLM} mit verschiedenen Dokumentqualitäten umgehen.
Gescannte Formulare mit schlechter Bildqualität, handschriftlichen Einträgen oder teilweise unleserlichen Bereichen stellen für reine \gls{OCR}-Systeme oft Probleme dar.
\glspl{VLM} können durch ihr Verständnis der Gesamtstruktur und des Kontextes auch in solchen Fällen noch relevante Informationen extrahieren.
Die Modelle nutzen ihr trainiertes Wissen über typische Dokumentstrukturen, um fehlende oder unleserliche Informationen zu kompensieren.


\section{Vorstellung der Modelle}\label{sec:models}

\subsection{Qwen-2.5-VL}\label{subsec:qwen2.5vl}

\subsection{Pixtral-12B}\label{subsec:pixtral}

\subsection{Benchmarks}\label{subsec:benchmarks}



\section{Fine-Tuning}\label{sec:finetuning}

\subsection{Parameter-Efficient Fine-Tuning (PEFT)}\label{subsec:peft}


\section{Information Extraction und Metriken}\label{sec:ieandmetrics}

\subsection{Information Extraction}\label{subsec:ie}

\subsection{Metriken}\label{subsec:metrics}


\section{Verwandte Arbeiten}\label{sec:relatedwork}
