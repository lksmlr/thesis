\chapter{Methodik und Datenaufbereitung}\label{ch:data}

Im Rahmen dieses Proof of Concepts werden verschiedene \glspl{VLM} vergleichend gegenüber der bestehenden \gls{YOLO}/\gls{OCR}-Pipeline evaluiert.
Für die Entscheidung, ob und welches \gls{VLM} die bisherige Pipeline ersetzen wird, sind sowohl die Modellleistungen als auch der Ressourcenverbrauch von zentraler Bedeutung.
Aus diesem Grund sollen die folgenden Forschungsfragen beantwortet werden:

\begin{enumerate}
    \item Inwiefern ist ein nicht angepasstes \gls{VLM} hinsichtlich der Klassifikations- und Extraktionsgenauigkeit der trainierten \gls{YOLO}/\gls{OCR}-Pipeline überlegen, und welche Vorteile bietet ein einzelnes generalistisches Modell gegenüber der Verwendung spezialisierter Einzelsysteme?
    \item Wie verhält sich ein kleineres, domänenspezifisch trainiertes Modell gegenüber einem leistungsstärkeren Basismodell in Bezug auf Performanz und Effizienz?
    \item In welchem Verhältnis steht der Ressourcenverbrauch der \glspl{VLM} zu dem der \gls{YOLO}/\gls{OCR}-Pipeline?
\end{enumerate}

\section{Modellierung der Dokumentenarten als JSON}\label{sec:documents_as_json}

Um eine standardisierte Weiterverarbeitung der Modellausgaben zu gewährleisten, ist eine feste Struktur essenziell.
Da moderne \glspl{VLM} darauf trainiert sind, Antworten im \gls{JSON}-Format zu liefern, werden die extrahierten Informationen in ein vordefiniertes \gls{JSON}-Schema überführt.
Ein wesentlicher Vorteil gegenüber der herkömmlichen Pipeline ist die gleichzeitige Durchführung der Klassifikation sowie der \gls{IE} in einem einzigen Inferenzschritt.
Das Modell klassifiziert die Dokumentenart, die im Feld \texttt{type} abgebildet wird.
Während das Feld \texttt{type} in allen \gls{JSON}-Schemata konsistent vorhanden ist, variieren die Felder der \gls{IE} je nach Dokumentenart.

Bei den binären, booleschen Feldern markiert ein \texttt{true} das Vorhandensein eines Merkmals, während ein \texttt{false} dessen Fehlen oder das Nicht-Erkennen repräsentiert.
Um eine strukturelle Konsistenz zu erzwingen, werden Felder, bei denen die dazugehörige Information im Dokument fehlt, bei booleschen Typen mit \texttt{false} und bei Textfeldern mit einer leeren Zeichenkette belegt.

Da die Dokumente in den meisten Fällen aus mehreren Seiten bestehen, repräsentiert ein \gls{JSON}-Objekt das mehrseitige Dokument.
Sollten Dokumente verschiedener Arten vermischt vorliegen, generiert das Modell für jede erkannte Dokumentenart ein separates \gls{JSON}-Objekt.

Um den \gls{JSON}-Output der \glspl{VLM} zu limitieren, existieren verschiedene Ansätze.
Während komplexere Lösungen nur Tokens zulassen die in \glspl{JSON} enthalten sein könnten, wird in dieser Evaluation ein einfacherer Ansatz gewählt.
Hierbei werden aus der generierten Antwort \glspl{JSON} gefiltert und geparst.
Sobald das Parsen der \gls{JSON} fehlschlägt, wird diese zurück an das \gls{VLM} geliefert, mit der Anweisung diese zu korrigieren.
Das Modell bekommt insgesamt drei Versuche eine valide \gls{JSON} zu generieren, bevor das Dokument als ungültig eingestuft wird.


\subsection{KG5b}\label{subsec:json_kg5b}

Das Schema für das Formular KG5b ist in Abbildung~\ref{fig:json_kg5b} definiert.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/kg5b_json}
    \caption{JSON-Schema des Dokumententyps KG5b}
    \label{fig:json_kg5b}
\end{figure}


\subsection{Ausbildungsvertrag}\label{subsec:json_vertrag}

Das Schema für die Ausbildungsverträge ist in Abbildung~\ref{fig:json_vertrag} definiert.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/vertrag_json}
    \caption{JSON-Schema des Dokumententyps Ausbildungsvertrag}
    \label{fig:json_vertrag}
\end{figure}


\subsection{Sonstige Dokumente}\label{subsec:json_sonstiges}

Das Schema für die nicht relevanten Dokumente ist in Abbildung~\ref{fig:json_sonstiges} definiert.
Da hier keine Informationen benötigt werden, sondern rein die Klassifikation von Nutzen ist, besteht das Schema ausschließlich aus dem Feld \texttt{type}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/sonstiges_json}
    \caption{JSON-Schema des Dokumententyps Sonstiges}
    \label{fig:json_sonstiges}
\end{figure}


\section{Evaluation}\label{sec:evaluation}

Um die generierte \gls{JSON} des \glspl{VLM} schlussendlich bewerten zu können, werden die Klassifikation und \gls{IE} unabhängig voneinander bewertet.


\subsection{Vergleichslogik der unterschiedlichen JSON-Felder}\label{subsec:comparator}

Mit dem Ziel, ein realistisches Ergebnis zu erhalten, müssen unterschiedliche Feldtypen spezifisch verglichen werden.
Die Menge an distinkten Typen beinhaltet: Namen, Booleans, Zeichenketten, Daten und Monate.

Namen werden mit der Levenshtein-Similarity verglichen, um kleinere Fehler oder verschiedene Varianten zu tolerieren.
Beispielsweise wird ein Name, der \texttt{ue} statt \texttt{ü} enthält, nicht als Fehler erkannt.
Der Schwellenwert, der zwischen korrekt und nicht korrekt entscheidet, liegt bei 0,8.
Des Weiteren werden minimale \gls{OCR}-Fehler gefiltert.
Die \gls{YOLO}/\gls{OCR}-Pipeline verwendet ebenso diesen Vergleich mit den gleichen Schwellenwerten, somit wird das Ergebnis vergleichbar.

Zeichenketten und Booleans werden exakt verglichen.
Hierzu zählen die Felder \texttt{type}, \texttt{stamp\_company}, \texttt{signature\_company}, \texttt{signature\_child}, \texttt{signature\_legal\_guardian} und \texttt{apprenticeship\_finished}.
Lediglich die booleschen Werte werden zu \texttt{true} normalisiert, da hier eine potenzielle Fehlerquelle liegt.

Daten werden einheitlich in das Format DD.MM.YYYY gebracht, obwohl dies nicht dem internationalen Standard entspricht.
Das ist darauf zurückzuführen, dass so die Erkennung des \glspl{VLM} am robustesten ist, da die deutschen Dokumente auch mit diesem Format ausgefüllt werden.

Monate werden in das Format MM normalisiert.
Hierbei werden ausgeschriebene Monatsnamen sowie vollständige Datumsangaben angepasst.


\subsection{Bewertung der Klassifikation}\label{subsec:evaluation_classification}

Die Bewertung der Klassifikation ist ein Multiclass-Problem, da hier mehr als zwei Klassen vorliegen (KG5b, Ausbildungsvertrag, Sonstiges).
Um die Güte der Modelle zu betrachten, wird eine Confusion-Matrix herangezogen, wobei der Fokus auf dem F1-Score liegt.


\subsection{Bewertung der Information Extraction}\label{subsec:evaluation_ie}

Um den Entity-Level F1-Score, der die Hauptmetrik für den Vergleich darstellt, zu berechnen werden einige Metadaten für jede generierte \gls{JSON} bestimmt.
Dazu gehören die Status der Felder, also ob diese vorhanden, nicht vorhanden oder halluziniert sind.
Sobald ein Feld als vorhanden markiert wird, wird mit den verschiedenen Vergleichslogiken die Richtigkeit der Felder bestimmt und den Metadaten hinzugefügt.

Auf Basis dieser Metadaten werden für den gesamten Dokumentenkorpus die True Positive (\gls{TP}), False Positive (\gls{FP}) und False Negative (\gls{FN}) gezählt.
Ein Feld zählt als \gls{TP}, wenn es vorhanden sowie korrekt ist.
Zu den \gls{FN} gehören Felder, die entweder fehlen oder vorhanden, aber falsch sind.
Ein Feld, das vorhanden aber falsch ist, zählt zusätzlich zu den \gls{FP} zusammen mit den Feldern die halluziniert wurden.
Diese doppelte Bestrafung stellt eine sehr strenge Bewertung für inhaltliche Fehler dar.
True Negatives (\gls{TN}) sind nicht zählbar, da die Menge an nicht gefundenen, leeren Feldern, theoretisch unendlich groß ist.


\subsection{Messung von Latenz, Energieverbrauch und VRAM-Nutzung}\label{subsec:measure}

Im Hinblick auf die Bewertung der Modelle werden neben der Güte der Klassifikation und \gls{IE} die Latenz, die \gls{VRAM}-Nutzung sowie der Energieverbrauch gemessen.

Die Latenz der Modelle stellt lediglich das Delta zwischen Start- und Endzeit der Inferenz dar.
Hierbei wird der Zeitpunkt gemessen, an dem die eigentliche Inferenz startet und wenn die Antwort bereitsteht.
Das Laden des Modells wird nicht betrachtet, da diese keinen Einfluss auf die Antwortzeit in der produktiven Umgebung hat und folglich nicht relevant für die Evaluation ist.

Des Weiteren wird der Energieverbrauch während der Inferenz in Joule erfasst.
Die Messung der Leistung in Watt ist nicht zielführend, da sie die Zeit nicht berücksichtigt und Modelle mit geringerer Latenz bei vergleichbarem Energieverbrauch benachteiligt werden.

Wie auch der Energieverbrauch wird die \gls{VRAM}-Nutzung während der Inferenz gemessen.
Der Messzeitraum für den Energieverbrauch als auch für die \gls{VRAM}-Nutzung ist gleich der Latenz der Modelle.
Mit der Initialisierung des Modells wird einmalig der statische Verbrauch gemessen.


\section{Daten und Wahl des Basismodells}\label{sec:data}

\subsection{Datenvorbereitung}\label{subsec:preprocessing}

Durch Datenlieferungen der Familienkasse stehen mehrere Tausend Dokumente als PDF oder Bilddatei zur Verfügung.
Um eine einheitliche Basis zu schaffen und die Klassifikations- und Extraktionsgüte zu verbessern, müssen die Daten vorverarbeitet werden.
Im Fokus stehen dabei die Korrektur der Bildausrichtung sowie die Konvertierung der PDF-Dokumente in Bilder.
Zur Korrektur der Rotation werden die Exchangeable Image File Format (\gls{EXIF})-Metadaten ausgelesen, um die ursprüngliche Ausrichtung wiederherzustellen.
Während ältere \glspl{VLM} Bilder oft auf fixe Größe skalierten, unterstützen die in dieser Arbeit evaluierten Modelle eine dynamische Skalierung.
Um jedoch die maximale Token-Anzahl, werden die Bilder auf eine maximale Pixelanzahl skaliert, wobei das ursprüngliche Seitenverhältnis beibehalten wird, um Details zu erhalten.


\subsection{Testdatensatz}\label{subsec:test_dataset}

Nach der Vorbereitung der Dokumente wurde ein Testdatensatz erstellt, der als Vergleichsbasis dient.
Dieser umfasst insgesamt 60 Dokumente, aufgeteilt in 20 Beispiele der jeweiligen Dokumentenarten.
Für eine möglichst genaue Darstellung der Realität, werden die Dokumente manuell ausgewählt.
% TODO: Wie setzt sich dieser Datensatz zusammen?
Zur Erstellung der Wahrheitswerte (Ground Truth) wurden alle Dokumente manuell annotiert und in das entsprechende \gls{JSON}-Format überführt.


\subsection{Prompt-Design}\label{subsec:prompting}
% TODO: Section erweitern
Für die Generierung der Schemata wurde ein Zero-Shot Prompt gewählt.
Dabei enthält der Prompt neben der Aufgabenstellung die drei definierten \gls{JSON}-Schemata.

Die Entscheidung gegen einen One-Shot oder Few-Shot Prompt fiel wegen des erhöhten Token-Verbrauchs.
Des Weiteren würde durch das Mitgeben von Beispielen die Latenz steigen.


\subsection{Modellauswahl}\label{subsec:modelselection}

Für die domänenspezifische Anpassung an die Dokumententypen muss ein geeignetes Basismodell gefunden werden.
Wie in Abbildung~\ref{fig:model_selection} dargestellt, erfolgte die Auswahl in einem iterativen Prozess.

In jeder Iteration wurden das Pixtral-12B sowie das Qwen-2.5-VL-7B gegen den Testdatensatz evaluiert.
Die Ergebnisse wurden analysiert, um das Prompt-Design stetig zu verfeinern.
Dieser Prozess wurde so lange fortgesetzt, bis durch die Anpassungen keine Steigerung der Metriken mehr erzielt werden konnte.


\subsection{Trainingsdatensatz}\label{subsec:train_dataset}
% TODO: Wie setzt sich dieser Datensatz zusammen?
Um ein qualitativ hochwertiges Fine-Tuning zu ermöglichen, ist ein deutlich umfangreicherer Datensatz als für die Evaluation erforderlich.
Der finale Trainingsdatensatz besteht aus 227 Verträgen, 165 KG5b-Formularen und 218 sonstigen Dokumenten.

Die Klassen sind ungleich verteilt, da KG5b-Formulare ein starres Layout aufweisen und somit ein gutes Ergebnis bei niedrigerer Anzahl erwartet wird.
Klassen mit hoher Varianz, wie die Ausbildungsverträge, wurden hingegen stärker gewichtet.
Zur effizienten Erstellung dieses Datensatzes wurde ein Model-Assisted Labeling eingesetzt.
Das in dem Prozess der Modellauswahl ermittelte beste Basismodell generierte mithilfe des dort evaluierten Prompts die \glspl{JSON}-Schemata für die Dokumente, die anschließend manuell geprüft und korrigiert wurden.
Dies reduzierte den Aufwand erheblich.


\section{Fine-Tuning}\label{sec:fine_tuning}

