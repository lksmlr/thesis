\chapter{Ergebnisse}\label{ch:results}
% TODO: Penalize hallucinations false
\section{Ergebnisse der OCR/YOLO-Pipeline gegenüber den Basismodellen}\label{sec:results_ie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/results_ie}
    \caption{Ergebnisse der Information Extraction}
    \label{fig:results_ie}
\end{figure}

Abbildung~\ref{fig:results_ie} stellt die F1-Scores der Basismodelle im Vergleich zur OCR/YOLO-Pipeline für die KG5b-Formulare und Ausbildungsverträge dar.

Bei den KG5b-Formularen zeigt sich ein enges Feld der Qwen-Modelle und der OCR/YOLO-Pipeline.
Die Pipeline liegt mit einem F1-Score von 0,75 geringfügig hinter dem Qwen-2.5-VL-7B (0,80).
Das größere Qwen-2.5-VL-32B bildet die Spitze mit 0,87, während sich das Pixtral-12B am unteren Ende befindet.

Ein differenzierteres Bild zeigt sich bei den heterogenen Verträgen.
Die OCR/YOLO-Pipeline fällt signifikant auf einen F1-Score von 0,51 ab.
Demgegenüber stehen die Qwen-Modelle mit 0,94 (Qwen-2.5-VL-32B) und 0,84 (Qwen-2.5-VL-7B), die bei den Verträgen besser abschnitten als bei den KG5b-Formularen..
Das Pixtral-12B bildet mit 0,50 erneut das Schlusslicht, liegt jedoch nur minimal hinter der OCR/YOLO-Pipeline.

Die Ergebnisse der Information Extraction zeigen die Abhängigkeit der Leistungsfähigkeit von der Dokumentenart bei der OCR/YOLO-Pipeline.
Im Vergleich zu den Qwen-Modellen sinkt die Leistung bei den variablen Verträgen gegenüber den starren KG5b-Formularen signifikant.
Die Daten belegen zusätzlich, das bereits ein kleines Modell wie das Qwen-2.5-VL-7B die Pipeline übertrifft.
In keiner Dokumentenart schnitt das Pixtral-12B besser als die bisherige Lösung ab, weswegen es unbrauchbar für diese Anwendung ist.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification}
\end{figure}

Abbildung~\ref{fig:results_classification} visualisiert die Klassifikationsleistung der Basismodelle und der OCR/YOLO-Pipeline anhand von F1-Scores.

Im Gegensatz zu der Information Extraction zeigt sich hier ein homogenes Bild.
Das Qwen-2.5-VL-32B erzielt in allen drei Kategorien die besten Ergebnisse und klassifiziert die KG5b-Formulare sogar mit einem Score von 1,0.
Die Ergebnisse des Pixtral-12B, Qwen-2.5-VL-7B und die OCR/YOLO-Pipeline liegen über alle Dokumentenarten hinweg dicht beisamen.
In der Kategorie der sonstigen Dokumente sinken alle Modelle gegenüber den anderen Dokumentenarten ab.
Hier erzielte das Pixtral-12B zusammen mit dem Qwen-2.5-VL-7B den niedrigsten Score mit 0,87.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_7b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_pixtral}
        \caption{Confusion Matrix für das Pixtral-12B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_yolo_ocr}
        \caption{Confusion Matrix für die OCR/YOLO-Pipeline}
    \end{subfigure}

    \caption{Confusion Matrices aller Modelle}
    \label{fig:confusion_matrices}
\end{figure}

Die Confusion Matrices in Abbildung~\ref{fig:confusion_matrices} repräsentiert die Ergebnisse der Klassifikation im Detail.

Während die F1-Scores der Klassifikation ein ähnliches Leistungsniveau zeigen, offenbaren die Matrizen unterschiedliche Fehlerschwerpunkte.
Das Qwen-2.5-VL-32B klassifiziert sowohl die KG5b-Formulare als auch die Verträge fehlerfrei.
Im Gegensatz dazu erkennt die OCR/YOLO-Pipeline die sonstigen Dokumente perfekt, ordnete aber fünf der anderen Dokumentenarten dieser zusätzlich zu.
Das Qwen-2.5-VL-7B und das Pixtral-12B haben ein ähnliches Fehlerschema und erkannten die Verträge perfekt.
Allerdings klassifizierten beiden Modelle KG5b-Formulare sowie sonstige Dokumente häufig als Vertrag.

Die gemeinsame Betrachtung der F1-Scores und der Confusion Matrices verdeutlichen, dass trotz quantitativ vergleichbarer Ergebnisse die Modelle qualitative Unterschiede aufweisen.
Besonders die Stabilität des Qwen-2.5-VL-32B ist hervorzuheben, da die relevanten Dokumente fehlerfrei erkannt werden.
Gegenüber stehen die kleineren VLMs, die Probleme mit der Trennung der Dokumentenklassen haben.
Die OCR/YOLO-Pipeline neigt zur Zuordnung zu den sonstigen Dokumenten, wodurch relevante Dokumente verloren gehen.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency}
    \caption{Ergebnisse der Latenzmessung}
    \label{fig:results_latency}
\end{figure}

Das Diagramm~\ref{fig:results_latency} zeigt die durchschnittliche und maximale \gls{Latenz} in Sekunden der Basismodelle und der OCR/YOLO-Pipeline.

Aufgrund der Architektur antwortet die OCR/YOLO-Pipeline mit einer durchschnittlichen \gls{Latenz} von 0,58 Sekunden nahezu in Echtzeit.
Gegenüber der Pipeline braucht selbst das kleinste Modell mit 7,43 Sekunden fast 13-mal länger.
Besonders auffällig ist die hohe maximale \gls{Latenz} des Qwen-2.5-VL-32B mit 252 Sekunden, womit dieses Modell deutlich über den anderen liegt.

Die \gls{Latenz}messung belegt den Zusammenhang der Modellgröße von VLMs mit dessen Antwortgeschwindigkeit.
Mit zunehmender Parameteranzahl wächst die \gls{Latenz} überproportional.

Mit den hier vorgestellten Ergebnissen ist zudem die Auswahl des Basismodells erfolgt.
Durch die Überlegenheit des Qwen-2.5-VL-7B gegenüber des Pixtral-12B in der Information Extraction sowie der Klassifikation, stellt sich das Qwen-Modell als besserer Kandidat für das training heraus.


\section{Ergebnisse des weitertrainierten Modells gegenüber dem größeren Basismodell}\label{sec:results_latency}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_ie_peft_32b_7b}
    \caption{Ergebnisse der Information Extraction}
    \label{fig:results_ie_peft_32b_7b}
\end{figure}

Abbildung~\ref{fig:results_ie_peft_32b_7b} stellt die F1-Scores der Information Extraction des Qwen-2.5-VL-7B-finetuned und des Qwen-2.5-VL-32B dar.

Bei den standardisierten KG5b-Formularen erzielten beide Modelle identische Ergebnisse mit einem F1-Score von 0,87.
Demgegenüber zeigen die Ausbildungsverträge einen deutlicheren Unterschied.
Während das Qwen-2.5-VL-32B ein F1-Score von 0,94 aufweist, liegt das angepasste Modell mit einer Differenz von 0,10 signifikant darunter.

Die Verteilung deutet darauf hin, dass ein domänenspezifisches Training ausreicht um bei starren Layouts auf die Leistung des Qwen-2.5-VL-32B zu schließen.
Bei variablen Dokumenten wie die Ausbildungsverträge profitiert das 32B-Modell jedoch weiterhin von seinem umfangreicheren Weltwissen.
Dadurch wird die Information Extraction präziser.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification_peft_32b_7b}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification_peft_32b_7b}
\end{figure}

Die Abbildung~\ref{fig:results_classification_peft_32b_7b} visualisiert die Klassifikationsleistung der Modelle anhand der F1-Scores.

Beide Modelle erreichen bei den KG5b-Formularen ein perfektes Ergebnis mit einem F1-Score von 1,0.
Die anderen beiden Kategorien legen einen minimalen Vorsprung des angepassten Modells dar.
Hier erreicht das Qwen-2.5-VL-7B-finetuned einen F1-Score von 0,97 bei den Verträgen und 0,95 bei den sonstigen Dokumenten.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_peft}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B-finetuned}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}
    \caption{Confusion Matrices aller Modelle}
    \label{fig:confusion_matrices_32b_peft}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency_peft_32b_7b}
    \caption{Ergebnisse der Latenzmessung pro Dokument}
    \label{fig:results_latency_peft_32b_7b}
\end{figure}



\section{Ressourcenverbrauch der VLMs gegenüber der OCR/YOLO-Pipeline}\label{sec:results_vram_usage}

Für den Vergleich des Ressourcenverbrauchs wird sowohl die \gls{VRAM}-Auslastung als auch der Energieverbrauch betrachtet.

In Abbildung~\ref{fig:results_vram_model} ist die statische \gls{VRAM}-Auslastung abgebildet.
Die niedrigste Auslastung hat die OCR/YOLO-Pipeline mit 4,21 GB\@.
Das Qwen-2.5-VL-7B und Qwen-2.5-VL-7B-finetuned befinden sich 16,4 GB und 16,88 GB auf einer Ebene.
Während das Pixtral-12B mit 23,89 GB knapp über den 7B Modellen liegt, ist das Qwen-2.5-VL-32B mit 64,18 GB mit Abstand das größte Modell.

Die dynamische \gls{VRAM}-Auslastung in Abbildung~\ref{fig:results_vram_inference} zeigt ein anderes Ergebnis, jedoch liegt auch hier die OCR/YOLO-Pipeline mit einer durchschnittlichen Auslastung von 0,49 GB und einer maximalen von 1,02 GB unter den Mitstreitern.
Interessant sind die Werte des Qwen-2.5-VL-7B-finetuned und des Qwen-2.5-VL-32B, denn hier liegt das kleinere Modell mit durchschnittlich 3,03 GB über dem großen Modell mit durchschnittlich 3,01 GB


\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_model}
    \caption{Ergebnisse der VRAM-Nutzung des geladenen Modells}
    \label{fig:results_vram_model}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_inference}
    \caption{Ergebnisse der VRAM-Nutzung während der Inferenz pro Dokument}
    \label{fig:results_vram_inference}
\end{figure}

Der Energieverbrauch in Abbildung~\ref{fig:results_energy_usage} zeigt den markantesten Unterschied zwischen der OCR/YOLO-Pipeline und den VLMs.
Während die Pipeline nur 117 J im Durchschnitt verbraucht, benötigt selbst das effizienteste Modell, das Qwen-2.5-VL-7B, etwas das 22-fache an Energie.
Im Vergleich zum Qwen-2.5-VL-32B, ist die Pipeline bei dem maximalen Verbrauch rund 250-fach sparsamer.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_energy}
    \caption{Ergebnisse des Energieverbrauchs pro Dokument}
    \label{fig:results_energy_usage}
\end{figure}