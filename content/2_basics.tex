\chapter{Theoretische Grundlagen}\label{ch:basics}

\section{Dokumentenarten}\label{sec:documenttypes}

Im Rahmen der Kindergeldbeantragung für Auszubildende sind verschiedene Nachweise gültig.
Zu den anerkannten Dokumententypen zählen der offizielle Vordruck der Bundesagentur für Arbeit (Formular KG5b)\cite{KG5b} sowie Ausbildungsverträge.
Die relevanten Informationen aus diesen Dokumenten sind von der fachlichen Seite vorgegeben.
Zusätzlich laden Kunden häufig weitere Unterlagen, wie beispielsweise Schulbescheinigungen, im Portal hoch.
Da diese für den Kindergeldantrag nicht im Fokus stehen, fallen sie im Folgenden in die Kategorie „Sonstiges“.


\subsection{KG5b-Formular}\label{subsec:kg5b}

Das Formular KG5b ist, wie bereits erwähnt, ein offizielles Dokument der Bundesagentur für Arbeit, welches als Bescheinigung der Ausbildungsstätte dient.
Volljährige Kinder weisen damit gegenüber der Familienkasse den Status ihrer Ausbildung nach, was die Voraussetzung für den weiteren Kindergeldbezug ist.

Anhang~\ref{sec:kg5b-formular} zeigt ein exemplarisch ausgefülltes KG5b-Formular mit Markierung der für den Kindergeldantrag relevanten Felder.


\subsection{Ausbildungsvertrag}\label{subsec:vertraege}

Im Gegensatz zu den standardisierten KG5b-Formularen weisen Ausbildungsverträge eine höhere Varianz auf.
Dies ist auf die Vielzahl unterschiedlicher zuständiger Stellen (z.\,B. Industrie- und Handelskammern, Handwerkskammern, Ärztekammern) und Firmen zurückzuführen, die jeweils ein eigenes Layout definieren.
Die Vielfalt der Dokumentenstruktur reicht dabei von formularbasierten Layouts bis hin zu unstrukturierten Fließtexten.

In Anhang~\ref{sec:ausbildungsvertrag-der-ihk} ist ein synthetischer Ausbildungsvertrag der Industrie- und Handelskammer~\cite{Vertrag} abgebildet, in dem die relevanten Felder markiert sind.


\subsection{Sonstige Dokumente}\label{subsec:other_documents}

Die Kategorie \texttt{Sonstiges} steht als Auffangklasse für alle restlichen Dokumente bereit.
Darin befinden sich zum Beispiel Schulbescheinigungen, Anträge auf Eintragung bei der Handelskammer oder Studienbescheinigungen.
Da diese Dokumente keine Relevanz für die Weiterbeantragung des Kindergeldes bei volljährigen Auszubildenden haben, benötigt das System keine Informationen aus ihnen.


\section{Fachliche Grundlagen}\label{sec:domain}

\subsection{Der Ist-Zustand}\label{subsec:current}

Der schematische Ablauf der OCR/YOLO-Pipeline ist in Abbildung~\ref{fig:aube} dargestellt und lässt sich in zwei Stufen einteilen.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\textwidth]{figures/aube}
    \caption{Schematischer Workflow der aktuellen OCR/YOLO-Pipeline zur Dokumentenverarbeitung}
    \label{fig:aube}
\end{figure}

In der ersten Stufe nimmt die Anwendung die hochgeladenen Dokumente des Kunden entgegen.
Wenn ein Dokument als PDF vorliegt, extrahiert die Pipeline den Text mithilfe des Textlayers und eines \gls{OCR}-Modells, während bei einer Bilddatei lediglich das \gls{OCR}-Modell zum Einsatz kommt.
Die erste Stufe liefert ein PDF/A und den extrahierten Text als Ergebnis zurück.

Auf Basis des extrahierten Textes führt dIe Pipeline in der nächsten Stufe zunächst eine Klassifikation durch.
Hierbei unterscheidet der Klassifikator zwischen den bereits vorgestellten Dokumententypen: KG5b, Vertrag und Sonstiges.
Erkennt der Klassifikator das Dokument als \texttt{Sonstiges}, endet an dieser Stelle die Bearbeitung.
Handelt es sich hingegen um ein KG5b-Formular oder einen Vertrag, startet das System je nach Dokumententyp eine Erkennung mit einem \gls{YOLO}-Modell.
Innerhalb der Bounding-Boxes extrahiert die Pipeline mit einem \gls{OCR}-Modell den Text und ordnet ihn dem jeweiligen Label zu.

Schlussendlich stehen die erkannten Informationen zur weiteren Verarbeitung bereit.


\subsection{Rahmenbedingungen und Infrastruktur}\label{subsec:infrastructure}

Die Entwicklung und Evaluation der Modelle erfolgt unter datenschutzrechtlichen Auflagen.
Da die vorliegende Arbeit personenbezogene Echtdaten verarbeitet, kommt eine isolierte On-Premises-Infrastruktur zum Einsatz.

Die technische Architektur ist in Abbildung~\ref{fig:infrastructure} schematisch dargestellt.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/infrastructure}
    \caption{Schematische Darstellung der Trainings- und Inferenzinfrastruktur im Kubernetes-Cluster}
    \label{fig:infrastructure}
\end{figure}

Das System basiert auf einem abgeschotteten Kubernetes-Cluster.
Für die rechenintensiven Aufgaben, insbesondere das Fine-Tuning und die Inferenz der VLMs, stehen innerhalb des Clusters zwei NVIDIA A40 GPUs mit jeweils 48 GB \gls{VRAM}\cite{NVIDIAA40} zur Verfügung.
Um die Bereitstellung der VLMs zu erleichtern, nutzt das System Kubeflow.
Kubeflow ist eine Open-Source-Plattform, die speziell für das Entwickeln, Trainieren und Deployen von Machine-Learning-Modellen konzipiert wurde.

Der Zugang zum Cluster sowie zur Integrated Development Environment (\gls{IDE}) erfolgt über eine noVNC-Schnittstelle (browserbasierter Remote-Desktop).

Um einen Test- oder Trainingslauf durchzuführen, durchläuft das System den folgenden Prozess:

\begin{enumerate}
    \item \textbf{Initialisierung:} Ein Python-Skript in der \gls{IDE} startet den Kubeflow-Job.
    \item \textbf{Datenbereitstellung:} Die Pipeline lädt sowohl das VLM als auch die zu klassifizierenden Dokumentenbilder aus dem S3-Store in den GPU-Cluster.
    \item \textbf{Verarbeitung:} Die Inferenz oder das Training findet im Kubernetes-Cluster statt.
    \item \textbf{Persistierung:} Das System lädt die Ergebnisse zurück in den S3-Store.
\end{enumerate}


\section{Vision Language Models}\label{sec:vlms}

Ein Vision Language Model (VLM) besteht aus drei Komponenten: einem Image-Encoder, einem Adapter und einem Large Language Model (LLM).
Abbildung~\ref{fig:vlm} stellt den Aufbau dar.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/vlm}
    \caption{Architektur eines Vision Language Models mit Encoder, Adapter und LLM}
    \label{fig:vlm}
\end{figure}

Der Image-Encoder verarbeitet die Bildeingabe und extrahiert visuelle Merkmale (Features).
Zu diesem Zweck kommen häufig vortrainierte Modelle wie Vision Transformer oder CLIP zum Einsatz.
Diese Modelle zerlegen ein Bild in kleinere Bildausschnitte (Patches), die sie ähnlich wie Token in Sprachmodellen behandeln.
Der Encoder überführt jeden Patch in einen Vektor, der die Eigenschaften repräsentiert.
Das Ergebnis ist eine Sequenz von Bildvektoren, die die visuellen Informationen des gesamten Bildes beinhaltet.

Der Adapter verbindet den Image-Encoder mit dem Sprachmodell.
Diese Schicht transformiert die Ausgabe des Image-Encoders in ein Format, das mit den Textvektoren des Sprachmodells kompatibel ist.
In vielen Modellen besteht der Adapter aus einer linearen Schicht oder einem kleinen neuronalen Netz.
Der Adapter projiziert die Bildvektoren des Image-Encoders in denselben Vektorraum wie die Textvektoren des LLM.

Die Hauptkomponente bildet das LLM, welches für die eigentliche Verarbeitung und Generierung zuständig ist.
Das LLM erhält die Textvektoren zusammen mit den Bildvektoren und fusioniert diese.
Mithilfe des Attention-Mechanismus versteht das Modell Beziehungen zwischen visuellen und textuellen Elementen.
So ist es möglich, Fragen zu Bildinhalten zu beantworten oder Bildbeschreibungen zu generieren.

Die größte Herausforderung dieser modernen Technologie ist der hohe \gls{VRAM}-Verbrauch.
Neben der statischen \gls{VRAM}-Nutzung kommt mit jeder Anfrage ein dynamischer Verbrauch dazu.
Dabei setzt sich der statische Verbrauch aus den Gewichten der Modelle zusammen, während sich der dynamische Verbrauch aus den Key-Value-Caches und den Aktivierungen bildet.


\section{Vorstellung der Modelle}\label{sec:models}

Für die Klassifikation und die Extraktion der Informationen aus den Dokumenten evaluiert diese Arbeit drei verschiedene VLMs.

\subsection{Pixtral-12B}\label{subsec:pixtral}

Mistral AI veröffentlichte das Pixtral-12B-2409\cite{Pixtral12B} im Jahr 2024.
Es basiert auf einem 12 Milliarden Parameter großen Text-Decoder mit einem zusätzlichen 400 Millionen Parameter umfassenden Vision-Encoder.
Das Modell wurde speziell auf das Verständnis von Bildern und Dokumenten trainiert, weshalb es einen optimalen Kandidaten für die vorliegende Arbeit darstellt.
Mit einem theoretischen Kontextfenster von 128.000 Token ermöglicht das Modell die gleichzeitige Verarbeitung mehrerer Bilder\cite{PixtralPaper}.

\subsection{Qwen-2.5-VL}\label{subsec:qwen2.5vl}

Alibaba Cloud veröffentlichte das Qwen2.5-VL-7B-Instruct\cite{Qwen7B} sowie das Qwen2.5-VL-32B-Instruct\cite{Qwen32B} im Jahr 2025.
Die beiden Modelle basieren auf dem gleichen Image-Encoder mit 600 Millionen Parametern.
Lediglich die Größe des Text-Decoders ist mit 7 Milliarden beziehungsweise 32 Milliarden Parametern unterschiedlich.
Eine Besonderheit dieser Modelle ist, dass sie speziell auf das Verarbeiten von Dokumenten trainiert wurden\cite{QwenPaper}.

\subsection{Benchmarks}\label{subsec:benchmarks}

Im Folgenden sind die Benchmarks der einzelnen Modelle gelistet.
Besonders das Ergebnis des DocVQA-Benchmarks ist von Interesse, da der Benchmark ein Visual Question Answering (\gls{VQA}) auf Dokumentenbildern durchführt\cite{DocVQA}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Modell} & \textbf{Pixtral-12B} & \textbf{Qwen2.5-VL-7B} & \textbf{Qwen2.5-VL-32B} \\
        \hline
        \textbf{DocVQA} & 90,7 & 95,7 & 94,8 \\
        \textbf{MMMU} & 52,0 & 58,6 & 70,0 \\
        \hline
    \end{tabular}
    \caption{Benchmark-Ergebnisse der evaluierten Modelle\cite{QwenPaper, Qwen32B, PixtralPaper}}
    \label{tab:benchmark}
\end{table}


\section{Parameter-Efficient Fine-Tuning}\label{sec:peft}

Das Anpassen der Gewichte eines bereits trainierten Modells an eine spezifische Domäne setzt eine Infrastruktur mit hoher Rechenleistung voraus.
Mithilfe von Parameter-Efficient Fine-Tuning (PEFT) lässt sich die Anzahl der zu trainierenden oder neuen Parameter senken.
Infolgedessen verringert sich auch der Rechenaufwand für das Fine-Tuning der Modelle\cite{peft}.

Eine der bekanntesten Methoden im Bereich PEFT ist die Low-Rank Adaptation (LoRA).
Anstatt das gesamte Modell neu zu trainieren, friert LoRA die ursprünglichen Gewichte ein und fügt stattdessen trainierbare Matrizen mit einem niedrigen Rang $r$ in jede Schicht der Transformer-Architektur ein.
Um Rechenaufwand und Speicherplatz zu sparen, trainiert der Algorithmus ausschließlich diese kleineren Matrizen\cite{LoRA}.

Ergänzend zu LoRA betrachtet diese Arbeit die Variante Rank-Stabilized LoRA (rsLoRA).
Bei der Standard-LoRA führt ein steigender Rang $r$ oft nicht zu einer besseren Performance, da der verwendete Faktor ($\alpha/r$) das Lernen bei höheren Rängen verlangsamen oder hemmen kann.
rsLoRA stabilisiert den Prozess, indem das Verfahren die Adapter durch die Quadratwurzel des Rangs ($\alpha/\sqrt{r}$) teilt\cite{rsLoRA}.

\section{Verwandte Arbeiten}\label{sec:relatedwork}

Das automatisierte Lesen, Verstehen und Analysieren von Dokumenten entwickelt sich zu einem zunehmend relevanten Forschungsfeld, das die Literatur häufig unter dem Begriff Document AI zusammenfasst.
Cui et al.\ liefern eine Übersicht über die Entwicklung dieses Bereichs und definieren den Begriff Document AI als Prozess, der Webseiten, digitale und gescannte Dokumente in strukturierte Information überführt~\cite{DocumentAI}.
Die Entwicklung lässt sich dabei in drei wesentliche Phasen einteilen, die die folgenden Unterkapitel besprechen.

\subsection{Grenzen sequentieller OCR-Pipelines}\label{subsec:deeplearning_ocr}

Frühe Ansätze im Bereich der Document AI beruhten auf Pipelines, bei denen ein \gls{OCR}-Modell den Text extrahierte.
Ein Problem besteht darin, dass herkömmliche \gls{OCR}-Modelle den Text als zusammenhängende Zeichenkette verarbeiten, wodurch die zweidimensionalen Layout-Informationen, wie die räumliche Anordnung von Tabellen und Textblöcken, verloren gehen.

Um diese Informationen zu behalten, stellten Katti et al.\ mit Chargrid einen Ansatz vor, der Dokumente als zweidimensionales Raster von Zeichen (Character Grid) repräsentiert.
Durch den Einsatz von Convolutional Neural Networks auf dieser Struktur konnte das Modell sowohl textuelle als auch räumliche Merkmale nutzen, was zu einer Leistungssteigerung bei Aufgaben wie der Informationsextraktion (\gls{Information Extraction}) aus Rechnungen führte~\cite{chargrid}.
Chargrid baut auf den Bounding-Boxen und Koordinaten der \gls{OCR}-Modelle auf und benutzt diese, um das Layout des Dokuments zu modellieren.

Dieser Ansatz der Verarbeitung des Textes mit einem \gls{OCR}-Modell ähnelt dem Klassifikationsschritt der aktuellen OCR/YOLO-Pipeline, die das Dokument ebenfalls ohne räumliche Merkmale klassifiziert.
Die Einbeziehung räumlicher Merkmale ist für die Robustheit notwendig, was die Fehleranfälligkeit insbesondere bei komplexen Layouts erklärt.

\subsection{Layoutbewusste Transformer-Modelle}\label{subsec:tranformer_ocr}

Mit der Vorstellung der Transformer-Architektur begann eine tiefere Analyse der Semantik.
Xu et al.\ erkannten, dass bisherige Lösungen sich fast ausschließlich auf Text konzentrieren, während das Layout eines Dokuments eine entscheidende Rolle für das Verständnis spielt.
LayoutLM verbindet ein vortrainiertes BERT-Modell mit Positionsvektoren und Bildvektoren.
Die Positionsvektoren speichern die räumliche Anordnung der Elemente, während die Bildvektoren visuelle Merkmale des Dokuments kodieren.
LayoutLM konnte auf diversen Benchmarks neue Bestwerte erzielen~\cite{LayoutLM}.

Einen weiteren Schritt markierte das TrOCR von Li et al., bei dem Transformer sowohl die CNN-basierten Komponenten für das Bildverständnis als auch die Recurrent Neural Networks (RNNs) für die Textgenerierung herkömmlicher \gls{OCR}-Modelle ersetzten.
Dies führte ebenfalls zu neuen Bestwerten in der Texterkennung~\cite{TrOCR}.

Trotz dieser Fortschritte bleibt bei LayoutLM und ähnlichen Modellen die Trennung zwischen Textextraktion durch ein \gls{OCR}-Modell und anschließender semantischer Verarbeitung bestehen.
Mit dieser Trennung stützt sich die Verarbeitung weiterhin auf mehrere Modelle, was die Abhängigkeit und Fehleranfälligkeit erhöht.
Jeder \gls{OCR}-Fehler beeinträchtigt die anschließende semantische Analyse.
Diese Abhängigkeit unterschiedlicher Modelle ist auch eine Schwachstelle der OCR/YOLO-Pipeline, weshalb ein End-To-End-Ansatz Abhilfe schaffen könnte.

\subsection{End-To-End-Architekturen}\label{subsec:end_to_end_transformer}

Um die Abhängigkeit und Fehleranfälligkeit sequentieller Pipelines zu reduzieren, entwickelte sich die Forschung hin zu End-To-End-Architekturen.
Kim et al.\ stellten mit Donut ein \gls{OCR}-freies Transformer-Modell vor, das durch einen Vision-Encoder und einen Textual-Decoder Bilder von Dokumenten direkt in eine strukturierte \gls{JSON}-Ausgabe überführt.
Durch den Wegfall des \gls{OCR}-Modells erreicht Donut eine hohe Genauigkeit bei gleichzeitig geringerer \gls{Latenz}~\cite{Donut}.
Die Robustheit eines solchen \gls{OCR}-freien Systems zeigten Blecher et al.\ anhand akademischer Dokumente mit Nougat.
Durch die Umwandlung von Dokumenten in Markdown ohne die Verwendung eines \gls{OCR}-Modells extrahiert Nougat mathematische Formeln ohne Verlust.
Traditionelle \gls{OCR}-Systeme scheiterten häufig an dieser Aufgabe, da besonders hier räumliche Zusammenhänge relevant sind~\cite{Nougat}.

Parallel dazu trieb die Forschung die Integration von Dokumentenverarbeitung in LLMs voran.
Wang et al.\ präsentierten mit DocLLM eine Erweiterung für LLMs, die auf teure Image-Encoder verzichtet.
DocLLM kombiniert die aus einem \gls{OCR}-Modell gewonnenen Bounding-Box-Koordinaten über einen Attention-Mechanismus mit dem Modell, wodurch es die Struktur des Dokuments versteht~\cite{DocLLM}.

Die Entwicklung von Donut und Nougat zeigt das Potenzial \gls{OCR}-freier Systeme.
DocLLM demonstriert, wie sich das Wissen von LLMs für Aufgaben im Bereich der Document AI nutzbar machen lässt.
Die aktuellen VLMs wie das Pixtral-12B und die Qwen-2.5-VL-Familie führen diese beiden Systeme zusammen.
Sie vereinen die Image-Encoder mit dem Wissen des LLMs und können visuelle und textuelle Merkmale verbinden.
Ähnlich wie die Evolution der Document AI beschäftigt sich diese Arbeit damit, ob ein solches generalistisches VLM die auf \gls{OCR}-Modellen beruhende Pipeline ersetzen kann.
