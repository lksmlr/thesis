\chapter{Diskussion}\label{ch:discussion}


\section{Interpretation der Fehler}\label{sec:error_patterns}

Sowohl die OCR/YOLO-Pipeline als auch die VLMs weisen spezifische Fehlermuster auf, die sich in Klassifikations-, Format-, Detektions- und Wertfehler unterteilen lassen.

Klassifikationsfehler wiegen in der aus mehreren Einzelkomponenten bestehenden OCR/YOLO-Pipeline besonders schwer, da die Klassifikation hier einen logischen Single Point of Failure darstellt.
Wie die Confusion Matrix in Abbildung~\ref{fig:confusion_matrices_overview} verdeutlicht, wurden fünf Dokumente fälschlicherweise als \texttt{Sonstiges} eingestuft, was aufgrund der Systemarchitektur (siehe Abbildung~\ref{fig:aube}) zu einem sofortigen Abbruch der Verarbeitung führt.
Bei diesen fünf Dokumenten handelte es sich auffälligerweise ausschließlich um fotografierte Dokumente.
Fotos weisen häufig Verzerrungen, Schattenwurf oder eine schlechte Beleuchtung auf, wodurch die \gls{OCR}-Ergebnisse des vorgeschalteten Systems schlechter ausfallen und die Klassifikation behindern.

Diese Klassifikationsfehler mindern die Leistung der Information Extraction, da mit einem einzigen Fehler bereits zehn (Vertrag) oder zwölf (KG5b) Felder fehlen.
Auch die Verwechslung von Verträgen mit KG5b-Formularen wirkt sich negativ auf die OCR/YOLO-Pipeline aus, da für beide Dokumentenarten unterschiedliche, spezialisierte Modelle trainiert wurden.
Ein entscheidender Vorteil der generalistischen VLMs ist hier die Robustheit durch das einheitliche Modell.
Da die Felder eines Vertrags eine Teilmenge des KG5b-Schemas darstellen, bleibt die Information Extraction teilweise erfolgreich, selbst wenn das Dokument falsch klassifiziert wurde.
Ein Fehler in der Klassifikation propagiert somit nicht so stark wie bei der OCR/YOLO-Pipeline.
Im Idealfall fehlt bei der Erkennung eines Vertrages statt eines KG5b-Formulars kein Feld und im umgekehrten Fall fehlen maximal zwei Felder.
Die Fehlklassifizierung als \texttt{Sonstiges} ist jedoch auch bei den VLMs kritisch, da das Modell in diesem Fall keine Extraktion durchführt.
Wie in den Confusion Matrices in Abbildung~\ref{fig:confusion_matrices_32b_peft} ersichtlich, erweisen sich jedoch insbesondere das Qwen-2.5-VL-7B-finetuned und das Qwen-2.5-VL-32B als äußerst zuverlässig in der Klassifizierung.

Die Detektionsfehler beziehen sich primär auf Checkboxen, Stempel und Unterschriften.
Checkboxen sind im KG5b-Formular für das Feld \texttt{apprenticeship\_finished} relevant (siehe Abbildung~\ref{fig:kg5bpage1}).
Hier müssen die Modelle zwischen leeren, angekreuzten und durchgestrichenen Boxen unterscheiden.
Detektionsfehler sind häufig auf den unvermeidbaren Medienbruch zurückzuführen, der entsteht, wenn Dokumente zur Unterschrift ausgedruckt und anschließend wieder digitalisiert werden.
Durch diesen Prozess verlieren Stempel und Unterschriften an Kontrast oder werden in Schwarz-Weiß-Scans schwer erkennbar.

Zusätzlich erschwert die räumliche Nähe der Felder \texttt{date\_document} und \texttt{signature\_company} die Extraktion.
Da handschriftliche Unterschriften regelmäßig den vorgesehenen Platz überschreiten, wird das Datumsfeld häufig überdeckt.
Während die OCR/YOLO-Pipeline hier oft keine valide Bounding-Box mehr liefern kann, zeigen das Qwen-2.5-VL-32B und das Qwen-2.5-VL-7B-finetuned eine hohe Robustheit gegenüber diesen Überlagerungen.
Die trotz der Überlappungen gute Leistung ist vermutlich auf die Augmentation der Daten im ursprünglichen Training zurückzuführen.

Formatfehler verdeutlichen Unterschiede in den \textit{Instruction-Following}-Fähigkeiten der Modelle.
Während die Qwen-Modelle nahezu fehlerfrei valide \gls{JSON}-Strukturen lieferten, scheiterte das Pixtral-12B trotz Retry-Logik an zwei Verträgen.
Das Modell generierte in keinem der Versuche die umschließenden Klammern (\{\}), wodurch das Parsen fehlschlug und 20 \gls{JSON}-Felder verloren gingen.
Dies deutet darauf hin, dass das Pixtral-12B eine geringere Stabilität bei der Einhaltung strikter Ausgabeformate aufweist als die Qwen-Modelle.

Bei den Wertfehlern muss zwischen \gls{OCR}-Fehlern und Mapping-Fehlern (falsche semantische Zuordnung) unterschieden werden.
Die OCR/YOLO-Pipeline zeigte Schwächen in der Erkennung von Namen und Datumsangaben.
Selbst die Levenshtein-Distanz mit einem Schwellenwert von 0,8 reichte in einigen Fällen nicht aus, um diese Fehler zu kompensieren.
Beim Pixtral-12B und Qwen-2.5-VL-7B traten neben \gls{OCR}-Fehlern vermehrt Mapping-Fehler auf, bei denen beispielsweise das Geburtsdatum des Kindes mit dem Startdatum der Ausbildung vertauscht wurde.
Durch das Fine-Tuning des Qwen-2.5-VL-7B konnte eine spürbare Reduzierung sowohl der \gls{OCR}- als auch der Mapping-Fehler erzielt werden.

Halluzinationen von Feldern traten primär bei den Basismodellen auf, insbesondere bei der Klasse \texttt{Sonstiges}, da hier die Modelle Felder der beiden anderen Schemata extrahierten.
Da diese zusätzlichen Felder durch die Schemata gefiltert werden können, stellen sie, anders als inhaltliche Halluzinationen, keine Einschränkung dar.


\section{Abwägung Effektivität und Ressourceneffizienz}\label{sec:effectiveness_resource_efficiency}

Die Ergebnisse der Untersuchung offenbaren einen Zielkonflikt zwischen der Leistungsfähigkeit der Modelle und ihrem Ressourcenbedarf.
Während die OCR/YOLO-Pipeline durch ihre extrem geringe \gls{Latenz} und Ressourceneffizienz auffällt, zeigen die VLMs eine Überlegenheit in der Qualität der Information Extraction.

Die OCR/YOLO-Pipeline bildet mit einem durchschnittlichen Energieverbrauch von 117 Joule und einer \gls{Latenz} von unter einer Sekunde die Basislinie für die Effizienz.
Demgegenüber steht das leistungsstärkste Modell, das Qwen-2.5-VL-32B, welches im Mittel die 160-fache Energie pro Dokument benötigt und eine \gls{Latenz} von durchschnittlich 64,16 Sekunden aufweist.
Selbst das kleinste Modell, das Qwen-2.5-VL-7B, verbraucht rund das 22-fache an Energie.
Kritisch betrachtet relativiert sich der reine Effizienzvorteil der Pipeline jedoch im operativen Kontext.
Ein niedriger F1-Score (0,51 bei Verträgen) zieht eine manuelle Nachbearbeitung durch Sachbearbeiter nach sich.
Die durch die schnelle technische Verarbeitung gewonnene Zeit wird somit im Gesamtprozess durch menschliche Korrektureingriffe wieder aufgezehrt.

Betrachtet man die Wirtschaftlichkeit, so übersteigen die Kosten für eine einminütige manuelle Korrektur die Energiekosten einer einminütigen \gls{GPU}-Inferenz um ein Vielfaches.
Die höhere Rechenzeit der VLMs gleicht sich somit durch die Einsparung von Arbeitszeit in der Sachbearbeitung aus.
Bei einem F1-Score bei Verträgen von 0,94 (Qwen-2.5-VL-32B) muss im Schnitt ein Feld korrigiert werden, während bei einem F1-Score von 0,51 (OCR/YOLO-Pipeline) vier bis fünf Felder korrigiert werden müssen.

Neben dem Energiebedarf ist die Infrastruktur, spezifisch die \gls{VRAM}-Auslastung, ein Faktor.
Während die OCR/YOLO-Pipeline auf einfachen \glspl{GPU} lauffähig ist, setzen die großen VLMs leistungsstarke \glspl{GPU} voraus.
Das Qwen-2.5-VL-32B übertrifft das Qwen-2.5-VL-7B-finetuned zwar in der Information Extraction bei Verträgen (F1-Score 0,94 gegenüber 0,84), erkauft sich diesen Vorteil jedoch mit einem wesentlich höheren \gls{VRAM}-Bedarf.

Insgesamt bringt das Qwen-2.5-VL-7B-finetuned den besten Kompromiss zwischen Effizienz und Effektivität.
Das trainierte Modell schlägt dabei in der Klassifikation das Qwen-2.5-VL-32B, nähert sich bei der Information Extraction der Leistung des großen Modells an und ist dabei effizienter.
Mit einem statischen \gls{VRAM}-Bedarf von circa 17 GB ermöglicht dieses Modell den Betrieb auf kleineren \glspl{GPU}, was die Hürden für eine Integration in die Infrastruktur senkt.
Hinsichtlich der Skalierbarkeit stellt die \gls{Latenz} der VLMs jedoch weiterhin eine Herausforderung dar, da eine niedrige \gls{Latenz} ein Cluster aus \glspl{GPU} voraussetzt.


\section{Komplexität und Wartbarkeit der Systemarchitekturen}\label{sec:maintainability}

Neben der Modellleistung stellt die Komplexität der Infrastruktur einen zentralen Faktor für den produktiven Einsatz dar.
Zur Einordnung wird das Qualitätsmodell der ISO/IEC~25010 herangezogen, wobei der Fokus auf dem Qualitätsmerkmal der Wartbarkeit mit den Teilaspekten Modularität, Analysierbarkeit, Änderbarkeit und Testbarkeit liegt.

Die Architektur der OCR/YOLO-Pipeline zeichnet sich durch eine Verkettung spezialisierter Komponenten aus (siehe Abbildung~\ref{fig:aube}).
Obwohl dieser Aufbau grundsätzlich modular ist, entsteht durch die sequenzielle Verarbeitung eine kritische Abhängigkeit.
Der Klassifikator nimmt eine zentrale Rolle ein, da seine Entscheidung den weiteren Verlauf bestimmt.
Fehlklassifikationen propagieren durch die nachgelagerten Komponenten, was die Fehleranalyse erschwert.
Wie bereits in der Fehleranalyse~\ref{sec:error_patterns} identifiziert, stellt der Klassifikator hier architektonisch einen \textit{Single Point of Failure} dar.

Hinsichtlich der Änderbarkeit offenbart die OCR/YOLO-Pipeline einen erhöhten Aufwand.
Die Einführung einer neuen Dokumentenart erfordert nicht nur Anpassungen am Klassifikator, sondern auch das Training eines neuen \gls{YOLO}-Modells.
Dies ist mit hohem manuellem Aufwand verbunden, da das Annotieren von Bounding-Boxes zeitintensiv ist.
Des Weiteren gibt es durch die vielen Komponenten zahlreiche Schnittstellen, die bei einer Änderung der Dokumentenstrukturen gegebenenfalls geändert werden müssen.
Im Gegensatz dazu reduziert der generalistische Ansatz mit den VLMs die Anzahl der Komponenten und Schnittstellen.
Änderungen an den Dokumentenarten beschränken sich auf die Änderungen des Prompts oder im Fall des Qwen-2.5-VL-7B-finetuned auf das Training eines Modells.
Hierbei muss auch ein Modell trainiert werden und gegebenenfalls die Schnittstelle angepasst werden, aber die Annotation der Daten ist aufgrund von Model-Assisted-Labeling wesentlich einfacher.

Der monolithische Ansatz bringt jedoch Nachteile für die Testbarkeit mit sich.
Ein VLM agiert als Black Box, wodurch die interne Logik schwer analysierbar ist.
Während in der OCR/YOLO-Pipeline die Komponenten logisch entkoppelt sind, weshalb eine Änderung am YOLO-Modell das OCR-Modell unberührt lässt, wirken sich Änderungen bei einem VLM auf alle Dokumentenarten aus.
Eine Änderung des Prompts oder das Fine-Tuning für die Verbesserung einer Dokumentenart kann Regressionen auf anderen Dokumentenarten haben.
Dies erhöht die Komplexität von Regressionstests.
Mit der Einführung von automatisierten Tests mit dem Validierungsdatensatz nach Änderungen kann dieses Risiko minimiert werden.


\section{Zusammenfassende Beantwortung der Forschungsfragen}\label{sec:answer_ffs}

\subsection{Beantwortung der Forschungsfrage 1}\label{subsec:answer_ff1}

\begin{enumerate}
    \item [1.] Wie verhält sich ein nicht domänenspezifisch angepasstes VLM hinsichtlich \gls{Latenz}, Klassifikations- und Extraktionsleistung im Vergleich zur OCR/YOLO-Pipeline, und welche Vorteile bietet ein einzelnes generalistisches Modell gegenüber spezialisierten Einzelsystemen?
\end{enumerate}

Hinsichtlich der \gls{Latenz} zeigt ein nicht domänenspezifisch angepasstes VLM Nachteile gegenüber der OCR/YOLO-Pipeline.
Selbst das effizienteste Basismodell, das Qwen-2.5-VL-7B, benötigt mit durchschnittlich 7,43 Sekunden fast 13-mal so viel Zeit wie die bisherige Lösung.

Während die Klassifikationsleistung der Basismodelle mit der Leistung der Pipeline übereinstimmt, zeigt sich ein klarer Unterschied in der Extraktionsgüte.
Bei Verträgen erreicht die OCR/YOLO-Pipeline im Gegensatz zu den Qwen-Modellen lediglich einen F1-Score von 0,51.
Mit einem F1-Score von 0,83 erreicht selbst das kleinste Basismodell, das Qwen-2.5-VL-7B, eine Leistungssteigerung von 0,32 Punkten.
Auch bei den starren KG5b-Formularen übertrifft Qwen-2.5-VL-7B die Pipeline knapp.
Einzig das Pixtral-12B erzielte gegenüber der Pipeline sowohl in der Klassifikation als auch in der Information Extraction schlechtere Ergebnisse.

Neben der verbesserten Leistung bietet ein generalistischer Ansatz wesentliche Vorteile in der Architektur und Wartbarkeit.
Durch den Single Point of Failure des Klassifikators ist die OCR/YOLO-Pipeline weniger robust gegenüber Klassifikationsfehlern.
Eine Fehlklassifikation führt zur Verwendung falscher Modelle oder zum sofortigen Abbruch.
Ein VLM hingegen ist weniger anfällig bei einem Teil von Fehlklassifikationen, da aufgrund überlappender Schemata dennoch relevante Informationen extrahiert werden.

Der monolithische Ansatz eines VLM reduziert des Weiteren die Anzahl der Komponenten und Schnittstellen drastisch.
Während die Pipeline für jede neue Dokumentenart neue Modelle und Schnittstellen braucht, reduziert sich der Aufwand bei einem VLM.
Anstatt zeitaufwendig Bounding-Boxes für das Training von \gls{YOLO}-Modellen zu annotieren, genügt die Anpassung des Prompts oder ein Fine-Tuning mittels Model-Assisted-Labeling, was den manuellen Aufwand senkt.


\subsection{Beantwortung der Forschungsfrage 2}\label{subsec:answer_ff2}

\begin{enumerate}
    \item [2.] Welche Auswirkungen hat ein domänenspezifisches Training eines kleineren Modells auf dessen \gls{Latenz}, Klassifikations- und Extraktionsleistung im Vergleich zu einem leistungsstärkeren Basismodell?
\end{enumerate}

Der Vergleich zeigt, dass ein Training eines kleineren Modells einen merklichen Effizienzvorteil gegenüber einem größeren Basismodell bringt und sogar in der Klassifikation überlegen ist, wobei man jedoch leichte Abstriche bei der Information Extraction hinnehmen muss.

Hinsichtlich der \gls{Latenz} schlägt das angepasste Modell aufgrund der Größe das 32B-Modell.
Während das Qwen-2.5-VL-7B-finetuned durchschnittlich innerhalb 9,57 Sekunden antwortet, benötigt das Qwen-2.5-VL-32B mit 64,16 Sekunden länger.
Insbesondere die maximale \gls{Latenz} des großen Modells ist mit 252,06 Sekunden bedenklich.

Die Klassifikationsleistung ist bei beiden Modellen beeindruckend, jedoch schneidet hier das angepasste Modell besser ab.
Mit einem F1-Score von 0,95 bei sonstigen Dokumenten erzielt das Qwen-2.5-VL-7B-finetuned einen minimal höheren Wert als das Qwen-2.5-VL-32B mit einem F1-Score von 0,91.
Dies verdeutlicht, dass das kleinere Modell durch das Training besser zwischen den Dokumentenarten unterscheiden kann als das Qwen-2.5-VL-32B mit seinem Wissen.

Bei der Information Extraction hängt die Leistung von der Varianz der Dokumentenarten ab.
Bei den starren KG5b-Formularen erzielten beide Modelle einen F1-Score von 0,87.
Die durch die hohe Varianz gekennzeichneten Ausbildungsverträge werden jedoch durch das Qwen-2.5-VL-32B mit einem F1-Score von 0,94 besser erkannt.
Das Weltwissen des großen Modells hilft ihm dabei, viele verschiedene Varianten von Dokumenten besser zu verstehen.

Trotz der minimal besseren Information Extraction des großen Modells bietet das Qwen-2.5-VL-7B-finetuned den besten Kompromiss aus Effizienz und Effektivität.


\subsection{Beantwortung der Forschungsfrage 3}\label{subsec:answer_ff3}

\begin{enumerate}
    \item [3.] Wie unterscheidet sich der Ressourcenverbrauch (Energieverbrauch und \gls{VRAM}"=Auslastung) der VLMs von dem der OCR/YOLO-Pipeline?
\end{enumerate}

Der Vergleich des Ressourcenverbrauchs zeigt die gravierendsten Unterschiede zwischen der bisherigen Pipeline und den VLMs.

Beim Energieverbrauch zeigt die Pipeline mit 117 Joule im Durchschnitt pro Dokument einen deutlichen Unterschied zu dem Verbrauch der VLMs.
Selbst das effizienteste Modell, das Qwen-2.5-VL-7B, steht mit einem Verbrauch von durchschnittlich 2587 Joule pro Dokument in starkem Kontrast.
Das leistungsstarke Qwen-2.5-VL-32B verbraucht mit im Mittel 18.786 Joule sogar das 160-fache der Energie der Pipeline.

Ein kritischer Aspekt für den operativen Betrieb ist neben dem statischen der dynamische \gls{VRAM}-Verbrauch.
Während die OCR/YOLO-Pipeline mit 0,49 GB kaum ins Gewicht fällt und durch die geringe \gls{Latenz} der Speicher schnell wieder freigegeben wird, verbrauchen die VLMs beträchtlich mehr.
Interessanterweise liegt der durchschnittliche dynamische Verbrauch des Qwen-2.5-VL-7B-finetuned mit 3,03 GB leicht über dem des größeren Qwen-2.5-VL-32B (3,01 GB).
Trotz des nahezu gleichen dynamischen \gls{VRAM}-Verbrauchs belegt das Qwen-2.5-VL-7B-finetuned mit 23,89 GB weniger statischen \gls{VRAM}, wodurch mehr Speicher für die dynamische Allokation freisteht.

Das Problem des dynamischen Verbrauchs liegt in der Variabilität.
Mit jeder Anfrage wächst der Verbrauch, weshalb die \glspl{GPU} über genügend Puffer verfügen muss, um Out-of-Memory-Fehler zu vermeiden.
Besonders bei der Verarbeitung von mehreren Bildern wird der Kontext pro Anfrage groß.
Die Skalierung fällt hier wesentlich schwerer als die der deterministischen OCR/YOLO-Pipeline, da mit gewünschter \gls{Latenz} und gewünschten gleichzeitigen Anfragen die Entscheidung der benötigten Hardware gefällt werden muss.


\section{Limitation und Probleme der Evaluation}\label{sec:limitations}

Im Verlauf der Implementierung und Evaluation traten technische Limitationen und Herausforderungen auf, die die Ergebnisse beeinflussen und Potenziale für zukünftige Optimierungen aufzeigen.
Diese lassen sich in framework-spezifische Probleme, Aspekte der Ressourcenverwaltung sowie methodische Grenzen der Klassifikation und Datenverarbeitung unterteilen.

Eine technische Hürde stellte die Inferenz des Pixtral-12B dar.
Das Modell wurde über den in Listing~\ref{lst:pixtral} dargestellten Code geladen.
Hierbei zeigte sich ein Memory Leak, das trotz Bereinigung des Speichers und des Caches dazu führte, dass der \gls{VRAM} beider Grafikkarten nach vier bis fünf Inferenzen vollständig belegt war.
Dieses Verhalten trat nur beim Pixtral-12B auf und zwang zu einer manuellen Verarbeitung der Datensätze in Batches.

\begin{listing}[H]
    \caption{Laden des Pixtral-Modells}
    \label{lst:pixtral}
    \inputminted[]{python}{listings/pixtral.py}
\end{listing}

Ein weiteres Problem bei der Implementierung trat bei den Qwen-Modellen auf.
Anfänglich wurden die Dokumente als Base64-Strings an das Modell übergeben.
Die in Listing~\ref{lst:qwen} verwendete Funktion \texttt{apply\_chat\_template} wies jedoch einen internen Fehler bei der Verarbeitung mehrerer Base64-Bilder auf, sodass lediglich das erste Bild vom Modell gesehen wurde.
Dieses Problem konnte durch die Umstellung auf PIL-Images gelöst werden.

\begin{listing}[H]
    \caption{Laden der Qwen-Modelle}
    \label{lst:qwen}
    \inputminted[]{python}{listings/qwen.py}
\end{listing}

Hinsichtlich der Ressourceneffizienz zeigten die Ergebnisse in Abbildung~\ref{fig:results_vram_inference} und Abbildung~\ref{fig:results_vram_model}, dass das Qwen-2.5-VL-7B-finetuned im Vergleich zum Basismodell einen erhöhten statischen und im Vergleich zum 32B-Modell sogar einen höheren dynamischen \gls{VRAM}-Verbrauch aufweist.
Dieser Overhead resultiert vermutlich aus der Art der Bereitstellung.
Die Gewichte der während des Fine-Tunings trainierten LoRA-Adapter werden nicht fest mit dem Basismodell gemerged, sondern zur Laufzeit dynamisch geladen, um Speicherplatz zu sparen.
In einem produktiven Einsatz wird der finale Adapter mit dem Basismodell gemerged und zusammen gespeichert, was den Overhead reduzieren wird.

Methodische Limitationen zeigten sich in der Robustheit der Klassifikation durch Fehlklassifikationen.
Wie in der Fehleranalyse diskutiert, führt eine Fehlklassifikation eines Vertrags als \texttt{Sonstiges} zum Verlust aller Felder, da für die Klasse \texttt{Sonstiges} keine Extraktion stattfindet.
Das Extrahieren von Vertragsdaten auch in sonstigen Dokumenten würde den Fehler der Klassifizierung als sonstiges Dokument reduzieren.
Jedoch wäre dieses Vorgehen mit einem erhöhten Labeling-Aufwand verbunden.

Zur Steigerung der Zuverlässigkeit der Ausgabeformate bietet sich der Einsatz von Constrained Decoding an.
Durch die Bereitstellung des Modells über Frameworks wie \gls{vLLM} kann die Ausgabe strikt auf ein \gls{JSON}-Schema beschränkt werden.
Damit könnte man nicht nur die Generierung von invaliden \gls{JSON}-Schemata verhindern, sondern könnte auch das Format für Datumsangaben und binäre Felder festsetzen.
Zusätzlich ermöglicht dieser Ansatz die Auswertung der \gls{logprobs} der generierten Token.
Durch die Analyse der Wahrscheinlichkeitsverteilung des Tokens, das auf \texttt{"type": "} folgt, ließe sich ein Confidence für die Klassifikation bestimmen.
Dokumente, bei denen das Modell eine niedrige Confidence aufweist, könnten so markiert werden für den Sachbearbeiter.

Außerdem wird in der Evaluation nicht betrachtet, dass eventuell mehrere Dokumentenarten in einem PDF zusammengefasst werden.
Um gemischte Dokumentenarten zu erkennen, müsste das Modell in der Lage sein ein Array von \gls{JSON}-Objekten zu liefern.
Hierfür müsste eine Anpassung der Trainingsdaten erfolgen mittels Augmentation, da für diese vermischten Dokumente wenig Trainingsdaten zur Verfügung stehen.

Eine weitere Limitation stellt die maximale Kontextlänge der Modelle dar.
Dokumente, die besonders umfangreich sind, neigen dazu das Kontextfenster des Modells zu überladen.
Dies führt zu einem Abbruch der Inferenz oder einer einem Abschneiden relevanter Inhalte.
Für den Fall dieser vielseitigen Dokumente muss für den produktiven Einsatz eine Strategie wie eine Vorfilterung entwickelt werden.


\section{Handlungsempfehlung für die Bundesagentur für Arbeit}\label{sec:recommendation_for_action}

Aus der Beantwortung der Forschungsfragen und der vorangegangenen Diskussion leitet sich eine Handlungsempfehlung für die Bundesagentur für Arbeit ab.
Die Entscheidung beruht auf einer Abwägung zwischen Fußabdruck, Leistungsfähigkeit und der wirtschaftlichen Notwendigkeit.

Als primäre Empfehlung wird der Einsatz des \textbf{Qwen-2.5-VL-7B-finetuned} ausgesprochen.
Dieses Modell stellt im Vergleich zur bestehenden OCR/YOLO-Pipeline sowie zum größeren Basismodell Qwen-2.5-VL-32B den besten Kompromiss dar.
Zwar weist die OCR/YOLO-Pipeline den geringsten ökologischen Fußabdruck auf, jedoch verursacht die geringe Extraktionsgüte bei Verträgen hohe Personalkosten.
Die Kosten der manuellen Korrektur durch Sachbearbeiter übersteigt die Kosten des Qwen-2.5-VL-7B-finetuned.
Der Einsatz des Qwen-2.5-VL-7B-finetuned erhöht den ökologischen Fußabdruck, aber bringt eine Reduzierung der Bearbeitungszeit pro Antrag mit sich.

Für die Skalierung über mehrere Projekte hinweg wird davon abgeraten ein einzelnes 32B-Modell bereitzustellen.
Stattdessen könnten die verschiedenen Fachverfahren LoRA-Adapter entwickeln die, wie in der Evaluation, zur Laufzeit geladen werden.
Durch die geringere Parameteranzahl könnten drei bis vier Instanzen statt eines Qwen-2.5-VL-32B geladen werden.
Die dynamische Anpassung der Adapter verbraucht minimal mehr Speicher und Energie, ist jedoch sparsamer als ein großes Modell.

Zusammenfassend ermöglicht der Wechsel auf das Qwen-2.5-VL-7B-finetuned der Bundesagentur für Arbeit, den Automatisierungsgrad bei Kindergeldanträgen zu erhöhen und Fachpersonal zu entlasten, ohne dabei unverhältnismäßige Ressourcen zu binden.
