\chapter{Methodik und Implementierung}\label{ch:data}

Im Rahmen dieses Proof of Concepts werden verschiedene \glspl{VLM} vergleichend gegenüber der bestehenden \gls{YOLO}/\gls{OCR}-Pipeline evaluiert.
Für die Entscheidung, ob eines der \glspl{VLM} die YOLO/OCR-Pipeline ersetzt, sollen die folgenden Forschungsfragen beantwortet werden:

\begin{enumerate}
    \item Inwiefern ist ein nicht domänenspezifisch angepasstes \gls{VLM} hinsichtlich der Klassifikations- und Extraktionsgenauigkeit der trainierten \gls{YOLO}/\gls{OCR}-Pipeline überlegen, und welche Vorteile bietet ein einzelnes generalistisches Modell gegenüber der Verwendung spezialisierter Einzelsysteme?
    \item Wie verhält sich ein kleineres, domänenspezifisch trainiertes Modell gegenüber einem leistungsstärkeren Basismodell in Bezug auf Performanz und Effizienz?
    \item In welchem Verhältnis steht der Ressourcenverbrauch der \glspl{VLM} zu dem der \gls{YOLO}/\gls{OCR}-Pipeline?
\end{enumerate}

In Abbildung~\ref{fig:overview} ist ein Überblick der gesamten Methodik abgebildet.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/overview}
    \caption{Überblick der Methodik.}
    \label{fig:overview}
\end{figure}

\section{Modellierung des Dokumentenbestands als Grundlage der Evaluation}\label{sec:data_preprocessing}


\subsection{Preprocessing und Darstellung der Dokumentenarten als JSON-Objekte}\label{sec:documents_as_json}

Durch Datenlieferungen der Familienkasse stehen mehrere Tausend Dokumente als PDF oder Bilddatei zur Verfügung.
Um eine einheitliche Basis zu schaffen und die Klassifikations- und Extraktionsgüte zu verbessern, müssen die Daten vorverarbeitet werden.
Im Fokus stehen dabei die Korrektur der Bildausrichtung sowie die Konvertierung der PDF-Dokumente in Bilder.
Zur Korrektur der Rotation werden die Exchangeable Image File Format (\gls{EXIF})-Metadaten ausgelesen, um die ursprüngliche Ausrichtung wiederherzustellen.
Während ältere \glspl{VLM} Bilder oft auf fixe Größe skalieren, unterstützen die in dieser Arbeit evaluierten Modelle eine dynamische Skalierung.
Um jedoch die maximale Token-Anzahl, werden die Bilder auf eine maximale Pixelanzahl skaliert, wobei das ursprüngliche Seitenverhältnis beibehalten wird.


\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               gobble=2,
               frame=lines,
               framesep=2mm]{python}
    from pathlib import Path
    from PIL import Image, ImageOps
    from pdf2image import convert_from_path


    def preproccesing(path_to_document: str,
                      max_pixels: int
                      ) -> [Image]:
        ...
        for _, page in enumerate(pages):
            # corrects the image orientation
            page = ImageOps.exif_transpose(page)

            if page.width * page.height > max_pixels:
                scale_factor = (max_pixels / (page.width * page.height)) ** 0.5

                new_width = int(page.width * scale_factor)
                new_height = int(page.height * scale_factor)

                page = page.resize((new_width, new_height),
                                   Image.Resampling.LANCZOS)

        ...
\end{minted}


Für die standardisierte Weiterverarbeitung der Modellausgaben ist eine feste Struktur essenziell.
Da moderne \glspl{VLM} darauf trainiert sind, Antworten im JavaScript Object Notation ()\gls{JSON})-Format zu liefern, werden die extrahierten Informationen in ein vordefiniertes \gls{JSON}-Schema überführt.
Ein wesentlicher Vorteil gegenüber der herkömmlichen Pipeline ist die gleichzeitige Durchführung der Klassifikation sowie der \gls{IE} in einem einzigen Inferenzschritt.
Das Modell klassifiziert die Dokumentenart, die im Feld \texttt{type} abgebildet wird.
Während das Feld \texttt{type} in allen \gls{JSON}-Schemata konsistent vorhanden ist, variieren die Felder der \gls{IE} je nach Dokumentenart.

Bei den binären, booleschen Feldern markiert ein \texttt{true} das Vorhandensein eines Merkmals, während ein \texttt{false} dessen Fehlen oder das Nicht-Erkennen repräsentiert.
Um eine strukturelle Konsistenz zu erzwingen, werden Felder, bei denen die dazugehörige Information im Dokument fehlt, bei booleschen Typen mit \texttt{false} und bei Textfeldern mit einer leeren Zeichenkette belegt.

Da die Dokumente in den meisten Fällen aus mehreren Seiten bestehen, repräsentiert ein \gls{JSON}-Objekt das mehrseitige Dokument.
Sollten Dokumente verschiedener Arten vermischt vorliegen, generiert das Modell für jede erkannte Dokumentenart ein separates \gls{JSON}-Objekt.

Die \gls{JSON}-Schemata für die relevanten Dokumentenarten sind in den Abbildungen~\ref{fig:json_kg5b} und~\ref{fig:json_vertrag} dargestellt.
Bei dem \gls{JSON}-Schema der sonstigen Dokumente~\ref{fig:json_sonstiges}, findet nur die Klassifikation statt, da hier keine Informationen relevant sind.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/kg5b_json}
    \caption{JSON-Schema des Dokumententyps KG5b}
    \label{fig:json_kg5b}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/vertrag_json}
    \caption{JSON-Schema des Dokumententyps Ausbildungsvertrag}
    \label{fig:json_vertrag}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/sonstiges_json}
    \caption{JSON-Schema des Dokumententyps Sonstiges}
    \label{fig:json_sonstiges}
\end{figure}


\subsection{Test- und Validierungsdatensatz}\label{subsec:test_val_dataset}

Nach der Vorbereitung der Dokumente wird ein Test- und Validierungsdatensatz erstellt.
Der Testdatensatz als auch der Validierungsdatensatz umfasst insgesamt 60 Dokumente, aufgeteilt in jeweils 30 Beispiele Dokumentenarten.
Um die reale Datenverteilung bestmöglich abzubilden, werden die Dokumente manuell ausgewählt.

Die Datensätze deckt dabei verschiedene Herausforderungen ab.
Neben Dokumenten mit geringer Qualität, vorrangig schlechte Scans und Fotos, befinden sich fehlerhaft ausgefüllte Dokumente sowie Dokumente mit fehlenden Unterschriften und Stempeln in den Datensätzen.
Um die hohe Varianz der Verträge und sonstigen Dokumente abzubilden, werden in den Datensätzen viele unterschiedliche Layouts gewählt.
Die Exemplare für die Verträge beinhalten sowohl starre Layouts als auch Fließtexte unterschiedlicher Firmen und Kammern.
Die sonstigen Dokumente beinhalten keine kontextfremden Dokumente, sondern Schulbescheinigungen, Studienbescheinigungen und Eintragungen bei der Handwerkskammer, um eine zusätzliche Herausforderung zu stellen.

Zur Erstellung der Wahrheitswerte (Ground Truth) werden alle Dokumente manuell annotiert und in das entsprechende \gls{JSON}-Format überführt.


\section{Evaluation der generierten JSON-Objekte}\label{sec:evaluation}

\subsection{Validierung des VLM-Outputs}\label{subsec:validate}

Um den \gls{JSON}-Output der \glspl{VLM} zu limitieren, existieren verschiedene Ansätze.
Während komplexere Lösungen nur Tokens zulassen die in \gls{JSON}-Objekten enthalten sein könnten, wird in dieser Evaluation ein einfacherer Ansatz gewählt.
Hierbei werden aus der generierten Antwort \glspl{JSON} gefiltert und geparst.
Sobald das Parsen der \gls{JSON} fehlschlägt, wird diese zurück an das \gls{VLM} geliefert, mit der Anweisung diese zu korrigieren.
Das Modell bekommt insgesamt drei Versuche eine valide \gls{JSON} zu generieren, bevor das Dokument als ungültig eingestuft wird.


\subsection{Vergleichslogik der unterschiedlichen JSON-Felder}\label{subsec:comparator}

Mit dem Ziel, ein realistisches Ergebnis zu erhalten, müssen unterschiedliche Feldtypen spezifisch verglichen werden.
Die Menge an distinkten Typen beinhaltet: Namen, Booleans, Zeichenketten, Daten und Monate.

Namen werden mit der Levenshtein-Similarity verglichen, um kleinere Fehler oder verschiedene Varianten zu tolerieren.
Beispielsweise wird ein Name, der \texttt{ue} statt \texttt{ü} enthält, nicht als Fehler erkannt.
Der Schwellenwert, der zwischen korrekt und nicht korrekt entscheidet, liegt bei 0,8.
Des Weiteren werden minimale \gls{OCR}-Fehler gefiltert.
Die \gls{YOLO}/\gls{OCR}-Pipeline verwendet ebenso diesen Vergleich mit den gleichen Schwellenwerten, somit wird das Ergebnis vergleichbar.

Zeichenketten und Booleans werden exakt verglichen.
Hierzu zählen die Felder \texttt{type}, \texttt{stamp\_company}, \texttt{signature\_company}, \texttt{signature\_child}, \texttt{signature\_legal\_guardian} und \texttt{apprenticeship\_finished}.
Lediglich die booleschen Werte werden zu \texttt{true} normalisiert, da hier eine potenzielle Fehlerquelle liegt.

Daten werden einheitlich in das Format DD.MM.YYYY gebracht, obwohl dies nicht dem internationalen Standard entspricht.
Das ist darauf zurückzuführen, dass so die Erkennung des \glspl{VLM} am robustesten ist, da die deutschen Dokumente auch mit diesem Format ausgefüllt werden.

Monate werden in das Format MM normalisiert.
Hierbei werden ausgeschriebene Monatsnamen sowie vollständige Datumsangaben angepasst.


\subsection{Bewertung der Klassifikation und der Information Extraction}\label{subsec:evaluation_classification_ie}

Um die generierten \gls{JSON}-Objekte des \glspl{VLM} schlussendlich bewerten zu können, werden die Klassifikation und \gls{IE} unabhängig voneinander bewertet.

Die Bewertung der Klassifikation ist ein Multiclass-Problem, da hier mehr als zwei Klassen vorliegen (KG5b, Ausbildungsvertrag, Sonstiges).
Für die Bestimmung der Güte der Modelle, wird eine Confusion-Matrix erstellt, wobei der Fokus auf dem F1-Score liegt.

Die Bewertung der Information Extraction ist komplexer als die der Klassifikation.
Um den Entity-Level F1-Score, der die Hauptmetrik für den Vergleich darstellt, zu berechnen werden einige Metadaten für jede generierte \gls{JSON} bestimmt.
Dazu gehören die Status der Felder, also ob diese vorhanden, nicht vorhanden oder halluziniert sind.
Sobald ein Feld als vorhanden markiert wird, wird mit den verschiedenen Vergleichslogiken die Richtigkeit der Felder bestimmt und den Metadaten hinzugefügt.

Auf Basis dieser Metadaten werden für den gesamten Dokumentenkorpus die True Positive (\gls{TP}), False Positive (\gls{FP}) und False Negative (\gls{FN}) gezählt.
Ein Feld zählt als \gls{TP}, wenn es vorhanden sowie korrekt ist.
Zu den \gls{FN} gehören Felder, die entweder fehlen oder vorhanden, aber falsch sind.
Ein Feld, das vorhanden aber falsch ist, zählt zusätzlich zu den \gls{FP} zusammen mit den Feldern die halluziniert wurden.
Diese doppelte Bestrafung stellt eine sehr strenge Bewertung für inhaltliche Fehler dar.
True Negatives (\gls{TN}) sind nicht zählbar, da die Menge an nicht gefundenen, leeren Feldern, theoretisch unendlich groß ist.

% TODO: Halluziantionen können weggelassen werden, Schema zeigen

\subsection{Messung von Latenz, Energieverbrauch und VRAM-Nutzung}\label{subsec:measure}

Im Hinblick auf die Bewertung der Modelle werden neben der Güte der Klassifikation und \gls{IE} die Latenz, die \gls{VRAM}-Nutzung sowie der Energieverbrauch gemessen.

Die Latenz der Modelle stellt lediglich das Delta zwischen Start- und Endzeit der Inferenz dar.
Hierbei wird der Zeitpunkt gemessen, an dem die eigentliche Inferenz startet und wenn die Antwort bereitsteht.
Das Laden des Modells wird nicht betrachtet, da diese keinen Einfluss auf die Antwortzeit in der produktiven Umgebung hat und folglich nicht relevant für die Evaluation ist.

Des Weiteren wird der Energieverbrauch während der Inferenz in Joule erfasst.
Die Messung der Leistung in Watt ist nicht zielführend, da sie die Zeit nicht berücksichtigt und Modelle mit geringerer Latenz bei vergleichbarem Energieverbrauch benachteiligt werden.

Wie auch der Energieverbrauch wird die \gls{VRAM}-Nutzung während der Inferenz gemessen.
Der Messzeitraum für den Energieverbrauch als auch für die \gls{VRAM}-Nutzung ist gleich der Latenz der Modelle.
Mit der Initialisierung des Modells wird einmalig der statische Verbrauch gemessen.


\section{Auswahl des Basismodells}\label{sec:model_selection}

\subsection{Prompt-Design}\label{subsec:prompting}

Die Gestaltung der Prompts für die Modellauswahl folgt der R-K-F-Formel.
Zur Strukturierung werden die Abschnitte durch Markdown-Überschriften getrennt, um dem Modell die Zuordnung zu erleichtern.
Während der Kontext die vorliegenden Dokumentenarten detailliert beschreibt, definiert das Format die \gls{JSON}-Schemata.
Zum Einsparen von Tokens wird das Modell strikt angewiesen lediglich das \gls{JSON}-Objekt zu generieren.

Um den Ressourcenverbrauch zu minimieren wird ein Zero-Shot Ansatz gewählt.
Während sowohl bei One-Shot als auch Few-Shot Prompts das Kontextfenster durch mitgegebene Bilder und Antworten belegt wird, ist bei Zero-Shot Prompts die Leistung des Modells rein auf die Instruktion zurückzuführen.
Dies minimiert die Kosten pro Inferenz als auch die Latenz.

\glspl{VLM} werden primär auf englischen Texten trainiert, jedoch werden auch deutschsprachige Prompts untersucht, um zu evaluieren, ob die Sprache der Prompts einen Einfluss auf die Klassifikation und \gls{IE} bei deutschen Dokumenten hat.


\subsection{Modellauswahl}\label{subsec:model_selection}

Für das Fine-Tuning muss ein geeignetes Basismodell gefunden werden.
Wie in den Lila markierten Felder in Abbildung~\ref{fig:overview} dargestellt, erfolgt die Auswahl in einem iterativen Prozess.

In jeder Iteration wird das Pixtral-12B sowie das Qwen-2.5-VL-7B mit dem Validierungsdatensatz evaluiert.
Daraufhin werden die Fehler der Klassifikation und \gls{IE} analysiert und auf dessen Grundlage der Prompt angepasst.
Nach der Anpassung wird erneut mit dem Validierungsdatensatz jedes Modell erprobt.

Dieser Prozess wird so lange fortgesetzt, bis durch die Anpassung des Prompts keine Steigerung der Metriken des Validierungsdatensatzes erzielt wird.
Schlussendlich wird das finale Modell mit dem verfeinerten Prompt mit dem Testdatensatz getestet.
Somit wird in ein Data Leakage ausgeschlossen und die Metriken werden Vergleichbar.


\section{Fine-Tuning des Basismodells}\label{sec:fine_tuning}

\subsection{Trainingsdatensatz}\label{subsec:train_dataset}

Um ein qualitativ hochwertiges Fine-Tuning zu ermöglichen, ist ein deutlich umfangreicherer Datensatz als für die Modellauswahl erforderlich.
Der finale Trainingsdatensatz umfasst 610 Dokumente, die sich aus 227 Verträgen, 165 KG5b-Formularen und 218 sonstigen Dokumenten zusammensetzen.

Die Klassen sind bewusst ungleich verteilt.
KG5b-Formulare haben ein starres Layout, wodurch schon mit einer geringeren Anzahl an Trainingsdaten ein gutes Ergebnis erwartet wird.
Infolge der hohen Varianz der Verträge und der sonstigen Dokumente erhalten diese im Training eine höhere Gewichtung.
Anders als beim Testdatensatz werden hier die Exemplare nicht manuell, sondern mithilfe einer Stichprobe ausgewählt.
Dabei werden die Daten aus einem einwöchigen Zeitraum betrachtet, wodurch ohne manuelle Auswahl eine genaue Repräsentation der realen Daten entsteht.
Des Weiteren wird so der Selektionsbias verhindert.

Zur effizienten Erstellung der Ground Truth wird ein Model-Assisted Labeling eingesetzt.
Hierbei generiert das im Prozess der Modellauswahl~\ref{subsec:model_selection} gewonnene Modell, zusammen mit dem angepassten Prompt, für jedes Dokument das jeweilige \gls{JSON}-Objekt.
Im Anschluss werden diese \gls{JSON}-Objekte manuell kontrolliert und korrigiert.
Durch den Einsatz des Modells zur Ermittelung der Ground Truth konnte der Aufwand erheblich gesenkt werden.


\subsection{Optimierung des Fine-Tuning}\label{subsec:optimise_fine_tuning}

Das Basismodell aus~\ref{subsec:model_selection} wurde mit mehreren Methoden des \gls{PEFT} und dem Framework Unsloth trainiert.
Im Laufe des Trainings wurde verschiedene Strategien angewandt, die sowohl die Veränderung der Parameter als auch des Trainingsdatensatzes~\ref{subsec:train_dataset} beinhalten.

Ein wesentlicher Aspekt der Optimierung lag in der Augmentation des Datensatzes.
Vor jedem erneuten Trainingslauf wurde der Datensatz vollständig randomisiert, um eine zufällige Reihenfolge der Trainingsdaten zu erhalten.
Des Weiteren wurden in einigen der Trainingsläufe die Seiten der einzelnen Dokumente getauscht, um dem Modell zu lernen die Informationen, ohne Position zu extrahieren.

Die Optimierung der Parameter erfolgte iterativ wie im blau markierten Bereich in Abbildung~\ref{fig:overview} zu erkennen.
Darunter zählten die Parameter der Ranggröße, des Optimizers




