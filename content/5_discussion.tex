\chapter{Diskussion}\label{ch:discussion}

\section{Interpretation der Fehler}\label{sec:perfomancy_error_patterns}

Sowohl die YOLO/OCR-Pipeline als auch die \glspl{VLM} weisen spezifische Fehlermuster auf, die sich in Klassifikations-, Format-, Detektions- und Wertfehler unterteilen lassen.

Klassifikationsfehler wiegen in der aus mehreren Einzelkomponenten bestehenden YOLO/OCR-Pipeline besonders schwer, da die Klassifikation hier einen Single Point of Failure darstellt.
Wie die Confusion Matrix in Abbildung~\ref{fig:confusion_matrices_overview} verdeutlicht, wurden fünf Dokumente fälschlicherweise als \texttt{Sonstiges} eingestuft, was wegen der Systemarchitektur (Abbildung~\ref{fig:aube}) zu einem direkten Abbruch führt.
Bei diesen fünf Dokumenten handelte es sich auffälligerweise ausschließlich um fotografierte Unterlagen.
Fotos weisen häufig Verzerrungen, Schattenwurf oder eine ungleichmäßige Beleuchtung auf, wodurch die \gls{OCR}-Ergebnisse schlechter ausfallen und die Klassifikation behindern.

Diese Klassifikationsfehler mindern die Leistung der Information Extraction, da mit einem einzigen Fehler schon zehn (Vertrag) oder zwölf (KG5b) Felder fehlen.
Auch die fehlerhafte Klassifikation von Verträgen als KG5b-Formular oder umgekehrt, wirkt sich stark auf die Information Extraction aus, da unterschiedliche \gls{YOLO}-Modelle für Verträge und KG5b-Formulare trainiert wurden.
Ein entscheidender Vorteil des generalistischen \glspl{VLM} ist die Robustheit durch das einheitliche Modell.
Da die Felder eines Vertrags eine Teilmenge des KG5b-Schemas darstellen, bleibt die Information Extraction teilweise erfolgreich, selbst das Dokument falsch klassifiziert wurde.
In Idealfall fehlen bei der Erkennung eines Vertrages statt eines KG5b-Formulars kein Feld und umgekehrten Fall maximal zwei.
Die Fehlklassifizierung als sonstiges Dokument ist jedoch auch bei den \glspl{VLM} kritisch, da hier das Modell keine weiteren Felder zurückliefert.
Gemäß den Confusion Matrices in Abbildung~\ref{fig:confusion_matrices_32b_peft} zu erkennen erweisen sich insbesondere die Modelle Qwen-2.5-VL-7B-finetuned und Qwen-2.5-VL-32B als äußert zuverlässig in der Klassifizierung.

Die Detektionsfehler beziehen sich auf Checkboxen, Stempel und Unterschriften.
Checkboxen sind im KG5b-Formular für das Feld \texttt{apprenticeship\_finished} relevant (siehe Abbildung~\ref{fig:kg5bpage1}).
Hier müssen die Modelle zwischen leeren, angekreuzten und durchgestrichenen Boxen unterscheiden.
Detektionsfehler sind häufig auf den unvermeidbaren Medienbruch zurückzuführen, der entsteht, wenn Dokumente zur Unterschrift oder Stempelung ausgedruckt und anschließend wieder digitalisiert werden.
Durch den Medienbruch verliert der Stempel und die Unterschriften ihren Kontrast oder werden in Schwarz-Weiß-Scans unkenntlich.

Zusätzlich erschwert die räumliche Nähe der Felder \texttt{date\_document} und \texttt{signature\_company} die Extraktion.
Da Unterschriften regelmäßig den vorgesehenen Platz überschreiten, wird das Datumsfeld häufig überdeckt.
Während die YOLO/OCR-Pipeline hier oft keine valide Bounding-Box mehr liefern kann, zeigen das Qwen-2.5-VL-32B und das Qwen-2.5-VL-7B-finetuned eine Robustheit gegenüber diesen Überlagerungen.

Formatfehler verdeutlichen Unterschiede zwischen den Instruction-Following Fähigkeiten der Basismodelle.
Während die Qwen-Modelle nahezu fehlerfrei valide \gls{JSON}-Strukturen lieferten, scheiterte das Pixtral-12B trotz der Retry-Logik an zwei Verträgen.
Das Modell hat in keinem der erneuten Versuche die umschließenden Klammern ({}) gesetzt, wodurch das Parsen fehlschlug.
Infolgedessen fehlten automatisch insgesamt 20 \gls{JSON}-Felder.
Dies deutet darauf hin, dass obwohl das Pixtral-12B mehr Parameter als das Qwen-2.5-VL-7B besitzt, eine geringere Stabilität bei der Generierung von \gls{JSON}-Strukturen aufweist.

Bei den Wertfehlern muss zwischen \gls{OCR}-Fehlern und Mapping-Fehlern (falsche semantische Zuordnung) unterschieden werden.
Die YOLO/OCR-Pipeline zeigte deutliche Schwächen in der Erkennung von Namen und Datumsangaben.
Selbst die Levenshtein-Similarity mit einem Threshold von 0,8 reichte bei einigen Fällen nicht aus, um die Fehler zu kompensieren.
Bei Pixtral-12B und Qwen-2.5-VL-7B traten neben \gls{OCR}-Fehlern vermehrt Mapping-Fehler auf, bei denen beispielsweise das Geburtsdatum des Kindes mit dem Startdatum der Ausbildung vertauscht wurde.
Durch das Fine-Tuning des Qwen-2.5-VL-7B konnte eine deutliche Reduzierung der \gls{OCR}- und Mapping-Fehler erzielt werden.
Das Qwen-2.5-VL-32B wies seltener \gls{OCR}-Fehler auf als das Qwen-2.5-VL-7B-finetuned, produzierte jedoch geringfügig mehr Mapping-Fehler.

Halluzinationen traten primär bei den Basismodellen auf, insbesondere bei der Klasse \texttt{Sonstiges}, wo die Modelle versuchten, Informationen zu extrahieren, die im Schema gar nicht vorgesehen waren.
Wegen der Konfiguration, können diese zusätzlichen Felder ohne Einfluss gefiltert werden, weshalb sie in der Evaluation nicht bestraft wurden.


\section{Abwägung Effektivität und Ressourceneffizienz}\label{sec:effectiveness_resource_efficiency}

Die Ergebnisse der Untersuchung offenbaren einen Zielkonflikt zwischen der Leistungsfähigkeit der Modelle und ihrem Ressourcenbedarf.
Während die YOLO/OCR-Pipeline durch ihre extrem geringe \gls{Latenz} und Ressourceneffizienz besticht, zeigen die \glspl{VLM} eine deutliche Überlegenheit in der Qualität der Information Extraction.

Die YOLO/OCR-Pipeline bildet mit einem durchschnittlichen Energieverbrauch von 117 J und einer \gls{Latenz} im Sub-Sekunden-Bereich die Basislinie für die Effizienz.
Demgegenüber steht das leistungsstärkste Modell, das Qwen-2.5-VL-32B, welches im Durchschnitt die 160-fache Energiemenge benötigt und eine Inferenzzeit von durchschnittlich 64,16 Sekunden aufweist.
Analytisch betrachtet korreliert in diesem Versuchsaufbau eine Steigerung der Extraktionsqualität direkt mit einem exponentiellen Anstieg des Ressourcenverbrauchs.
Kritisch betrachtet relativiert sich der Effizienzvorteil der Pipeline jedoch im operativen Kontext: Ein niedriger F1-Score (0,51 bei Verträgen ) führt zwangsläufig zu manuellen Nacharbeiten durch Sachbearbeiter.
Die durch die schnelle technische Verarbeitung gewonnene Zeit wird somit im Gesamtprozess durch menschliche Korrektureingriffe wieder aufgezehrt.

Neben dem Energiebedarf ist die Hardware-Infrastruktur, spezifisch die \gls{VRAM}-Auslastung, ein limitierender Faktor.
Während die YOLO/OCR-Pipeline auf Standard-Hardware lauffähig ist, setzen die großen \glspl{VLM} leistungsstarke Rechenzentrums-GPUs voraus.
Das Qwen-2.5-VL-32B übertrifft das nachtrainierte Qwen-2.5-VL-7B zwar in der Extraktion von Verträgen (F1-Score 0,94 gegenüber 0,84), erkauft sich diesen Vorteil jedoch mit einem fast vierfach höheren statischen VRAM-Bedarf.

Als technischer \enquote{Sweet Spot} zwischen Verbrauch, \gls{Latenz} und Extraktionsgüte kristallisiert sich das Qwen-2.5-VL-7B-finetuned heraus.
Es übertrifft das Altsystem sowohl in der Klassifikation als auch in der Information Extraction signifikant und bleibt dabei ressourcentechnisch handhabbar.
Mit einem statischen \gls{VRAM}-Bedarf von circa 17 GB  lässt sich dieses Modell auf gängigen Enterprise-GPUs betreiben, ohne die extremen Hardwareanforderungen des 32B-Modells zu stellen.
Zwar bestimmen die gewünschte \gls{Latenz} und der Durchsatz weiterhin die Skalierung der Infrastruktur, jedoch senkt das 7B-Modell die Einstiegshürde für einen produktiven Einsatz drastisch.


\section{Komplexität und Wartbarkeit der Systemarchitekturen}\label{sec:maintainability}

\section{Zusammenfassende Beantwortung der Forschungsfragen}\label{subsec:answer_ffs}

\section{Limitation und Probleme der Untersuchung}\label{sec:limitations}

- name usw auch aus sonstigen extrahieren lassen wegen klassifikationsfehlern
- bounding boxen für Kontrolle ob wirklich unterschriften zugeordnet werden können (labeln hätte um einiges länger gedauert)
- base64 ging nicht



\section{Handlungsempfehlung für die Bundesagentur für Arbeit}\label{sec:recommendation_for_action}

- mitarbeiterkosten
- andere Projekte
- weiter labeln mit 32B modell -> 7B-finetuned noch robuster

- yolo ocr schneidet am besten ab wegen pipline design bei sonstigen dokumenten
- fehlerhafte klassifikation zieht sich durch
- confusion matrix yolo ocr viele als sonstige -> große Fehler
- kg5d
- hohen Verbrauch erklären
