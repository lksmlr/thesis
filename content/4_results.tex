\chapter{Ergebnisse}\label{ch:results}

Die folgenden Ergebnisse basieren auf der Evaluation mit dem Testdatensatz.
Bei der~\gls{Information Extraction} wurden die Halluzinationen von Feldern nicht bestraft, da diese durch das Schema gefiltert werden können.

\section{Ergebnisse der OCR/YOLO-Pipeline gegenüber den Basismodellen}\label{sec:results_ie}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/results_ie}
    \caption{Ergebnisse der~\gls{Information Extraction}}
    \label{fig:results_ie}
\end{figure}

Abbildung~\ref{fig:results_ie} stellt die F1-Scores der Basismodelle im Vergleich zur OCR/YOLO-Pipeline für die KG5b-Formulare und Ausbildungsverträge dar.

Bei den KG5b-Formularen liegen die Qwen-Modelle und die OCR/YOLO-Pipeline nah beieinander.
Die Pipeline liegt mit einem F1-Score von 0,75 geringfügig hinter dem Qwen-2.5-VL-7B.
Das größere Qwen-2.5-VL-32B bildet die Spitze mit einem F1-Score von 0,87, während sich das Pixtral-12B am unteren Ende befindet.

Ein differenzierteres Bild zeigt sich bei den heterogenen Verträgen.
Die OCR/YOLO-Pipeline fällt auf einen F1-Score von 0,51 ab.
Demgegenüber stehen die Qwen-Modelle mit 0,94 (Qwen-2.5-VL-32B) und 0,84 (Qwen-2.5-VL-7B), die bei den Verträgen besser abschneiden als bei den KG5b-Formularen.
Das Pixtral-12B bildet erneut das Schlusslicht, liegt jedoch nur minimal hinter der OCR/YOLO-Pipeline.

Die Ergebnisse der~\gls{Information Extraction} verdeutlichen die Abhängigkeit der Leistungsfähigkeit von der Dokumentenart bei der OCR/YOLO-Pipeline.
Im Vergleich zu den Qwen-Modellen sinkt die Leistung bei den variablen Verträgen gegenüber den starren KG5b-Formularen.
Die Daten belegen zusätzlich, dass bereits ein kleines Modell wie das Qwen-2.5-VL-7B die Pipeline übertrifft.
In keiner Dokumentenart schneidet das Pixtral-12B besser als die bisherige Lösung ab.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification}
\end{figure}

Abbildung~\ref{fig:results_classification} visualisiert die Klassifikationsleistung der Basismodelle und der OCR/YOLO-Pipeline anhand von F1-Scores.

Im Gegensatz zur~\gls{Information Extraction} zeigt sich hier ein homogenes Bild.
Das Qwen-2.5-VL-32B erzielt in allen drei Kategorien die besten Ergebnisse und klassifiziert die KG5b-Formulare sogar mit einem Score von 1,0.
Die Ergebnisse des Pixtral-12B, des Qwen-2.5-VL-7B und der OCR/YOLO-Pipeline liegen über alle Dokumentenarten hinweg dicht beisammen.
In der Kategorie der sonstigen Dokumente fällt die Leistung aller Modelle im Vergleich zu den anderen Dokumentenarten ab.
Hier erzielt das Pixtral-12B zusammen mit dem Qwen-2.5-VL-7B den niedrigsten Score mit 0,87.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_7b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_pixtral}
        \caption{Confusion Matrix für das Pixtral-12B}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_yolo_ocr}
        \caption{Confusion Matrix für die OCR/YOLO-Pipeline}
    \end{subfigure}

    \caption{Confusion Matrices aller Modelle}
    \label{fig:confusion_matrices}
\end{figure}

Die Confusion Matrices in Abbildung~\ref{fig:confusion_matrices} repräsentieren die Ergebnisse der Klassifikation im Detail.

Während die F1-Scores der Klassifikation ein ähnliches Leistungsniveau zeigen, offenbaren die Matrizen unterschiedliche Fehlerschwerpunkte.
Das Qwen-2.5-VL-32B klassifiziert sowohl die KG5b-Formulare als auch die Verträge fehlerfrei.
Im Gegensatz dazu erkennt die OCR/YOLO-Pipeline die sonstigen Dokumente perfekt, ordnet diesen aber zusätzlich fünf Dokumente anderer Arten zu.
Das Qwen-2.5-VL-7B und das Pixtral-12B haben ein ähnliches Fehlerschema und erkennen die Verträge perfekt.
Allerdings klassifizieren beide Modelle KG5b-Formulare sowie sonstige Dokumente häufig als Vertrag.

Die gemeinsame Betrachtung der F1-Scores und der Confusion Matrices verdeutlicht, dass trotz quantitativ vergleichbarer Ergebnisse die Modelle qualitative Unterschiede aufweisen.
Besonders die Stabilität des Qwen-2.5-VL-32B ist hervorzuheben, da die relevanten Dokumente fehlerfrei erkannt werden.
Demgegenüber stehen die kleineren VLMs, die Probleme mit der Trennung der Dokumentenklassen haben.
Die OCR/YOLO-Pipeline neigt zur Zuordnung zu den sonstigen Dokumenten, wodurch relevante Dokumente verloren gehen.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency}
    \caption{Ergebnisse der Latenzmessung}
    \label{fig:results_latency}
\end{figure}

Das Diagramm~\ref{fig:results_latency} zeigt die durchschnittliche und maximale \gls{Latenz} der Basismodelle und der OCR/YOLO-Pipeline in Sekunden.

Aufgrund der Architektur antwortet die OCR/YOLO-Pipeline mit einer durchschnittlichen \gls{Latenz} von 0,58 Sekunden nahezu in Echtzeit.
Gegenüber der Pipeline benötigt selbst das kleinste Modell fast dreizehnmal länger.
Besonders auffällig ist die hohe maximale \gls{Latenz} des Qwen-2.5-VL-32B mit 252 Sekunden, womit dieses Modell deutlich über den anderen liegt.

Die \gls{Latenz}messung belegt den Zusammenhang zwischen der Modellgröße von VLMs und deren Antwortgeschwindigkeit.
Mit zunehmender Parameteranzahl wächst die \gls{Latenz} überproportional.

Diese Ergebnisse begründen zudem die Auswahl des Basismodells.
Durch die Überlegenheit des Qwen-2.5-VL-7B gegenüber dem Pixtral-12B in der~\gls{Information Extraction} sowie der Klassifikation stellt sich das Qwen-Modell als besserer Kandidat für das Training heraus.


\section{Ergebnisse des weitertrainierten Modells gegenüber dem größeren Basismodell}\label{sec:results_latency_peft}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_ie_peft_32b_7b}
    \caption{Ergebnisse der~\gls{Information Extraction}}
    \label{fig:results_ie_peft_32b_7b}
\end{figure}

Abbildung~\ref{fig:results_ie_peft_32b_7b} stellt die F1-Scores der~\gls{Information Extraction} des Qwen-2.5-VL-7B-finetuned und des Qwen-2.5-VL-32B dar.

Bei den standardisierten KG5b-Formularen erzielen beide Modelle identische Ergebnisse mit einem F1-Score von 0,87.
Demgegenüber zeigen die Ausbildungsverträge einen deutlicheren Unterschied.
Während das Qwen-2.5-VL-32B einen F1-Score von 0,94 aufweist, liegt das angepasste Modell mit einer Differenz von 0,10 darunter.

Die Verteilung deutet darauf hin, dass ein domänenspezifisches Training ausreicht, um bei starren Layouts auf die Leistung des Qwen-2.5-VL-32B aufzuschließen.
Bei variablen Dokumenten wie den Ausbildungsverträgen erzielt das 32B-Modell hingegen weiterhin die höchste Präzision in der~\gls{Information Extraction}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_classification_peft_32b_7b}
    \caption{Ergebnisse der Klassifikation}
    \label{fig:results_classification_peft_32b_7b}
\end{figure}

Abbildung~\ref{fig:results_classification_peft_32b_7b} visualisiert die Klassifikationsleistung der Modelle anhand der F1-Scores.

Beide Modelle erreichen bei den KG5b-Formularen ein perfektes Ergebnis.
Die anderen beiden Kategorien zeigen einen minimalen Vorsprung des angepassten Modells.
Hier erreicht das Qwen-2.5-VL-7B-finetuned einen F1-Score von 0,97 bei den Verträgen und 0,95 bei den sonstigen Dokumenten.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_peft}
        \caption{Confusion Matrix für das Qwen-2.5-VL-7B-finetuned}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/results_confusion_matrix_qwen_32b}
        \caption{Confusion Matrix für das Qwen-2.5-VL-32B}
    \end{subfigure}
    \caption{Confusion Matrices beider Modelle}
    \label{fig:confusion_matrices_32b_peft}
\end{figure}

\newpage
Einen Einblick in die Fehlerverteilung liefern die Confusion Matrices in Abbildung~\ref{fig:confusion_matrices_32b_peft}.

Das Qwen-2.5-VL-7B-finetuned klassifiziert die KG5b-Formulare sowie die sonstigen Dokumente vollständig korrekt und ordnet lediglich einen Vertrag als sonstiges Dokument ein.
Im Gegensatz dazu erkennt das Qwen-2.5-VL-32B die KG5b-Formulare und Verträge fehlerfrei, ordnet jedoch drei der sonstigen Dokumente fälschlicherweise den Verträgen zu.

Die gemeinsame Betrachtung der F1-Scores und der Confusion Matrices belegt, dass das Fine-Tuning die Klassifikationsleistung steigert.
Ein spezifisches Training schafft in der Klassifikation einen Vorteil gegenüber einem größeren Basismodell.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_latency_peft_32b_7b}
    \caption{Ergebnisse der Latenzmessung pro Dokument}
    \label{fig:results_latency_peft_32b_7b}
\end{figure}

Abbildung~\ref{fig:results_latency_peft_32b_7b} stellt die durchschnittlichen und maximalen Latenzen des Qwen-2.5-VL-32B und Qwen-2.5-VL-7B-finetuned dar.

Es zeigt sich ein deutlicher Vorteil des kleineren Modells.
Das Qwen-2.5-VL-7B-finetuned antwortet nach durchschnittlich 9,57 Sekunden, während das Qwen-2.5-VL-32B mehr als das Sechsfache an Zeit benötigt.
Bei den Maximalwerten liegt das große Modell mit 252,06 Sekunden deutlich über dem Qwen-2.5-VL-7B-finetuned.
Dort liegt die maximale \gls{Latenz} näher an dem Durchschnittswert.

Die Messwerte verdeutlichen, dass ein Fine-Tuning keinen deutlichen Einfluss auf die Inferenzzeit gegenüber dem Basismodell hat.
Mit der höheren Parameteranzahl benötigt das Qwen-2.5-VL-32B länger zum Antworten als das angepasste Modell.


\section{Ressourcenverbrauch der VLMs gegenüber der OCR/YOLO-Pipeline}\label{sec:results_vram_energy_usage}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_model}
    \caption{Ergebnisse der VRAM-Nutzung des geladenen Modells}
    \label{fig:results_vram_model}
\end{figure}

Abbildung~\ref{fig:results_vram_model} visualisiert die statische VRAM-Nutzung der VLMs sowie der OCR/YOLO-Pipeline bei der Initialisierung in Gigabyte.

Die OCR/YOLO-Pipeline verzeichnet trotz der Verwendung mehrerer Modelle einen geringen statischen Verbrauch von 4,21 GB.
Die beiden 7B-Modelle liegen auf einem ähnlichen Niveau.
Den mit Abstand höchsten Verbrauch weist das Qwen-2.5-VL-32B auf, das mit 64,18 GB den VRAM-Verbrauch der Pipeline um mehr als das Fünfzehnfache übersteigt.
Zwischen den drei Qwen-Modellen reiht sich das Pixtral-12B ein.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_vram_inference}
    \caption{Ergebnisse der VRAM-Nutzung während der Inferenz pro Dokument}
    \label{fig:results_vram_inference}
\end{figure}

Abbildung~\ref{fig:results_vram_inference} repräsentiert den durchschnittlichen und maximalen dynamischen Verbrauch in Gigabyte pro Inferenz.

Auch in dieser Messung befindet sich die OCR/YOLO-Pipeline mit einer durchschnittlichen Auslastung von 0,49 GB unter den anderen Modellen.
Der maximale Bedarf liegt mit 1,02 GB unter dem Durchschnittsverbrauch des effizientesten VLMs.
Auffällig ist das Verhältnis des Qwen-2.5-VL-7B-finetuned und des größeren Qwen-2.5-VL-32B.
Das kleinere, trainierte Modell hat mit durchschnittlich 3,03 GB einen geringfügig höheren Verbrauch als das 32B-Modell.

Die gemeinsame Betrachtung der statischen und dynamischen VRAM-Auslastung verdeutlicht den Architekturunterschied zwischen der Pipeline und den VLMs.
Selbst das kleinste Modell ist im Gegensatz zur OCR/YOLO-Pipeline um ein Vielfaches größer, was einen deutlichen Unterschied bezüglich der benötigten Hardware zur Laufzeit darstellt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results_energy}
    \caption{Ergebnisse des Energieverbrauchs pro Dokument}
    \label{fig:results_energy_usage}
\end{figure}

Abbildung~\ref{fig:results_energy_usage} veranschaulicht den durchschnittlichen und maximalen Energieverbrauch pro Inferenz in Joule.

Der Energieverbrauch zeigt den markantesten Unterschied zwischen den Systemen.
Die OCR/YOLO-Pipeline verbraucht durchschnittlich lediglich 117 Joule.
Selbst das effizienteste Modell, das Qwen-2.5-VL-7B, verbraucht durchschnittlich das 22-Fache der Pipeline.
Das Qwen-2.5-VL-7B-finetuned liegt knapp über seinem Basismodell.
Den mit Abstand höchsten Energiebedarf hat das Qwen-2.5-VL-32B.
Mit durchschnittlich 18.786 Joule und maximal 82.121 Joule übertrifft das Modell die Pipeline im Mittel um das 160-Fache.

Die Messung des Energieverbrauchs belegt den quantitativen Unterschied der VLMs gegenüber der spezialisierten Pipeline.
Es wird deutlich, dass die Skalierung der Parameteranzahl mit einem höheren Energieverbrauch einhergeht.
Obwohl die VLMs eine höhere Klassifikations- und Extraktionsleistung aufweisen, verdeutlicht der reine Energieverbrauch, dass der Einsatz eines solchen Modells mit einem ungleich höheren Ressourceneinsatz erfordert.